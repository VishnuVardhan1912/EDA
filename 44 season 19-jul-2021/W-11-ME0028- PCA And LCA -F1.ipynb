{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b05edaf",
   "metadata": {},
   "source": [
    "# Universalbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faa06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ignore harmful warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82547d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0   1   25           1      49     91107       4    1.6          1         0   \n",
       "1   2   45          19      34     90089       3    1.5          1         0   \n",
       "2   3   39          15      11     94720       1    1.0          1         0   \n",
       "3   4   35           9     100     94112       1    2.7          2         0   \n",
       "4   5   35           8      45     91330       4    1.0          2         0   \n",
       "\n",
       "   Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "0              0                   1           0       0           0  \n",
       "1              0                   1           0       0           0  \n",
       "2              0                   0           0       0           0  \n",
       "3              0                   0           0       0           0  \n",
       "4              0                   0           0       0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Universal bank data\n",
    "\n",
    "bankdata = pd.read_csv(r\"D:\\iiit notes\\Programming\\AI\\Internship practice\\44 season 19-jul-2021\\Universalbank.csv\",header=0)\n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb45ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  5000 non-null   int64  \n",
      " 1   Age                 5000 non-null   int64  \n",
      " 2   Experience          5000 non-null   int64  \n",
      " 3   Income              5000 non-null   int64  \n",
      " 4   ZIP Code            5000 non-null   int64  \n",
      " 5   Family              5000 non-null   int64  \n",
      " 6   CCAvg               5000 non-null   float64\n",
      " 7   Education           5000 non-null   int64  \n",
      " 8   Mortgage            5000 non-null   int64  \n",
      " 9   Personal Loan       5000 non-null   int64  \n",
      " 10  Securities Account  5000 non-null   int64  \n",
      " 11  CD Account          5000 non-null   int64  \n",
      " 12  Online              5000 non-null   int64  \n",
      " 13  CreditCard          5000 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 547.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset information\n",
    "\n",
    "bankdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdb61b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:  3530\n",
      "Class 1:  1470\n",
      "proportion:  2.401 : 1 \n"
     ]
    }
   ],
   "source": [
    "# Display no of '0' and '1' in Target variable\n",
    "\n",
    "bankdata_values = bankdata.CreditCard.value_counts()\n",
    "print('Class 0: ',bankdata_values[0])\n",
    "print('Class 1: ', bankdata_values[1])\n",
    "print('proportion: ', round(bankdata_values[0]/bankdata_values[1], 3), ': 1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111759e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the columns which are not impacting Target variable\n",
    "\n",
    "del bankdata['ID']\n",
    "del bankdata['ZIP Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c10095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col1 variable used to create the dummy variables\n",
    "\n",
    "cols1 = ['Family', 'Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c788bb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                    45\n",
       "Experience             47\n",
       "Income                162\n",
       "Family                  4\n",
       "CCAvg                 108\n",
       "Education               3\n",
       "Mortgage              347\n",
       "Personal Loan           2\n",
       "Securities Account      2\n",
       "CD Account              2\n",
       "Online                  2\n",
       "CreditCard              2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57579e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cols2 variable for MinMaxScaler function\n",
    "\n",
    "cols2 = ['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe1d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cols3 variable for normalization\n",
    "\n",
    "cols3 = ['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Education', 'Mortgage', \n",
    "         'Personal Loan', 'Securities Account', 'CD Account', 'Online'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21497c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>25.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experience</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>49.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCAvg</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mortgage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Loan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Securities Account</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD Account</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditCard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0     1     2      3     4\n",
       "Age                 25.0  45.0  39.0   35.0  35.0\n",
       "Experience           1.0  19.0  15.0    9.0   8.0\n",
       "Income              49.0  34.0  11.0  100.0  45.0\n",
       "CCAvg                1.6   1.5   1.0    2.7   1.0\n",
       "Mortgage             0.0   0.0   0.0    0.0   0.0\n",
       "Personal Loan        0.0   0.0   0.0    0.0   0.0\n",
       "Securities Account   1.0   1.0   0.0    0.0   0.0\n",
       "CD Account           0.0   0.0   0.0    0.0   0.0\n",
       "Online               0.0   0.0   0.0    0.0   0.0\n",
       "CreditCard           0.0   0.0   0.0    0.0   1.0\n",
       "Family_1             0.0   0.0   1.0    1.0   0.0\n",
       "Family_2             0.0   0.0   0.0    0.0   0.0\n",
       "Family_3             0.0   1.0   0.0    0.0   0.0\n",
       "Family_4             1.0   0.0   0.0    0.0   1.0\n",
       "Education_1          1.0   1.0   1.0    0.0   0.0\n",
       "Education_2          0.0   0.0   0.0    1.0   1.0\n",
       "Education_3          0.0   0.0   0.0    0.0   0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variables for cols1\n",
    "\n",
    "bankdata = pd.get_dummies(bankdata, columns=cols1)\n",
    "bankdata.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ce77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indentify the dependent and Target variable\n",
    "\n",
    "IndepVar = []\n",
    "for col in bankdata.columns:\n",
    "    if col != 'CreditCard':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'CreditCard'\n",
    "\n",
    "x = bankdata[IndepVar]\n",
    "y = bankdata[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c60cbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data test into train and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 6)\n",
    "x_test_F1 = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6137c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling for col2 by using MinMaxScaler function\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train[cols2] = mmscaler.fit_transform(x_train[cols2])\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test[cols2] = mmscaler.fit_transform(x_test[cols2])\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028947d6",
   "metadata": {},
   "source": [
    "# Random Forest without LDA & PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdded59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 28 344]\n",
      " [  1 877]]\n",
      "Outcome Values : \n",
      " 28 344 1 877\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.08      0.14       372\n",
      "           0       0.72      1.00      0.84       878\n",
      "\n",
      "    accuracy                           0.72      1250\n",
      "   macro avg       0.84      0.54      0.49      1250\n",
      "weighted avg       0.79      0.72      0.63      1250\n",
      "\n",
      "Accuracy : 72.4 %\n",
      "Precision : 96.6 %\n",
      "Recall : 7.5 %\n",
      "F1 Score : 0.14\n",
      "Balanced Accuracy : 53.7 %\n",
      "MCC 0.225\n",
      "roc_auc_score: 0.537\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y_pred = modelRF.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c7786",
   "metadata": {},
   "source": [
    "# Logistic Regression without LDA & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bf20041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 74 298]\n",
      " [ 13 865]]\n",
      "Outcome Values : \n",
      " 74 298 13 865\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.20      0.32       372\n",
      "           0       0.74      0.99      0.85       878\n",
      "\n",
      "    accuracy                           0.75      1250\n",
      "   macro avg       0.80      0.59      0.59      1250\n",
      "weighted avg       0.78      0.75      0.69      1250\n",
      "\n",
      "Accuracy : 75.1 %\n",
      "Precision : 85.1 %\n",
      "Recall : 19.9 %\n",
      "F1 Score : 0.322\n",
      "Balanced Accuracy : 59.2 %\n",
      "MCC 0.331\n",
      "roc_auc_score: 0.592\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = modelLR.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y1_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9df89f",
   "metadata": {},
   "source": [
    "# Decision Tree without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707d7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[171 201]\n",
      " [247 631]]\n",
      "Outcome Values : \n",
      " 171 201 247 631\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.46      0.43       372\n",
      "           0       0.76      0.72      0.74       878\n",
      "\n",
      "    accuracy                           0.64      1250\n",
      "   macro avg       0.58      0.59      0.59      1250\n",
      "weighted avg       0.65      0.64      0.65      1250\n",
      "\n",
      "Accuracy : 64.2 %\n",
      "Precision : 40.9 %\n",
      "Recall : 46.0 %\n",
      "F1 Score : 0.433\n",
      "Balanced Accuracy : 59.0 %\n",
      "MCC 0.173\n",
      "roc_auc_score: 0.589\n"
     ]
    }
   ],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y2_pred = modelDT.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y2_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y2_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae172f7",
   "metadata": {},
   "source": [
    "# SVM without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bda09c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[178 194]\n",
      " [251 627]]\n",
      "Outcome Values : \n",
      " 178 194 251 627\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.48      0.44       372\n",
      "           0       0.76      0.71      0.74       878\n",
      "\n",
      "    accuracy                           0.64      1250\n",
      "   macro avg       0.59      0.60      0.59      1250\n",
      "weighted avg       0.66      0.64      0.65      1250\n",
      "\n",
      "Accuracy : 64.4 %\n",
      "Precision : 41.5 %\n",
      "Recall : 47.8 %\n",
      "F1 Score : 0.444\n",
      "Balanced Accuracy : 59.6 %\n",
      "MCC 0.185\n",
      "roc_auc_score: 0.596\n"
     ]
    }
   ],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "modelSVMGaussian.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y3_pred = modelSVMGaussian.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y3_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y3_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7542a2",
   "metadata": {},
   "source": [
    "# KNN Without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3adea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[112 260]\n",
      " [130 748]]\n",
      "Outcome Values : \n",
      " 112 260 130 748\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.30      0.36       372\n",
      "           0       0.74      0.85      0.79       878\n",
      "\n",
      "    accuracy                           0.69      1250\n",
      "   macro avg       0.60      0.58      0.58      1250\n",
      "weighted avg       0.66      0.69      0.67      1250\n",
      "\n",
      "Accuracy : 68.8 %\n",
      "Precision : 46.3 %\n",
      "Recall : 30.1 %\n",
      "F1 Score : 0.365\n",
      "Balanced Accuracy : 57.6 %\n",
      "MCC 0.177\n",
      "roc_auc_score: 0.577\n"
     ]
    }
   ],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y4_pred = modelKNN.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y4_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y4_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45089087",
   "metadata": {},
   "source": [
    "# Train the Principal Component Analysis (PCA) with train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea429b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.83838810e-01 1.44098323e-01 1.31109583e-01 1.15360610e-01\n",
      " 1.11395617e-01 9.68989534e-02 6.26894986e-02 5.94549149e-02\n",
      " 4.48918944e-02 1.82923961e-02 1.41944688e-02 1.13669152e-02\n",
      " 6.24557064e-03 1.62444884e-04 1.33719181e-32 3.71804174e-33]\n"
     ]
    }
   ],
   "source": [
    "# Principal component analysis (PCA) is a statistical technique to convert high dimensional data to low dimensional data\n",
    "# by selecting the most important features that capture maximum information about the dataset. The features are selected\n",
    "# on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal\n",
    "# component. The feature that is responsible for second highest variance is considered the second principal component,\n",
    "# and so on. It is important to mention that principal components do not have any correlation with each other.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "applyPCA = PCA()\n",
    "\n",
    "x1_train = applyPCA.fit_transform(x_train)\n",
    "x1_test = applyPCA.transform(x_test)\n",
    "explained_variance = applyPCA.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139b662",
   "metadata": {},
   "source": [
    "# Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31059d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-b9014944d7fc>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-b9014944d7fc>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    y5from sklearn.metrics import classification_report, confusion_matrix\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x1_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y5from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y5_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y5_pred), 3))_pred = modelRF.predict(x1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d07865",
   "metadata": {},
   "source": [
    "# Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d37255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y6_pred = modelLR.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y6_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y6_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88ca61",
   "metadata": {},
   "source": [
    "#  Decision Tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x1_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y7_pred = modelDT.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y7_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y7_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c807da",
   "metadata": {},
   "source": [
    "# SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "\n",
    "modelSVMGaussian.fit(x1_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y8_pred = modelSVMGaussian.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y8_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y8_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078deeab",
   "metadata": {},
   "source": [
    "# KNN with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ea81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x1_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y9_pred = modelKNN.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y9_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y9_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200961ea",
   "metadata": {},
   "source": [
    "# Train the Linear Discriminant Analysis (LDA) with train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb39af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "applyLDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "x2_train = applyLDA.fit_transform(x_train, y_train)\n",
    "x2_test = applyLDA.transform(x_test)\n",
    "\n",
    "explained_variance = applyLDA.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2662173",
   "metadata": {},
   "source": [
    "# Random Forest with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c62cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x2_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y10_pred = modelRF.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y10_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y10_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d26643",
   "metadata": {},
   "source": [
    "# Logistic Regression with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x2_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y11_pred = modelLR.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y11_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y11_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07d581",
   "metadata": {},
   "source": [
    "# Decision Tree with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x2_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y12_pred = modelDT.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y12_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y12_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abac15",
   "metadata": {},
   "source": [
    "# SVM with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "\n",
    "modelSVMGaussian.fit(x2_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y13_pred = modelSVMGaussian.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y13_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y13_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2897f",
   "metadata": {},
   "source": [
    "# KNN with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x2_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y14_pred = modelKNN.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y14_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y14_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa651d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
