{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cafd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore harmless warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Import psql to run queries\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91913c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>WineType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  WineType  \n",
       "0                          3.92     1065         0  \n",
       "1                          3.40     1050         0  \n",
       "2                          3.17     1185         0  \n",
       "3                          3.45     1480         0  \n",
       "4                          2.93      735         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(r\"C:\\Users\\Anil\\Desktop\\data_science\\62 Session 24-Aug-2021-20210824\\wine.csv\",header=0)\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cba8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    int64  \n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    int64  \n",
      " 13  WineType                      178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eaf2bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         126\n",
       "malic_acid                      133\n",
       "ash                              79\n",
       "alcalinity_of_ash                63\n",
       "magnesium                        53\n",
       "total_phenols                    97\n",
       "flavanoids                      132\n",
       "nonflavanoid_phenols             39\n",
       "proanthocyanins                 101\n",
       "color_intensity                 132\n",
       "hue                              78\n",
       "od280/od315_of_diluted_wines    122\n",
       "proline                         121\n",
       "WineType                          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150bef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent and Target (dependent) variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in wine.columns:\n",
    "    if col != 'WineType':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'WineType'\n",
    "\n",
    "x = wine[IndepVar]\n",
    "y = wine[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bff52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13), (142,), (36,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test (random sampling)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify=y, random_state=6)\n",
    "x_test_F1 = x_test.copy()\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1773e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train = mmscaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test = mmscaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3450cd7",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e76ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Confusion_Matrix:\n",
      " [[12  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "\n",
      "Class: 0\n",
      "Confusion_Matrix: \n",
      " [[24  0]\n",
      " [ 0 12]]\n",
      "Outcome_values:\n",
      " 12 0 0 24\n",
      "TP=12, FP=0, TN=24, FN=0\n",
      "Accuracy: 100.0 %\n",
      "Precision: 100.0 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 1.0\n",
      "Balanced_Accuracy: 100.0 %\n",
      "MCC: 1.0\n",
      "\n",
      "\n",
      "Class: 1\n",
      "Confusion_Matrix: \n",
      " [[22  0]\n",
      " [ 1 13]]\n",
      "Outcome_values:\n",
      " 13 1 0 22\n",
      "TP=13, FP=0, TN=22, FN=1\n",
      "Accuracy: 97.2 %\n",
      "Precision: 100.0 %\n",
      "Recall: 92.9 %\n",
      "F1_score: 0.963\n",
      "Balanced_Accuracy: 96.4 %\n",
      "MCC: 0.942\n",
      "\n",
      "\n",
      "Class: 2\n",
      "Confusion_Matrix: \n",
      " [[25  1]\n",
      " [ 0 10]]\n",
      "Outcome_values:\n",
      " 10 0 1 25\n",
      "TP=10, FP=1, TN=25, FN=0\n",
      "Accuracy: 97.2 %\n",
      "Precision: 90.9 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 0.952\n",
      "Balanced_Accuracy: 98.1 %\n",
      "MCC: 0.935\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Overall_evaluation_Metrics:\n",
      "\n",
      "Accuracy: 98.13 %\n",
      "Precision: 96.97 %\n",
      "Recall: 97.633 %\n",
      "F1_score: 0.972\n",
      "Balanced_Accuracy: 98.17 %\n",
      "MCC: 0.959\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier is used for classification problems\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "modelXGB = XGBClassifier()\n",
    "\n",
    "# Fit the model with train data\n",
    "\n",
    "modelXGB.fit(x_train, y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y_pred = modelXGB.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics for MultiClassification Analysis\n",
    "\n",
    "actual = y_test\n",
    "predict = y_pred\n",
    "\n",
    "# Giving number of classes to the labe_classes\n",
    "label_classes = [0,1,2]\n",
    "\n",
    "# printing the multiclass confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report,multilabel_confusion_matrix\n",
    "print(\"Confusion_Matrix:\\n\",confusion_matrix(actual,predict,labels = label_classes))\n",
    "\n",
    "matrix = multilabel_confusion_matrix(actual,predict,labels = label_classes) \n",
    "# creating a lists to append the metric values by each class for to calcualte overall evaluation metris\n",
    "avg_accuracy = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_f1score = []\n",
    "avg_balanced_accuracy = []\n",
    "avg_mcc = []\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in label_classes:\n",
    "    print(\"Class: {0}\".format(i))\n",
    "    # separating the true_positive and true_negative values from multilabel confusion_matrix\n",
    "    matrix_data = matrix[i]\n",
    "    tp = matrix_data[1][1]\n",
    "    fn = matrix_data[1][0]\n",
    "    fp = matrix_data[0][1]\n",
    "    tn = matrix_data[0][0]\n",
    "    \n",
    "    # printing the class wise confusion_matrix\n",
    "    print(\"Confusion_Matrix: \\n\",matrix_data)\n",
    "    \n",
    "    # printing the outcome values\n",
    "    print(\"Outcome_values:\\n\",tp,fn,fp,tn)\n",
    "    \n",
    "    # calculating the evaluation metrics\n",
    "    accuracy = round((tp+tn)/(tp+fn+fp+tn),3)\n",
    "    avg_accuracy.append(accuracy)\n",
    "    \n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    avg_precision.append(precision)\n",
    "    \n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    avg_recall.append(recall)\n",
    "    \n",
    "    f1_score = round((2*tp/(2*tp+fp+fn)),3)\n",
    "    avg_f1score.append(f1_score)\n",
    "    \n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    \n",
    "    balanced_accuracy = round((recall + specificity)/2,3)\n",
    "    avg_balanced_accuracy.append(balanced_accuracy)\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.\n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    # importing square root\n",
    "    from math import sqrt\n",
    "    n = (tp*tn)-(fp*fn)\n",
    "    d = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = round(n/d,3)\n",
    "    avg_mcc.append(mcc)\n",
    "    \n",
    "    # importing metrics from sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy:\",round(accuracy*100,2),\"%\")\n",
    "    print(\"Precision:\",round(precision*100,2),\"%\")\n",
    "    print(\"Recall:\",round(recall*100,3),\"%\")\n",
    "    print(\"F1_score:\",f1_score)\n",
    "    print(\"Balanced_Accuracy:\",round(balanced_accuracy*100,2),\"%\")\n",
    "    print(\"MCC:\",mcc)\n",
    "    print(\"\\n\")\n",
    "# printing the overall_evaluation_metrics\n",
    "# importing mean from statistics\n",
    "from statistics import mean\n",
    "print(\"-------------------------------------------------------------------\\n\")\n",
    "print(\"Overall_evaluation_Metrics:\\n\")\n",
    "print(\"Accuracy:\",round(mean(avg_accuracy)*100,2),\"%\")\n",
    "print(\"Precision:\",round(mean(avg_precision)*100,2),\"%\")\n",
    "print(\"Recall:\",round(mean(avg_recall)*100,3),\"%\")\n",
    "print(\"F1_score:\",round(mean(avg_f1score),3))\n",
    "print(\"Balanced_Accuracy:\",round(mean(avg_balanced_accuracy)*100,2),\"%\")\n",
    "print(\"MCC:\",mean(avg_mcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5028505",
   "metadata": {},
   "source": [
    "## GBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703a8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix:\n",
      " [[12  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "\n",
      "Class: 0\n",
      "Confusion_Matrix: \n",
      " [[24  0]\n",
      " [ 0 12]]\n",
      "Outcome_values:\n",
      " 12 0 0 24\n",
      "TP=12, FP=0, TN=24, FN=0\n",
      "Accuracy: 100.0 %\n",
      "Precision: 100.0 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 1.0\n",
      "Balanced_Accuracy: 100.0 %\n",
      "MCC: 1.0\n",
      "\n",
      "\n",
      "Class: 1\n",
      "Confusion_Matrix: \n",
      " [[22  0]\n",
      " [ 1 13]]\n",
      "Outcome_values:\n",
      " 13 1 0 22\n",
      "TP=13, FP=0, TN=22, FN=1\n",
      "Accuracy: 97.2 %\n",
      "Precision: 100.0 %\n",
      "Recall: 92.9 %\n",
      "F1_score: 0.963\n",
      "Balanced_Accuracy: 96.4 %\n",
      "MCC: 0.942\n",
      "\n",
      "\n",
      "Class: 2\n",
      "Confusion_Matrix: \n",
      " [[25  1]\n",
      " [ 0 10]]\n",
      "Outcome_values:\n",
      " 10 0 1 25\n",
      "TP=10, FP=1, TN=25, FN=0\n",
      "Accuracy: 97.2 %\n",
      "Precision: 90.9 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 0.952\n",
      "Balanced_Accuracy: 98.1 %\n",
      "MCC: 0.935\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Overall_evaluation_Metrics:\n",
      "\n",
      "Accuracy: 98.13 %\n",
      "Precision: 96.97 %\n",
      "Recall: 97.633 %\n",
      "F1_score: 0.972\n",
      "Balanced_Accuracy: 98.17 %\n",
      "MCC: 0.959\n"
     ]
    }
   ],
   "source": [
    "# GradientBoosting Classifier is used for classification problems\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0,\n",
    "criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "min_impurity_split=None, init=None, random_state=None, max_features=None,\n",
    "verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1,\n",
    "n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "# Fit the model with train data\n",
    "\n",
    "modelGBC.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y_pred = modelXGB.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics for MultiClassification Analysis\n",
    "\n",
    "actual = y_test\n",
    "predict = y_pred\n",
    "\n",
    "# Giving number of classes to the labe_classes\n",
    "label_classes = [0,1,2]\n",
    "\n",
    "# printing the multiclass confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report,multilabel_confusion_matrix\n",
    "print(\"Confusion_Matrix:\\n\",confusion_matrix(actual,predict,labels = label_classes))\n",
    "\n",
    "matrix = multilabel_confusion_matrix(actual,predict,labels = label_classes) \n",
    "# creating a lists to append the metric values by each class for to calcualte overall evaluation metris\n",
    "avg_accuracy = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_f1score = []\n",
    "avg_balanced_accuracy = []\n",
    "avg_mcc = []\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in label_classes:\n",
    "    print(\"Class: {0}\".format(i))\n",
    "    # separating the true_positive and true_negative values from multilabel confusion_matrix\n",
    "    matrix_data = matrix[i]\n",
    "    tp = matrix_data[1][1]\n",
    "    fn = matrix_data[1][0]\n",
    "    fp = matrix_data[0][1]\n",
    "    tn = matrix_data[0][0]\n",
    "    \n",
    "    # printing the class wise confusion_matrix\n",
    "    print(\"Confusion_Matrix: \\n\",matrix_data)\n",
    "    \n",
    "    # printing the outcome values\n",
    "    print(\"Outcome_values:\\n\",tp,fn,fp,tn)\n",
    "    \n",
    "    # calculating the evaluation metrics\n",
    "    accuracy = round((tp+tn)/(tp+fn+fp+tn),3)\n",
    "    avg_accuracy.append(accuracy)\n",
    "    \n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    avg_precision.append(precision)\n",
    "    \n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    avg_recall.append(recall)\n",
    "    \n",
    "    f1_score = round((2*tp/(2*tp+fp+fn)),3)\n",
    "    avg_f1score.append(f1_score)\n",
    "    \n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    \n",
    "    balanced_accuracy = round((recall + specificity)/2,3)\n",
    "    avg_balanced_accuracy.append(balanced_accuracy)\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.\n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    # importing square root\n",
    "    from math import sqrt\n",
    "    n = (tp*tn)-(fp*fn)\n",
    "    d = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = round(n/d,3)\n",
    "    avg_mcc.append(mcc)\n",
    "    \n",
    "    # importing metrics from sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy:\",round(accuracy*100,2),\"%\")\n",
    "    print(\"Precision:\",round(precision*100,2),\"%\")\n",
    "    print(\"Recall:\",round(recall*100,3),\"%\")\n",
    "    print(\"F1_score:\",f1_score)\n",
    "    print(\"Balanced_Accuracy:\",round(balanced_accuracy*100,2),\"%\")\n",
    "    print(\"MCC:\",mcc)\n",
    "    print(\"\\n\")\n",
    "# printing the overall_evaluation_metrics\n",
    "# importing mean from statistics\n",
    "from statistics import mean\n",
    "print(\"-------------------------------------------------------------------\\n\")\n",
    "print(\"Overall_evaluation_Metrics:\\n\")\n",
    "print(\"Accuracy:\",round(mean(avg_accuracy)*100,2),\"%\")\n",
    "print(\"Precision:\",round(mean(avg_precision)*100,2),\"%\")\n",
    "print(\"Recall:\",round(mean(avg_recall)*100,3),\"%\")\n",
    "print(\"F1_score:\",round(mean(avg_f1score),3))\n",
    "print(\"Balanced_Accuracy:\",round(mean(avg_balanced_accuracy)*100,2),\"%\")\n",
    "print(\"MCC:\",mean(avg_mcc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14ce6a",
   "metadata": {},
   "source": [
    "##  Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081f4f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix:\n",
      " [[12  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "\n",
      "Class: 0\n",
      "Confusion_Matrix: \n",
      " [[24  0]\n",
      " [ 0 12]]\n",
      "Outcome_values:\n",
      " 12 0 0 24\n",
      "TP=12, FP=0, TN=24, FN=0\n",
      "Accuracy: 100.0 %\n",
      "Precision: 100.0 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 1.0\n",
      "Balanced_Accuracy: 100.0 %\n",
      "MCC: 1.0\n",
      "\n",
      "\n",
      "Class: 1\n",
      "Confusion_Matrix: \n",
      " [[22  0]\n",
      " [ 1 13]]\n",
      "Outcome_values:\n",
      " 13 1 0 22\n",
      "TP=13, FP=0, TN=22, FN=1\n",
      "Accuracy: 97.2 %\n",
      "Precision: 100.0 %\n",
      "Recall: 92.9 %\n",
      "F1_score: 0.963\n",
      "Balanced_Accuracy: 96.4 %\n",
      "MCC: 0.942\n",
      "\n",
      "\n",
      "Class: 2\n",
      "Confusion_Matrix: \n",
      " [[25  1]\n",
      " [ 0 10]]\n",
      "Outcome_values:\n",
      " 10 0 1 25\n",
      "TP=10, FP=1, TN=25, FN=0\n",
      "Accuracy: 97.2 %\n",
      "Precision: 90.9 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 0.952\n",
      "Balanced_Accuracy: 98.1 %\n",
      "MCC: 0.935\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Overall_evaluation_Metrics:\n",
      "\n",
      "Accuracy: 98.13 %\n",
      "Precision: 96.97 %\n",
      "Recall: 97.633 %\n",
      "F1_score: 0.972\n",
      "Balanced_Accuracy: 98.17 %\n",
      "MCC: 0.959\n"
     ]
    }
   ],
   "source": [
    "# Light GBM Classifier is used for classification problems\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "modelLGBM = LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=100,\n",
    "subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0,\n",
    "min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0,\n",
    "colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=- 1,\n",
    "silent=True, importance_type='split')\n",
    "\n",
    "# Fit the model with train data\n",
    "\n",
    "modelLGBM.fit(x_train, y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y_pred = modelXGB.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics for MultiClassification Analysis\n",
    "\n",
    "actual = y_test\n",
    "predict = y_pred\n",
    "\n",
    "# Giving number of classes to the labe_classes\n",
    "label_classes = [0,1,2]\n",
    "\n",
    "# printing the multiclass confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report,multilabel_confusion_matrix\n",
    "print(\"Confusion_Matrix:\\n\",confusion_matrix(actual,predict,labels = label_classes))\n",
    "\n",
    "matrix = multilabel_confusion_matrix(actual,predict,labels = label_classes) \n",
    "# creating a lists to append the metric values by each class for to calcualte overall evaluation metris\n",
    "avg_accuracy = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_f1score = []\n",
    "avg_balanced_accuracy = []\n",
    "avg_mcc = []\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in label_classes:\n",
    "    print(\"Class: {0}\".format(i))\n",
    "    # separating the true_positive and true_negative values from multilabel confusion_matrix\n",
    "    matrix_data = matrix[i]\n",
    "    tp = matrix_data[1][1]\n",
    "    fn = matrix_data[1][0]\n",
    "    fp = matrix_data[0][1]\n",
    "    tn = matrix_data[0][0]\n",
    "    \n",
    "    # printing the class wise confusion_matrix\n",
    "    print(\"Confusion_Matrix: \\n\",matrix_data)\n",
    "    \n",
    "    # printing the outcome values\n",
    "    print(\"Outcome_values:\\n\",tp,fn,fp,tn)\n",
    "    \n",
    "    # calculating the evaluation metrics\n",
    "    accuracy = round((tp+tn)/(tp+fn+fp+tn),3)\n",
    "    avg_accuracy.append(accuracy)\n",
    "    \n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    avg_precision.append(precision)\n",
    "    \n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    avg_recall.append(recall)\n",
    "    \n",
    "    f1_score = round((2*tp/(2*tp+fp+fn)),3)\n",
    "    avg_f1score.append(f1_score)\n",
    "    \n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    \n",
    "    balanced_accuracy = round((recall + specificity)/2,3)\n",
    "    avg_balanced_accuracy.append(balanced_accuracy)\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.\n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    # importing square root\n",
    "    from math import sqrt\n",
    "    n = (tp*tn)-(fp*fn)\n",
    "    d = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = round(n/d,3)\n",
    "    avg_mcc.append(mcc)\n",
    "    \n",
    "    # importing metrics from sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy:\",round(accuracy*100,2),\"%\")\n",
    "    print(\"Precision:\",round(precision*100,2),\"%\")\n",
    "    print(\"Recall:\",round(recall*100,3),\"%\")\n",
    "    print(\"F1_score:\",f1_score)\n",
    "    print(\"Balanced_Accuracy:\",round(balanced_accuracy*100,2),\"%\")\n",
    "    print(\"MCC:\",mcc)\n",
    "    print(\"\\n\")\n",
    "# printing the overall_evaluation_metrics\n",
    "# importing mean from statistics\n",
    "from statistics import mean\n",
    "print(\"-------------------------------------------------------------------\\n\")\n",
    "print(\"Overall_evaluation_Metrics:\\n\")\n",
    "print(\"Accuracy:\",round(mean(avg_accuracy)*100,2),\"%\")\n",
    "print(\"Precision:\",round(mean(avg_precision)*100,2),\"%\")\n",
    "print(\"Recall:\",round(mean(avg_recall)*100,3),\"%\")\n",
    "print(\"F1_score:\",round(mean(avg_f1score),3))\n",
    "print(\"Balanced_Accuracy:\",round(mean(avg_balanced_accuracy)*100,2),\"%\")\n",
    "print(\"MCC:\",mean(avg_mcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695467ba",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb13efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix:\n",
      " [[12  0  0]\n",
      " [ 1 12  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "\n",
      "Class: 0\n",
      "Confusion_Matrix: \n",
      " [[23  1]\n",
      " [ 0 12]]\n",
      "Outcome_values:\n",
      " 12 0 1 23\n",
      "TP=12, FP=1, TN=23, FN=0\n",
      "Accuracy: 97.2 %\n",
      "Precision: 92.3 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 0.96\n",
      "Balanced_Accuracy: 97.9 %\n",
      "MCC: 0.941\n",
      "\n",
      "\n",
      "Class: 1\n",
      "Confusion_Matrix: \n",
      " [[22  0]\n",
      " [ 2 12]]\n",
      "Outcome_values:\n",
      " 12 2 0 22\n",
      "TP=12, FP=0, TN=22, FN=2\n",
      "Accuracy: 94.4 %\n",
      "Precision: 100.0 %\n",
      "Recall: 85.7 %\n",
      "F1_score: 0.923\n",
      "Balanced_Accuracy: 92.8 %\n",
      "MCC: 0.886\n",
      "\n",
      "\n",
      "Class: 2\n",
      "Confusion_Matrix: \n",
      " [[25  1]\n",
      " [ 0 10]]\n",
      "Outcome_values:\n",
      " 10 0 1 25\n",
      "TP=10, FP=1, TN=25, FN=0\n",
      "Accuracy: 97.2 %\n",
      "Precision: 90.9 %\n",
      "Recall: 100.0 %\n",
      "F1_score: 0.952\n",
      "Balanced_Accuracy: 98.1 %\n",
      "MCC: 0.935\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Overall_evaluation_Metrics:\n",
      "\n",
      "Accuracy: 96.27 %\n",
      "Precision: 94.4 %\n",
      "Recall: 95.233 %\n",
      "F1_score: 0.945\n",
      "Balanced_Accuracy: 96.27 %\n",
      "MCC: 0.9206666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLR = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                             intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', \n",
    "                             max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "modelLR.fit(x_train,y_train)\n",
    "\n",
    "y_pred = modelLR.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics for MultiClassification Analysis\n",
    "\n",
    "actual = y_test\n",
    "predict = y_pred\n",
    "\n",
    "# Giving number of classes to the labe_classes\n",
    "label_classes = [0,1,2]\n",
    "\n",
    "# printing the multiclass confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report,multilabel_confusion_matrix\n",
    "print(\"Confusion_Matrix:\\n\",confusion_matrix(actual,predict,labels = label_classes))\n",
    "\n",
    "matrix = multilabel_confusion_matrix(actual,predict,labels = label_classes) \n",
    "# creating a lists to append the metric values by each class for to calcualte overall evaluation metris\n",
    "avg_accuracy = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_f1score = []\n",
    "avg_balanced_accuracy = []\n",
    "avg_mcc = []\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in label_classes:\n",
    "    print(\"Class: {0}\".format(i))\n",
    "    # separating the true_positive and true_negative values from multilabel confusion_matrix\n",
    "    matrix_data = matrix[i]\n",
    "    tp = matrix_data[1][1]\n",
    "    fn = matrix_data[1][0]\n",
    "    fp = matrix_data[0][1]\n",
    "    tn = matrix_data[0][0]\n",
    "    \n",
    "    # printing the class wise confusion_matrix\n",
    "    print(\"Confusion_Matrix: \\n\",matrix_data)\n",
    "    \n",
    "    # printing the outcome values\n",
    "    print(\"Outcome_values:\\n\",tp,fn,fp,tn)\n",
    "    \n",
    "    # calculating the evaluation metrics\n",
    "    accuracy = round((tp+tn)/(tp+fn+fp+tn),3)\n",
    "    avg_accuracy.append(accuracy)\n",
    "    \n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    avg_precision.append(precision)\n",
    "    \n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    avg_recall.append(recall)\n",
    "    \n",
    "    f1_score = round((2*tp/(2*tp+fp+fn)),3)\n",
    "    avg_f1score.append(f1_score)\n",
    "    \n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    \n",
    "    balanced_accuracy = round((recall + specificity)/2,3)\n",
    "    avg_balanced_accuracy.append(balanced_accuracy)\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.\n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    # importing square root\n",
    "    from math import sqrt\n",
    "    n = (tp*tn)-(fp*fn)\n",
    "    d = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = round(n/d,3)\n",
    "    avg_mcc.append(mcc)\n",
    "    \n",
    "    # importing metrics from sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy:\",round(accuracy*100,2),\"%\")\n",
    "    print(\"Precision:\",round(precision*100,2),\"%\")\n",
    "    print(\"Recall:\",round(recall*100,3),\"%\")\n",
    "    print(\"F1_score:\",f1_score)\n",
    "    print(\"Balanced_Accuracy:\",round(balanced_accuracy*100,2),\"%\")\n",
    "    print(\"MCC:\",mcc)\n",
    "    print(\"\\n\")\n",
    "# printing the overall_evaluation_metrics\n",
    "# importing mean from statistics\n",
    "from statistics import mean\n",
    "print(\"-------------------------------------------------------------------\\n\")\n",
    "print(\"Overall_evaluation_Metrics:\\n\")\n",
    "print(\"Accuracy:\",round(mean(avg_accuracy)*100,2),\"%\")\n",
    "print(\"Precision:\",round(mean(avg_precision)*100,2),\"%\")\n",
    "print(\"Recall:\",round(mean(avg_recall)*100,3),\"%\")\n",
    "print(\"F1_score:\",round(mean(avg_f1score),3))\n",
    "print(\"Balanced_Accuracy:\",round(mean(avg_balanced_accuracy)*100,2),\"%\")\n",
    "print(\"MCC:\",mean(avg_mcc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
