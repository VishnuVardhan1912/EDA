{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Import psql to run queries \n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>TumorType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  radius error  texture error  perimeter error  \\\n",
       "0                 0.07871        1.0950         0.9053            8.589   \n",
       "1                 0.05667        0.5435         0.7339            3.398   \n",
       "2                 0.05999        0.7456         0.7869            4.585   \n",
       "3                 0.09744        0.4956         1.1560            3.445   \n",
       "4                 0.05883        0.7572         0.7813            5.438   \n",
       "\n",
       "   area error  smoothness error  compactness error  concavity error  \\\n",
       "0      153.40          0.006399            0.04904          0.05373   \n",
       "1       74.08          0.005225            0.01308          0.01860   \n",
       "2       94.03          0.006150            0.04006          0.03832   \n",
       "3       27.23          0.009110            0.07458          0.05661   \n",
       "4       94.44          0.011490            0.02461          0.05688   \n",
       "\n",
       "   concave points error  symmetry error  fractal dimension error  \\\n",
       "0               0.01587         0.03003                 0.006193   \n",
       "1               0.01340         0.01389                 0.003532   \n",
       "2               0.02058         0.02250                 0.004571   \n",
       "3               0.01867         0.05963                 0.009208   \n",
       "4               0.01885         0.01756                 0.005115   \n",
       "\n",
       "   worst radius  worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0         25.38          17.33           184.60      2019.0            0.1622   \n",
       "1         24.99          23.41           158.80      1956.0            0.1238   \n",
       "2         23.57          25.53           152.50      1709.0            0.1444   \n",
       "3         14.91          26.50            98.87       567.7            0.2098   \n",
       "4         22.54          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   worst fractal dimension  TumorType  \n",
       "0                  0.11890          0  \n",
       "1                  0.08902          0  \n",
       "2                  0.08758          0  \n",
       "3                  0.17300          0  \n",
       "4                  0.07678          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the cancer dataset \n",
    "\n",
    "cancer = pd.read_csv(r\"D:\\00 Datasets\\Others\\Data-08\\breast_cancer.csv\", header=0) \n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  TumorType                569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Display the informationof dataset\n",
    "\n",
    "cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                456\n",
       "mean texture               479\n",
       "mean perimeter             522\n",
       "mean area                  539\n",
       "mean smoothness            474\n",
       "mean compactness           537\n",
       "mean concavity             537\n",
       "mean concave points        542\n",
       "mean symmetry              432\n",
       "mean fractal dimension     499\n",
       "radius error               540\n",
       "texture error              519\n",
       "perimeter error            533\n",
       "area error                 528\n",
       "smoothness error           547\n",
       "compactness error          541\n",
       "concavity error            533\n",
       "concave points error       507\n",
       "symmetry error             498\n",
       "fractal dimension error    545\n",
       "worst radius               457\n",
       "worst texture              511\n",
       "worst perimeter            514\n",
       "worst area                 544\n",
       "worst smoothness           411\n",
       "worst compactness          529\n",
       "worst concavity            539\n",
       "worst concave points       492\n",
       "worst symmetry             500\n",
       "worst fractal dimension    535\n",
       "TumorType                    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the unique variables count\n",
    "\n",
    "cancer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent and Target (dependent) variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in cancer.columns:\n",
    "    if col != 'TumorType':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'TumorType'\n",
    "\n",
    "x = cancer[IndepVar]\n",
    "y = cancer[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455,), (114,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test (random sampling)\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify=y, random_state=42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train = mmscaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test = mmscaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingClassifier is used for classification problems\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "                                      subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n",
    "                                      warm_start=False, validation_fraction=0.1, n_iter_no_change=None, \n",
    "                                      tol=0.0001, ccp_alpha=0.0)\n",
    "# fit the model with train data\n",
    "\n",
    "modelGBC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[72  0]\n",
      " [ 4 38]]\n",
      "Outcome values : \n",
      " 72 0 4 38\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      1.00      0.97        72\n",
      "           0       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "Accuracy : 96.5 %\n",
      "Precision : 94.7 %\n",
      "Recall : 100.0 %\n",
      "F1 Score : 0.973\n",
      "Balanced Accuracy : 95.2 %\n",
      "MCC : 0.926\n",
      "roc_auc_score: 0.952\n"
     ]
    }
   ],
   "source": [
    "# Predict the model with test data\n",
    "\n",
    "y_pred = modelGBC.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[71  1]\n",
      " [ 4 38]]\n",
      "Outcome values : \n",
      " 71 1 4 38\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.99      0.97        72\n",
      "           0       0.97      0.90      0.94        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Accuracy : 95.6 %\n",
      "Precision : 94.7 %\n",
      "Recall : 98.6 %\n",
      "F1 Score : 0.966\n",
      "Balanced Accuracy : 94.6 %\n",
      "MCC : 0.906\n",
      "roc_auc_score: 0.945\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "modelRF = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                                 criterion='entropy', max_depth=None, max_features='auto',\n",
    "                                 max_leaf_nodes=None, max_samples=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=1, min_samples_split=2,\n",
    "                                 min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                 n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "                                 warm_start=False)\n",
    "\n",
    "modelRF = modelRF.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = modelRF.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y1_pred \n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))\n",
    "print('-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier (Multi-Class Classification- Wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>WineType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  WineType  \n",
       "0                          3.92     1065         0  \n",
       "1                          3.40     1050         0  \n",
       "2                          3.17     1185         0  \n",
       "3                          3.45     1480         0  \n",
       "4                          2.93      735         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the power dataset \n",
    "\n",
    "wine = pd.read_csv(r\"D:\\00 Datasets\\Others\\Data-08\\wine.csv\", header=0) \n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    int64  \n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    int64  \n",
      " 13  WineType                      178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "# display the dataset information\n",
    "\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         126\n",
       "malic_acid                      133\n",
       "ash                              79\n",
       "alcalinity_of_ash                63\n",
       "magnesium                        53\n",
       "total_phenols                    97\n",
       "flavanoids                      132\n",
       "nonflavanoid_phenols             39\n",
       "proanthocyanins                 101\n",
       "color_intensity                 132\n",
       "hue                              78\n",
       "od280/od315_of_diluted_wines    122\n",
       "proline                         121\n",
       "WineType                          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of unique values count in each variable\n",
    "\n",
    "wine.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent and Target (dependent) variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in wine.columns:\n",
    "    if col != 'WineType':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'WineType'\n",
    "\n",
    "x = wine[IndepVar]\n",
    "y = wine[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13), (142,), (36,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test (random sampling)\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify=y, random_state=42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train = mmscaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test = mmscaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingClassifier is used for classification problems\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBMC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "                                       subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "                                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                       min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n",
    "                                       random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n",
    "                                       warm_start=False, validation_fraction=0.1, n_iter_no_change=None, \n",
    "                                       tol=0.0001, ccp_alpha=0.0)\n",
    "# fit the model with train data\n",
    "\n",
    "modelGBMC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1  0]\n",
      " [ 2 13  0]\n",
      " [ 0  0 10]]\n",
      "Print Class: 0\n",
      "TP=10, FP=1, TN=23, FN=2\n",
      "Accuracy: 0.917\n",
      "Precision: 0.909\n",
      "Sensitivity: 0.833\n",
      "F1-Score: 0.87\n",
      "Specificity: 0.958\n",
      "Balanced Accuracy: 0.896\n",
      "MCC: 0.81\n",
      "\n",
      "Print Class: 1\n",
      "TP=13, FP=2, TN=20, FN=1\n",
      "Accuracy: 0.917\n",
      "Precision: 0.867\n",
      "Sensitivity: 0.929\n",
      "F1-Score: 0.897\n",
      "Specificity: 0.909\n",
      "Balanced Accuracy: 0.919\n",
      "MCC: 0.828\n",
      "\n",
      "Print Class: 2\n",
      "TP=10, FP=0, TN=26, FN=0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 1.0\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "MCC: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict model with test data \n",
    "\n",
    "y_pred = modelGBMC.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y_pred\n",
    "\n",
    "# Class = Label 0-3\n",
    "\n",
    "lst_classes = [0, 1, 2]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "    \n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy));    # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 94.4667%\n",
      "Precision: 92.5333%\n",
      "Recall or Sensitivity: 92.0667%\n",
      "F1-Score: 0.9223\n",
      "Specificity or True Nagative Rate: 95.5667%\n",
      "Balanced Accuracy: 93.8333%\n",
      "\n",
      "MCC: 0.8793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean() \n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "modelRF = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                                 criterion='entropy', max_depth=None, max_features='auto',\n",
    "                                 max_leaf_nodes=None, max_samples=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=1, min_samples_split=2,\n",
    "                                 min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                 n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "                                 warm_start=False)\n",
    "# Fit the model with train data\n",
    "\n",
    "modelRF = modelRF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 1 14  0]\n",
      " [ 0  0 10]]\n",
      "Print Class: 0\n",
      "TP=11, FP=0, TN=24, FN=1\n",
      "Accuracy: 0.972\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.917\n",
      "F1-Score: 0.957\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.958\n",
      "MCC: 0.938\n",
      "\n",
      "Print Class: 1\n",
      "TP=14, FP=1, TN=21, FN=0\n",
      "Accuracy: 0.972\n",
      "Precision: 0.933\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 0.966\n",
      "Specificity: 0.955\n",
      "Balanced Accuracy: 0.978\n",
      "MCC: 0.944\n",
      "\n",
      "Print Class: 2\n",
      "TP=10, FP=0, TN=26, FN=0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 1.0\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "MCC: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict model with test data \n",
    "\n",
    "y_pred = modelRF.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y_pred\n",
    "\n",
    "# Class = Label 0-3\n",
    "\n",
    "lst_classes = [0, 1, 2]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "    \n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy));    # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 98.1333%\n",
      "Precision: 97.7667%\n",
      "Recall or Sensitivity: 97.2333%\n",
      "F1-Score: 0.9743\n",
      "Specificity or True Nagative Rate: 98.5%\n",
      "Balanced Accuracy: 97.8667%\n",
      "\n",
      "MCC: 0.9607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean() \n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for GradientBoostingClassifier\n",
    "\n",
    "parameters = {'learning_rate': [0.01,0.1,0.3,1],\n",
    "              'n_estimators' : [5,50,250, 500],\n",
    "              'max_depth'    : [1,3,5,7,9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier model and Fine tune hyper parametrs\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBMC = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search - perform 2 fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "modelGBMC_GS = GridSearchCV(estimator = modelGBMC, param_grid = parameters, cv = 2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.3, 1],\n",
       "                         'max_depth': [1, 3, 5, 7, 9],\n",
       "                         'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with train data (longer time)\n",
    "\n",
    "modelGBMC_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingClassifier(learning_rate=0.3, max_depth=1, n_estimators=50)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.9577464788732395\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", modelGBMC_GS.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", modelGBMC_GS.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", modelGBMC_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 1 14  0]\n",
      " [ 0  0 10]]\n",
      "Print Class: 0\n",
      "TP=11, FP=0, TN=24, FN=1\n",
      "Accuracy: 0.972\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.917\n",
      "F1-Score: 0.957\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.958\n",
      "MCC: 0.938\n",
      "\n",
      "Print Class: 1\n",
      "TP=14, FP=1, TN=21, FN=0\n",
      "Accuracy: 0.972\n",
      "Precision: 0.933\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 0.966\n",
      "Specificity: 0.955\n",
      "Balanced Accuracy: 0.978\n",
      "MCC: 0.944\n",
      "\n",
      "Print Class: 2\n",
      "TP=10, FP=0, TN=26, FN=0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 1.0\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "MCC: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict model with test data \n",
    "\n",
    "y_pred = modelGBMC_GS.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y_pred\n",
    "\n",
    "# Class = Label 0-3\n",
    "\n",
    "lst_classes = [0, 1, 2]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "    \n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy));    # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 98.1333%\n",
      "Precision: 97.7667%\n",
      "Recall or Sensitivity: 97.2333%\n",
      "F1-Score: 0.9743\n",
      "Specificity or True Nagative Rate: 98.5%\n",
      "Balanced Accuracy: 97.8667%\n",
      "\n",
      "MCC: 0.9607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean() \n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500], 'learning_rate': [0.01, 0.06210526315789474, 0.11421052631578947, 0.16631578947368422, 0.21842105263157896, 0.2705263157894737, 0.32263157894736844, 0.37473684210526315, 0.4268421052631579, 0.4789473684210527, 0.5310526315789474, 0.5831578947368421, 0.6352631578947369, 0.6873684210526316, 0.7394736842105263, 0.791578947368421, 0.8436842105263158, 0.8957894736842106, 0.9478947368421053, 1.0], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, None]}\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Model using RandomizedSearchCV \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50 , stop = 500, num = 10)] # returns 10 numbers \n",
    "\n",
    "learning_rate = [float(x) for x in np.linspace(start = 0.01 , stop = 1, num = 20)]\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(1, 9, num = 9)] \n",
    "\n",
    "max_depth.append(None)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators, 'learning_rate': learning_rate,\n",
    "          'max_depth': max_depth}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 450, 'max_depth': 1, 'learning_rate': 0.9478947368421053}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBMC1 = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search - perform 2 fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "modelGBMC1_GS = RandomizedSearchCV(estimator = modelGBMC1, param_distributions=random_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "modelGBMC1_GS.fit(x_train, y_train)\n",
    "\n",
    "print(modelGBMC1_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 1 14  0]\n",
      " [ 0  0 10]]\n",
      "Print Class: 0\n",
      "TP=11, FP=0, TN=24, FN=1\n",
      "Accuracy: 0.972\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.917\n",
      "F1-Score: 0.957\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.958\n",
      "MCC: 0.938\n",
      "\n",
      "Print Class: 1\n",
      "TP=14, FP=1, TN=21, FN=0\n",
      "Accuracy: 0.972\n",
      "Precision: 0.933\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 0.966\n",
      "Specificity: 0.955\n",
      "Balanced Accuracy: 0.978\n",
      "MCC: 0.944\n",
      "\n",
      "Print Class: 2\n",
      "TP=10, FP=0, TN=26, FN=0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 1.0\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "MCC: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict model with test data \n",
    "\n",
    "y_pred = modelGBMC1_GS.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y_pred\n",
    "\n",
    "# Class = Label 0-3\n",
    "\n",
    "lst_classes = [0, 1, 2]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "    \n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy));    # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 98.1333%\n",
      "Precision: 97.7667%\n",
      "Recall or Sensitivity: 97.2333%\n",
      "F1-Score: 0.9743\n",
      "Specificity or True Nagative Rate: 98.5%\n",
      "Balanced Accuracy: 97.8667%\n",
      "\n",
      "MCC: 0.9607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean() \n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification for Universal Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0   1   25           1      49     91107       4    1.6          1         0   \n",
       "1   2   45          19      34     90089       3    1.5          1         0   \n",
       "2   3   39          15      11     94720       1    1.0          1         0   \n",
       "3   4   35           9     100     94112       1    2.7          2         0   \n",
       "4   5   35           8      45     91330       4    1.0          2         0   \n",
       "\n",
       "   Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "0              0                   1           0       0           0  \n",
       "1              0                   1           0       0           0  \n",
       "2              0                   0           0       0           0  \n",
       "3              0                   0           0       0           0  \n",
       "4              0                   0           0       0           1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Universal bank data\n",
    "\n",
    "bankdata = pd.read_csv(r\"D:\\00 Datasets\\Others\\Data-06\\Universalbank.csv\", header=0) \n",
    "bankdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the columns which are not influencing the target variable\n",
    "\n",
    "del bankdata['ID']\n",
    "del bankdata['ZIP Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols1 is variables - crating a dummy variables & cols2 variables - MinMaxScalar function\n",
    "\n",
    "cols1 = ['Family', 'Education']\n",
    "cols2 = ['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>25.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experience</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>49.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCAvg</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mortgage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Loan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Securities Account</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD Account</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditCard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0     1     2      3     4\n",
       "Age                 25.0  45.0  39.0   35.0  35.0\n",
       "Experience           1.0  19.0  15.0    9.0   8.0\n",
       "Income              49.0  34.0  11.0  100.0  45.0\n",
       "CCAvg                1.6   1.5   1.0    2.7   1.0\n",
       "Mortgage             0.0   0.0   0.0    0.0   0.0\n",
       "Personal Loan        0.0   0.0   0.0    0.0   0.0\n",
       "Securities Account   1.0   1.0   0.0    0.0   0.0\n",
       "CD Account           0.0   0.0   0.0    0.0   0.0\n",
       "Online               0.0   0.0   0.0    0.0   0.0\n",
       "CreditCard           0.0   0.0   0.0    0.0   1.0\n",
       "Family_1             0.0   0.0   1.0    1.0   0.0\n",
       "Family_2             0.0   0.0   0.0    0.0   0.0\n",
       "Family_3             0.0   1.0   0.0    0.0   0.0\n",
       "Family_4             1.0   0.0   0.0    0.0   1.0\n",
       "Education_1          1.0   1.0   1.0    0.0   0.0\n",
       "Education_2          0.0   0.0   0.0    1.0   1.0\n",
       "Education_3          0.0   0.0   0.0    0.0   0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variable for all range values\n",
    "\n",
    "bankdata = pd.get_dummies(bankdata, columns=cols1)\n",
    "bankdata.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent and Target variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in bankdata.columns:\n",
    "    if col != 'CreditCard':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'CreditCard'\n",
    "\n",
    "x = bankdata[IndepVar]\n",
    "y = bankdata[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 16), (1500, 16), (3500,), (1500,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into train and test \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, stratify=y, random_state=142)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train[cols2] = mmscaler.fit_transform(x_train[cols2])\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test[cols2] = mmscaler.fit_transform(x_test[cols2])\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  79  362]\n",
      " [  21 1038]]\n",
      "Outcome values : \n",
      " 79 362 21 1038\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.18      0.29       441\n",
      "           0       0.74      0.98      0.84      1059\n",
      "\n",
      "    accuracy                           0.74      1500\n",
      "   macro avg       0.77      0.58      0.57      1500\n",
      "weighted avg       0.76      0.74      0.68      1500\n",
      "\n",
      "Accuracy : 74.5 %\n",
      "Precision : 79.0 %\n",
      "Recall : 17.9 %\n",
      "F1 Score : 0.292\n",
      "Balanced Accuracy : 58.0 %\n",
      "MCC : 0.291\n",
      "roc_auc_score: 0.58\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier is used for classification problems\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "                                      subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n",
    "                                      warm_start=False, validation_fraction=0.1, n_iter_no_change=None, \n",
    "                                      tol=0.0001, ccp_alpha=0.0)\n",
    "# Fit the model with train data\n",
    "\n",
    "modelGBC.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data\n",
    "\n",
    "y_pred = modelGBC.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[116 325]\n",
      " [108 951]]\n",
      "Outcome values : \n",
      " 116 325 108 951\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.26      0.35       441\n",
      "           0       0.75      0.90      0.81      1059\n",
      "\n",
      "    accuracy                           0.71      1500\n",
      "   macro avg       0.63      0.58      0.58      1500\n",
      "weighted avg       0.68      0.71      0.68      1500\n",
      "\n",
      "Accuracy : 71.1 %\n",
      "Precision : 51.8 %\n",
      "Recall : 26.3 %\n",
      "F1 Score : 0.349\n",
      "Balanced Accuracy : 58.0 %\n",
      "MCC : 0.206\n",
      "roc_auc_score: 0.581\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "modelRF = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                                 criterion='entropy', max_depth=None, max_features='auto',\n",
    "                                 max_leaf_nodes=None, max_samples=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=1, min_samples_split=2,\n",
    "                                 min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                 n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "                                 warm_start=False)\n",
    "\n",
    "modelRF = modelRF.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = modelRF.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y1_pred \n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))\n",
    "print('-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[116 325]\n",
      " [108 951]]\n",
      "Outcome values : \n",
      " 116 325 108 951\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.26      0.35       441\n",
      "           0       0.75      0.90      0.81      1059\n",
      "\n",
      "    accuracy                           0.71      1500\n",
      "   macro avg       0.63      0.58      0.58      1500\n",
      "weighted avg       0.68      0.71      0.68      1500\n",
      "\n",
      "Accuracy : 71.1 %\n",
      "Precision : 51.8 %\n",
      "Recall : 26.3 %\n",
      "F1 Score : 0.349\n",
      "Balanced Accuracy : 58.0 %\n",
      "MCC : 0.206\n",
      "roc_auc_score: 0.581\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "\n",
    "modelLR = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                             intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                             solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, \n",
    "                             warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "modelLR = modelRF.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = modelLR.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y1_pred \n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))\n",
    "print('-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500], 'learning_rate': [0.01, 0.06210526315789474, 0.11421052631578947, 0.16631578947368422, 0.21842105263157896, 0.2705263157894737, 0.32263157894736844, 0.37473684210526315, 0.4268421052631579, 0.4789473684210527, 0.5310526315789474, 0.5831578947368421, 0.6352631578947369, 0.6873684210526316, 0.7394736842105263, 0.791578947368421, 0.8436842105263158, 0.8957894736842106, 0.9478947368421053, 1.0], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, None]}\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Model using RandomizedSearchCV \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50 , stop = 500, num = 10)] # returns 10 numbers \n",
    "\n",
    "learning_rate = [float(x) for x in np.linspace(start = 0.01 , stop = 1, num = 20)]\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(1, 9, num = 9)] \n",
    "\n",
    "max_depth.append(None)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 450, 'max_depth': 1, 'learning_rate': 0.9478947368421053}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBC1 = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search - perform 2 fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "modelGBC1_GS = RandomizedSearchCV(estimator = modelGBC1, param_distributions=random_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "modelGBC1_GS.fit(x_train, y_train)\n",
    "\n",
    "print(modelGBMC1_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  83  358]\n",
      " [  27 1032]]\n",
      "Outcome values : \n",
      " 83 358 27 1032\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.19      0.30       441\n",
      "           0       0.74      0.97      0.84      1059\n",
      "\n",
      "    accuracy                           0.74      1500\n",
      "   macro avg       0.75      0.58      0.57      1500\n",
      "weighted avg       0.75      0.74      0.68      1500\n",
      "\n",
      "Accuracy : 74.3 %\n",
      "Precision : 75.5 %\n",
      "Recall : 18.8 %\n",
      "F1 Score : 0.301\n",
      "Balanced Accuracy : 58.2 %\n",
      "MCC : 0.284\n",
      "roc_auc_score: 0.581\n"
     ]
    }
   ],
   "source": [
    "# Predict the model with test data\n",
    "\n",
    "y1_pred = modelGBC1_GS.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y1_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for GradientBoostingClassifier\n",
    "\n",
    "parameters = {'learning_rate': [0.01,0.1,0.3,1,10],\n",
    "              'n_estimators' : [5,50,250, 500],\n",
    "              'max_depth'    : [1,3,5,7,9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "modelGBC = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search - perform 2 fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "modelGBC_GS = GridSearchCV(estimator = modelGBC, param_grid = parameters, cv = 5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.3, 1, 10],\n",
       "                         'max_depth': [1, 3, 5, 7, 9],\n",
       "                         'n_estimators': [5, 50, 250, 500]})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with train data (longer time)\n",
    "\n",
    "modelGBC_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " GradientBoostingClassifier(n_estimators=50)\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7417142857142857\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", modelGBC_GS.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", modelGBC_GS.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", modelGBC_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  77  364]\n",
      " [  17 1042]]\n",
      "Outcome values : \n",
      " 77 364 17 1042\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.17      0.29       441\n",
      "           0       0.74      0.98      0.85      1059\n",
      "\n",
      "    accuracy                           0.75      1500\n",
      "   macro avg       0.78      0.58      0.57      1500\n",
      "weighted avg       0.76      0.75      0.68      1500\n",
      "\n",
      "Accuracy : 74.6 %\n",
      "Precision : 81.9 %\n",
      "Recall : 17.5 %\n",
      "F1 Score : 0.288\n",
      "Balanced Accuracy : 58.0 %\n",
      "MCC : 0.298\n",
      "roc_auc_score: 0.579\n"
     ]
    }
   ],
   "source": [
    "# Predict the model with test data\n",
    "\n",
    "y_pred = modelGBC_GS.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n',matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3)\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
