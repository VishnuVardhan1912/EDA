{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ba5bb4",
   "metadata": {},
   "source": [
    "# loan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837138e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandasql as psql\n",
    "\n",
    "# pip install pandasql\n",
    "\n",
    "# import datetime class from datetime module\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4fdcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>not.fully.paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit.policy             purpose  int.rate  installment  log.annual.inc  \\\n",
       "0              1  debt_consolidation    0.1189       829.10       11.350407   \n",
       "1              1         credit_card    0.1071       228.22       11.082143   \n",
       "2              1  debt_consolidation    0.1357       366.86       10.373491   \n",
       "3              1  debt_consolidation    0.1008       162.34       11.350407   \n",
       "4              1         credit_card    0.1426       102.92       11.299732   \n",
       "\n",
       "     dti  fico  days.with.cr.line  revol.bal  revol.util  inq.last.6mths  \\\n",
       "0  19.48   737        5639.958333      28854        52.1               0   \n",
       "1  14.29   707        2760.000000      33623        76.7               0   \n",
       "2  11.63   682        4710.000000       3511        25.6               1   \n",
       "3   8.10   712        2699.958333      33667        73.2               1   \n",
       "4  14.97   667        4066.000000       4740        39.5               0   \n",
       "\n",
       "   delinq.2yrs  pub.rec  not.fully.paid  \n",
       "0            0        0               0  \n",
       "1            0        0               0  \n",
       "2            0        0               0  \n",
       "3            0        0               0  \n",
       "4            1        0               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the loan bank data\n",
    "\n",
    "loans = pd.read_csv(r\"D:\\iiit notes\\Programming\\AI\\Internship practice\\44 season 19-jul-2021\\loan_data.csv\", header=0) \n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5a4bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>NFPaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit.policy             purpose  int.rate  installment  log.annual.inc  \\\n",
       "0              1  debt_consolidation    0.1189       829.10       11.350407   \n",
       "1              1         credit_card    0.1071       228.22       11.082143   \n",
       "2              1  debt_consolidation    0.1357       366.86       10.373491   \n",
       "3              1  debt_consolidation    0.1008       162.34       11.350407   \n",
       "4              1         credit_card    0.1426       102.92       11.299732   \n",
       "\n",
       "     dti  fico  days.with.cr.line  revol.bal  revol.util  inq.last.6mths  \\\n",
       "0  19.48   737        5639.958333      28854        52.1               0   \n",
       "1  14.29   707        2760.000000      33623        76.7               0   \n",
       "2  11.63   682        4710.000000       3511        25.6               1   \n",
       "3   8.10   712        2699.958333      33667        73.2               1   \n",
       "4  14.97   667        4066.000000       4740        39.5               0   \n",
       "\n",
       "   delinq.2yrs  pub.rec  NFPaid  \n",
       "0            0        0       0  \n",
       "1            0        0       0  \n",
       "2            0        0       0  \n",
       "3            0        0       0  \n",
       "4            1        0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the name of variable\n",
    "\n",
    "loans =loans.rename(columns = {'not.fully.paid': 'NFPaid'}, inplace = False)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66869ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>NFPaid</th>\n",
       "      <th>purpose_all_other</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_small_business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit.policy  int.rate  installment  log.annual.inc    dti  fico  \\\n",
       "0              1    0.1189       829.10       11.350407  19.48   737   \n",
       "1              1    0.1071       228.22       11.082143  14.29   707   \n",
       "2              1    0.1357       366.86       10.373491  11.63   682   \n",
       "3              1    0.1008       162.34       11.350407   8.10   712   \n",
       "4              1    0.1426       102.92       11.299732  14.97   667   \n",
       "\n",
       "   days.with.cr.line  revol.bal  revol.util  inq.last.6mths  delinq.2yrs  \\\n",
       "0        5639.958333      28854        52.1               0            0   \n",
       "1        2760.000000      33623        76.7               0            0   \n",
       "2        4710.000000       3511        25.6               1            0   \n",
       "3        2699.958333      33667        73.2               1            0   \n",
       "4        4066.000000       4740        39.5               0            1   \n",
       "\n",
       "   pub.rec  NFPaid  purpose_all_other  purpose_credit_card  \\\n",
       "0        0       0                  0                    0   \n",
       "1        0       0                  0                    1   \n",
       "2        0       0                  0                    0   \n",
       "3        0       0                  0                    0   \n",
       "4        0       0                  0                    1   \n",
       "\n",
       "   purpose_debt_consolidation  purpose_educational  purpose_home_improvement  \\\n",
       "0                           1                    0                         0   \n",
       "1                           0                    0                         0   \n",
       "2                           1                    0                         0   \n",
       "3                           1                    0                         0   \n",
       "4                           0                    0                         0   \n",
       "\n",
       "   purpose_major_purchase  purpose_small_business  \n",
       "0                       0                       0  \n",
       "1                       0                       0  \n",
       "2                       0                       0  \n",
       "3                       0                       0  \n",
       "4                       0                       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the category variables\n",
    "\n",
    "cat_cols = ['purpose']\n",
    "\n",
    "# Convert the catagory variable into dummy variables\n",
    "\n",
    "loans = pd.get_dummies(loans,columns=cat_cols)\n",
    "loans=pd.DataFrame(loans)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cf18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9578 entries, 0 to 9577\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   credit.policy               9578 non-null   int64  \n",
      " 1   int.rate                    9578 non-null   float64\n",
      " 2   installment                 9578 non-null   float64\n",
      " 3   log.annual.inc              9578 non-null   float64\n",
      " 4   dti                         9578 non-null   float64\n",
      " 5   fico                        9578 non-null   int64  \n",
      " 6   days.with.cr.line           9578 non-null   float64\n",
      " 7   revol.bal                   9578 non-null   int64  \n",
      " 8   revol.util                  9578 non-null   float64\n",
      " 9   inq.last.6mths              9578 non-null   int64  \n",
      " 10  delinq.2yrs                 9578 non-null   int64  \n",
      " 11  pub.rec                     9578 non-null   int64  \n",
      " 12  NFPaid                      9578 non-null   int64  \n",
      " 13  purpose_all_other           9578 non-null   uint8  \n",
      " 14  purpose_credit_card         9578 non-null   uint8  \n",
      " 15  purpose_debt_consolidation  9578 non-null   uint8  \n",
      " 16  purpose_educational         9578 non-null   uint8  \n",
      " 17  purpose_home_improvement    9578 non-null   uint8  \n",
      " 18  purpose_major_purchase      9578 non-null   uint8  \n",
      " 19  purpose_small_business      9578 non-null   uint8  \n",
      "dtypes: float64(6), int64(7), uint8(7)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9ef3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8045\n",
       "1    1533\n",
       "Name: NFPaid, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans['NFPaid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1caee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent and Target variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in loans.columns:\n",
    "    if col != 'NFPaid':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'NFPaid'\n",
    "\n",
    "x = loans[IndepVar]\n",
    "y = loans[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbfd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 12)\n",
    "x_test_F1 = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a3166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify variables which are fit for scaling\n",
    "\n",
    "cols = ['int.rate', 'installment', 'log.annual.inc', 'dti', 'fico', 'days.with.cr.line', 'revol.bal', 'revol.util']\n",
    "\n",
    "# Scaling the features by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train[cols] = mmscaler.fit_transform(x_train[cols])\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test[cols] = mmscaler.fit_transform(x_test[cols])\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c70f57",
   "metadata": {},
   "source": [
    "# Random Forest without LDA & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12c4e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[   0  488]\n",
      " [   0 2386]]\n",
      "Outcome Values : \n",
      " 0 488 0 2386\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       488\n",
      "           0       0.83      1.00      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.42      0.50      0.45      2874\n",
      "weighted avg       0.69      0.83      0.75      2874\n",
      "\n",
      "Accuracy : 83.0 %\n",
      "Precision : nan %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.0\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC nan\n",
      "roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y_pred = modelRF.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bb5d0",
   "metadata": {},
   "source": [
    "# Logistic Regression without LDA & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb6fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  10  478]\n",
      " [  11 2375]]\n",
      "Outcome Values : \n",
      " 10 478 11 2375\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.02      0.04       488\n",
      "           0       0.83      1.00      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.65      0.51      0.47      2874\n",
      "weighted avg       0.77      0.83      0.76      2874\n",
      "\n",
      "Accuracy : 83.0 %\n",
      "Precision : 47.6 %\n",
      "Recall : 2.0 %\n",
      "F1 Score : 0.039\n",
      "Balanced Accuracy : 50.7 %\n",
      "MCC 0.07\n",
      "roc_auc_score: 0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = modelLR.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y1_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y1_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd2614",
   "metadata": {},
   "source": [
    "# Decision Tree without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f02972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 124  364]\n",
      " [ 457 1929]]\n",
      "Outcome Values : \n",
      " 124 364 457 1929\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.25      0.23       488\n",
      "           0       0.84      0.81      0.82      2386\n",
      "\n",
      "    accuracy                           0.71      2874\n",
      "   macro avg       0.53      0.53      0.53      2874\n",
      "weighted avg       0.73      0.71      0.72      2874\n",
      "\n",
      "Accuracy : 71.4 %\n",
      "Precision : 21.3 %\n",
      "Recall : 25.4 %\n",
      "F1 Score : 0.232\n",
      "Balanced Accuracy : 53.1 %\n",
      "MCC 0.058\n",
      "roc_auc_score: 0.531\n"
     ]
    }
   ],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y2_pred = modelDT.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y2_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y2_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46974968",
   "metadata": {},
   "source": [
    "# SVM without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e2ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 261  227]\n",
      " [ 701 1685]]\n",
      "Outcome Values : \n",
      " 261 227 701 1685\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.53      0.36       488\n",
      "           0       0.88      0.71      0.78      2386\n",
      "\n",
      "    accuracy                           0.68      2874\n",
      "   macro avg       0.58      0.62      0.57      2874\n",
      "weighted avg       0.78      0.68      0.71      2874\n",
      "\n",
      "Accuracy : 67.7 %\n",
      "Precision : 27.1 %\n",
      "Recall : 53.5 %\n",
      "F1 Score : 0.36\n",
      "Balanced Accuracy : 62.0 %\n",
      "MCC 0.192\n",
      "roc_auc_score: 0.621\n"
     ]
    }
   ],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "modelSVMGaussian.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y3_pred = modelSVMGaussian.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y3_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y3_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bfb1de",
   "metadata": {},
   "source": [
    "# KNN Without PCA & LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec4c102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  41  447]\n",
      " [  81 2305]]\n",
      "Outcome Values : \n",
      " 41 447 81 2305\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.34      0.08      0.13       488\n",
      "           0       0.84      0.97      0.90      2386\n",
      "\n",
      "    accuracy                           0.82      2874\n",
      "   macro avg       0.59      0.53      0.52      2874\n",
      "weighted avg       0.75      0.82      0.77      2874\n",
      "\n",
      "Accuracy : 81.6 %\n",
      "Precision : 33.6 %\n",
      "Recall : 8.4 %\n",
      "F1 Score : 0.134\n",
      "Balanced Accuracy : 52.5 %\n",
      "MCC 0.093\n",
      "roc_auc_score: 0.525\n"
     ]
    }
   ],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y4_pred = modelKNN.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y4_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y4_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe2a5a",
   "metadata": {},
   "source": [
    "# Train the Principal Component Analysis (PCA) with train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cb62f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.70350903e-01 5.26343855e-02 4.51284661e-02 2.74139052e-02\n",
      " 2.01639802e-02 1.49645574e-02 1.19621976e-02 1.13944105e-02\n",
      " 9.91289478e-03 8.95880118e-03 7.05676282e-03 6.44225078e-03\n",
      " 5.78360411e-03 3.74839448e-03 2.19124563e-03 9.69680833e-04\n",
      " 8.28188722e-04 9.53711941e-05 1.39077347e-33]\n"
     ]
    }
   ],
   "source": [
    "# Principal component analysis (PCA) is a statistical technique to convert high dimensional data to low dimensional data\n",
    "# by selecting the most important features that capture maximum information about the dataset. The features are selected\n",
    "# on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal\n",
    "# component. The feature that is responsible for second highest variance is considered the second principal component,\n",
    "# and so on. It is important to mention that principal components do not have any correlation with each other.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "applyPCA = PCA()\n",
    "\n",
    "x1_train = applyPCA.fit_transform(x_train)\n",
    "x1_test = applyPCA.transform(x_test)\n",
    "explained_variance = applyPCA.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95667a7b",
   "metadata": {},
   "source": [
    "# Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc84f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[   0  488]\n",
      " [   0 2386]]\n",
      "Outcome Values : \n",
      " 0 488 0 2386\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       488\n",
      "           0       0.83      1.00      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.42      0.50      0.45      2874\n",
      "weighted avg       0.69      0.83      0.75      2874\n",
      "\n",
      "Accuracy : 83.0 %\n",
      "Precision : nan %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.0\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC nan\n",
      "roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x1_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y5_pred = modelRF.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y5_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y5_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7015d",
   "metadata": {},
   "source": [
    "# Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbcdfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  10  478]\n",
      " [  11 2375]]\n",
      "Outcome Values : \n",
      " 10 478 11 2375\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.02      0.04       488\n",
      "           0       0.83      1.00      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.65      0.51      0.47      2874\n",
      "weighted avg       0.77      0.83      0.76      2874\n",
      "\n",
      "Accuracy : 83.0 %\n",
      "Precision : 47.6 %\n",
      "Recall : 2.0 %\n",
      "F1 Score : 0.039\n",
      "Balanced Accuracy : 50.7 %\n",
      "MCC 0.07\n",
      "roc_auc_score: 0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y6_pred = modelLR.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y6_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y6_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58d29c",
   "metadata": {},
   "source": [
    "# Decision Tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b9696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 123  365]\n",
      " [ 466 1920]]\n",
      "Outcome Values : \n",
      " 123 365 466 1920\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.25      0.23       488\n",
      "           0       0.84      0.80      0.82      2386\n",
      "\n",
      "    accuracy                           0.71      2874\n",
      "   macro avg       0.52      0.53      0.53      2874\n",
      "weighted avg       0.73      0.71      0.72      2874\n",
      "\n",
      "Accuracy : 71.1 %\n",
      "Precision : 20.9 %\n",
      "Recall : 25.2 %\n",
      "F1 Score : 0.228\n",
      "Balanced Accuracy : 52.8 %\n",
      "MCC 0.053\n",
      "roc_auc_score: 0.528\n"
     ]
    }
   ],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x1_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y7_pred = modelDT.predict(x1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y7_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y7_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0342ef",
   "metadata": {},
   "source": [
    "# SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7992670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 257  231]\n",
      " [ 695 1691]]\n",
      "Outcome Values : \n",
      " 257 231 695 1691\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.53      0.36       488\n",
      "           0       0.88      0.71      0.79      2386\n",
      "\n",
      "    accuracy                           0.68      2874\n",
      "   macro avg       0.57      0.62      0.57      2874\n",
      "weighted avg       0.78      0.68      0.71      2874\n",
      "\n",
      "Accuracy : 67.8 %\n",
      "Precision : 27.0 %\n",
      "Recall : 52.7 %\n",
      "F1 Score : 0.357\n",
      "Balanced Accuracy : 61.8 %\n",
      "MCC 0.188\n",
      "roc_auc_score: 0.618\n"
     ]
    }
   ],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "\n",
    "modelSVMGaussian.fit(x1_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y8_pred = modelSVMGaussian.predict(x1_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y8_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y8_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffa03d",
   "metadata": {},
   "source": [
    "# KNN with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12cff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  41  447]\n",
      " [  81 2305]]\n",
      "Outcome Values : \n",
      " 41 447 81 2305\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.34      0.08      0.13       488\n",
      "           0       0.84      0.97      0.90      2386\n",
      "\n",
      "    accuracy                           0.82      2874\n",
      "   macro avg       0.59      0.53      0.52      2874\n",
      "weighted avg       0.75      0.82      0.77      2874\n",
      "\n",
      "Accuracy : 81.6 %\n",
      "Precision : 33.6 %\n",
      "Recall : 8.4 %\n",
      "F1 Score : 0.134\n",
      "Balanced Accuracy : 52.5 %\n",
      "MCC 0.093\n",
      "roc_auc_score: 0.525\n"
     ]
    }
   ],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x1_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y9_pred = modelKNN.predict(x1_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y9_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y9_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf45da",
   "metadata": {},
   "source": [
    "# Train the Linear Discriminant Analysis (LDA) with train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75c518ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "applyLDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "x2_train = applyLDA.fit_transform(x_train, y_train)\n",
    "x2_test = applyLDA.transform(x_test)\n",
    "\n",
    "explained_variance = applyLDA.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170c003",
   "metadata": {},
   "source": [
    "# Random Forest with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bafae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[   0  488]\n",
      " [   0 2386]]\n",
      "Outcome Values : \n",
      " 0 488 0 2386\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       488\n",
      "           0       0.83      1.00      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.42      0.50      0.45      2874\n",
      "weighted avg       0.69      0.83      0.75      2874\n",
      "\n",
      "Accuracy : 83.0 %\n",
      "Precision : nan %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.0\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC nan\n",
      "roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest Classification model and train model using the training dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "modelRF = modelRF.fit(x2_train, y_train)\n",
    "\n",
    "# Predict the model with the test data set\n",
    "\n",
    "y10_pred = modelRF.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y10_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y10_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a18347",
   "metadata": {},
   "source": [
    "# Logistic Regression with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "765fdfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  19  469]\n",
      " [  18 2368]]\n",
      "Outcome Values : \n",
      " 19 469 18 2368\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.04      0.07       488\n",
      "           0       0.83      0.99      0.91      2386\n",
      "\n",
      "    accuracy                           0.83      2874\n",
      "   macro avg       0.67      0.52      0.49      2874\n",
      "weighted avg       0.78      0.83      0.77      2874\n",
      "\n",
      "Accuracy : 83.1 %\n",
      "Precision : 51.4 %\n",
      "Recall : 3.9 %\n",
      "F1 Score : 0.072\n",
      "Balanced Accuracy : 51.6 %\n",
      "MCC 0.105\n",
      "roc_auc_score: 0.516\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "modelLR = modelLR.fit(x2_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y11_pred = modelLR.predict(x2_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y11_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y11_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5056b1",
   "metadata": {},
   "source": [
    "# Decision Tree with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47621f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 102  386]\n",
      " [ 281 2105]]\n",
      "Outcome Values : \n",
      " 102 386 281 2105\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.21      0.23       488\n",
      "           0       0.85      0.88      0.86      2386\n",
      "\n",
      "    accuracy                           0.77      2874\n",
      "   macro avg       0.56      0.55      0.55      2874\n",
      "weighted avg       0.75      0.77      0.76      2874\n",
      "\n",
      "Accuracy : 76.8 %\n",
      "Precision : 26.6 %\n",
      "Recall : 20.9 %\n",
      "F1 Score : 0.234\n",
      "Balanced Accuracy : 54.6 %\n",
      "MCC 0.101\n",
      "roc_auc_score: 0.546\n"
     ]
    }
   ],
   "source": [
    "# To build the decision tree model with Over sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                random_state=None, splitter='best')\n",
    "\n",
    "modelDT = modelDT.fit(x2_train,y_train)\n",
    "\n",
    "# Predict with test data\n",
    "\n",
    "y12_pred = modelDT.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y12_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y12_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204206e1",
   "metadata": {},
   "source": [
    "# SVM with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7a91926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 250  238]\n",
      " [ 671 1715]]\n",
      "Outcome Values : \n",
      " 250 238 671 1715\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.51      0.35       488\n",
      "           0       0.88      0.72      0.79      2386\n",
      "\n",
      "    accuracy                           0.68      2874\n",
      "   macro avg       0.57      0.62      0.57      2874\n",
      "weighted avg       0.78      0.68      0.72      2874\n",
      "\n",
      "Accuracy : 68.4 %\n",
      "Precision : 27.1 %\n",
      "Recall : 51.2 %\n",
      "F1 Score : 0.355\n",
      "Balanced Accuracy : 61.5 %\n",
      "MCC 0.186\n",
      "roc_auc_score: 0.616\n"
     ]
    }
   ],
   "source": [
    "# Training the SVM algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced')\n",
    "\n",
    "modelSVMGaussian.fit(x2_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "\n",
    "y13_pred = modelSVMGaussian.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y13_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y13_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23938ee1",
   "metadata": {},
   "source": [
    "# KNN with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bcd3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[  47  441]\n",
      " [  81 2305]]\n",
      "Outcome Values : \n",
      " 47 441 81 2305\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.37      0.10      0.15       488\n",
      "           0       0.84      0.97      0.90      2386\n",
      "\n",
      "    accuracy                           0.82      2874\n",
      "   macro avg       0.60      0.53      0.53      2874\n",
      "weighted avg       0.76      0.82      0.77      2874\n",
      "\n",
      "Accuracy : 81.8 %\n",
      "Precision : 36.7 %\n",
      "Recall : 9.6 %\n",
      "F1 Score : 0.153\n",
      "Balanced Accuracy : 53.1 %\n",
      "MCC 0.114\n",
      "roc_auc_score: 0.531\n"
     ]
    }
   ],
   "source": [
    "# Build the algorithm with KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "modelKNN = neighbors.KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                          metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelKNN.fit(x2_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y14_pred = modelKNN.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "predicted = y14_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual, predicted, labels=[1, 0], sample_weight=None, normalize=None,)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp,fn, fp, tn = confusion_matrix(actual, predicted, labels=[1,0]).reshape(-1)\n",
    "\n",
    "print('Outcome Values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', matrix)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "\n",
    "specificity = round(tn/(tn+fp), 3)\n",
    "\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3)\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3)\n",
    "precision = round(tp/(tp+fp), 3)\n",
    "f1Score = round((2*tp/(2*tp + fp +fn)), 3);\n",
    "\n",
    "# Mathews Correlatin coefficient (MCC). Range of values of MCC lie between -1 to +1\n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "m = (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)\n",
    "\n",
    "MCC = round(((tp* tn) - (fp * fn)) / sqrt(m), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2), '%')\n",
    "print('Precision :', round(precision*100, 2), '%')\n",
    "print('Recall :', round(sensitivity*100, 2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2), '%')\n",
    "print('MCC', MCC)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Area under ROC curve \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y14_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208db4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
