{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2febc4f0",
   "metadata": {},
   "source": [
    "# letter-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4f7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import pandasql as psql\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from itertools import cycle\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Ignore harmless warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25bec62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  20000 non-null  object\n",
      " 1   xbox    20000 non-null  int64 \n",
      " 2   ybox    20000 non-null  int64 \n",
      " 3   width   20000 non-null  int64 \n",
      " 4   height  20000 non-null  int64 \n",
      " 5   onpix   20000 non-null  int64 \n",
      " 6   xbar    20000 non-null  int64 \n",
      " 7   ybar    20000 non-null  int64 \n",
      " 8   x2bar   20000 non-null  int64 \n",
      " 9   y2bar   20000 non-null  int64 \n",
      " 10  xybar   20000 non-null  int64 \n",
      " 11  x2ybar  20000 non-null  int64 \n",
      " 12  xy2bar  20000 non-null  int64 \n",
      " 13  xedge   20000 non-null  int64 \n",
      " 14  xedgey  20000 non-null  int64 \n",
      " 15  yedge   20000 non-null  int64 \n",
      " 16  yedgex  20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>xbox</th>\n",
       "      <th>ybox</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>onpix</th>\n",
       "      <th>xbar</th>\n",
       "      <th>ybar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybar</th>\n",
       "      <th>xy2bar</th>\n",
       "      <th>xedge</th>\n",
       "      <th>xedgey</th>\n",
       "      <th>yedge</th>\n",
       "      <th>yedgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter  xbox   ybox   width   height  onpix   xbar   ybar   x2bar  y2bar   \\\n",
       "0      T      2      8       3       5       1      8     13      0       6   \n",
       "1      D      4     11       6       8       6     10      6      2       6   \n",
       "2      S      4     11       5       8       3      8      8      6       9   \n",
       "3      B      4      2       5       4       4      8      7      6       6   \n",
       "4      A      1      1       3       2       1      8      2      2       2   \n",
       "\n",
       "   xybar   x2ybar  xy2bar  xedge   xedgey  yedge   yedgex  \n",
       "0       6      10       8       0       8       0       8  \n",
       "1      10       3       7       3       7       3       9  \n",
       "2       5       6       6       0       8       9       7  \n",
       "3       7       6       6       2       8       7      10  \n",
       "4       8       2       8       1       6       2       7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'letter recognition' dataset\n",
    "\n",
    "lettersdata = pd.read_csv(r\"D:\\iiit notes\\Programming\\AI\\Internship practice\\51 season Multinomial Classifier 02-Aug-2021\\letter-recognition.csv\", header=0)\n",
    "lettersdata.head()\n",
    "\n",
    "# Display the dataset information\n",
    "\n",
    "lettersdata.info()\n",
    "\n",
    "# Get the sub-set of data for letters A, B, P & R\n",
    "\n",
    "lettersdata_N = psql.sqldf(\"select * \\\n",
    "from lettersdata \\\n",
    "where letter in ('A', 'B', 'C', 'D', 'E', \\\n",
    "'P', 'Q', 'R', 'S', 'T' )\")\n",
    "\n",
    "lettersdata_N.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63a94fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7752 entries, 0 to 7751\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  7752 non-null   object\n",
      " 1   xbox    7752 non-null   int64 \n",
      " 2   ybox    7752 non-null   int64 \n",
      " 3   width   7752 non-null   int64 \n",
      " 4   height  7752 non-null   int64 \n",
      " 5   onpix   7752 non-null   int64 \n",
      " 6   xbar    7752 non-null   int64 \n",
      " 7   ybar    7752 non-null   int64 \n",
      " 8   x2bar   7752 non-null   int64 \n",
      " 9   y2bar   7752 non-null   int64 \n",
      " 10  xybar   7752 non-null   int64 \n",
      " 11  x2ybar  7752 non-null   int64 \n",
      " 12  xy2bar  7752 non-null   int64 \n",
      " 13  xedge   7752 non-null   int64 \n",
      " 14  xedgey  7752 non-null   int64 \n",
      " 15  yedge   7752 non-null   int64 \n",
      " 16  yedgex  7752 non-null   int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the new 'letter_recognition' data information\n",
    "\n",
    "lettersdata_N.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8023701a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    805\n",
       "C    736\n",
       "B    766\n",
       "E    768\n",
       "S    748\n",
       "A    789\n",
       "T    796\n",
       "R    758\n",
       "Q    783\n",
       "P    803\n",
       "Name: letter, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the count of each letter\n",
    "\n",
    "lettersdata_N['letter'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3054d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent and Target variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in lettersdata_N.columns:\n",
    "    if col != 'letter':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'letter'\n",
    "\n",
    "x = lettersdata_N[IndepVar]\n",
    "y = lettersdata_N[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41c43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder and convert the target variable from object to integer values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#creating labelEncoder\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "\n",
    "y=le.fit_transform(y)\n",
    "\n",
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512431f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 6)\n",
    "\n",
    "# Copy the test data to back-up file\n",
    "\n",
    "x_test_F1 = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb38125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train_scaled)\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb10a2",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f1e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'None' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=None,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y1_pred = modelBAG.predict(x_test)\n",
    "y1_pred_proba = modelBAG.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f3843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[173   0   0   0   0   0   0   0   0   0]\n",
      " [  1 145   0   1  16   3  10  12   8   2]\n",
      " [  1   0 166   0   1   0   1   0   0   1]\n",
      " [  2  15   0 138   0   1   4   6   3   3]\n",
      " [  0   2   1   0 162   0   1   2   0   0]\n",
      " [  0  28   0  56   0 201   3   5   2   0]\n",
      " [  0   0   2   0   1   0 184   0   1   1]\n",
      " [  3   2   0   1   0   0   0 155   0   0]\n",
      " [  2   6   0   0  19   0   3   0 165   0]\n",
      " [  0   0   1   0   1   0   0   0   1 214]]\n",
      "Print Class: 0\n",
      "TP=173, FP=0, TN=1756, FN=9\n",
      "Accuracy: 0.995\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.951\n",
      "F1-Score: 0.975\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.976\n",
      "MCC: 0.972\n",
      "\n",
      "Print Class: 1\n",
      "TP=145, FP=53, TN=1687, FN=53\n",
      "Accuracy: 0.945\n",
      "Precision: 0.732\n",
      "Sensitivity: 0.732\n",
      "F1-Score: 0.732\n",
      "Specificity: 0.97\n",
      "Balanced Accuracy: 0.851\n",
      "MCC: 0.702\n",
      "\n",
      "Print Class: 2\n",
      "TP=166, FP=4, TN=1764, FN=4\n",
      "Accuracy: 0.996\n",
      "Precision: 0.976\n",
      "Sensitivity: 0.976\n",
      "F1-Score: 0.976\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.987\n",
      "MCC: 0.974\n",
      "\n",
      "Print Class: 3\n",
      "TP=138, FP=34, TN=1708, FN=58\n",
      "Accuracy: 0.953\n",
      "Precision: 0.802\n",
      "Sensitivity: 0.704\n",
      "F1-Score: 0.75\n",
      "Specificity: 0.98\n",
      "Balanced Accuracy: 0.842\n",
      "MCC: 0.726\n",
      "\n",
      "Print Class: 4\n",
      "TP=162, FP=6, TN=1732, FN=38\n",
      "Accuracy: 0.977\n",
      "Precision: 0.964\n",
      "Sensitivity: 0.81\n",
      "F1-Score: 0.88\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.904\n",
      "MCC: 0.872\n",
      "\n",
      "Print Class: 5\n",
      "TP=201, FP=94, TN=1639, FN=4\n",
      "Accuracy: 0.949\n",
      "Precision: 0.681\n",
      "Sensitivity: 0.98\n",
      "F1-Score: 0.804\n",
      "Specificity: 0.946\n",
      "Balanced Accuracy: 0.963\n",
      "MCC: 0.793\n",
      "\n",
      "Print Class: 6\n",
      "TP=184, FP=5, TN=1727, FN=22\n",
      "Accuracy: 0.986\n",
      "Precision: 0.974\n",
      "Sensitivity: 0.893\n",
      "F1-Score: 0.932\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.945\n",
      "MCC: 0.925\n",
      "\n",
      "Print Class: 7\n",
      "TP=155, FP=6, TN=1752, FN=25\n",
      "Accuracy: 0.984\n",
      "Precision: 0.963\n",
      "Sensitivity: 0.861\n",
      "F1-Score: 0.909\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.929\n",
      "MCC: 0.902\n",
      "\n",
      "Print Class: 8\n",
      "TP=165, FP=30, TN=1728, FN=15\n",
      "Accuracy: 0.977\n",
      "Precision: 0.846\n",
      "Sensitivity: 0.917\n",
      "F1-Score: 0.88\n",
      "Specificity: 0.983\n",
      "Balanced Accuracy: 0.95\n",
      "MCC: 0.868\n",
      "\n",
      "Print Class: 9\n",
      "TP=214, FP=3, TN=1714, FN=7\n",
      "Accuracy: 0.995\n",
      "Precision: 0.986\n",
      "Sensitivity: 0.968\n",
      "F1-Score: 0.977\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.983\n",
      "MCC: 0.974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y1_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y1_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e3bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 97.57%\n",
      "Precision: 89.24%\n",
      "Recall or Sensitivity: 87.92%\n",
      "F1-Score: 0.8815\n",
      "Specificity or True Nagative Rate: 98.66%\n",
      "Balanced Accuracy: 93.3%\n",
      "\n",
      "MCC: 0.8708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e392442",
   "metadata": {},
   "source": [
    "# Bagging Classifier with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25e40fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162   1   0   0   1   0   1   0   0   0]\n",
      " [  2 130   0   2  14   2  20  23  14   5]\n",
      " [  1   0 167   0   4   0   2   0   0   1]\n",
      " [  7  16   0 142   1   1  10   7   3   3]\n",
      " [  0   1   1   0 157   1   1   1   0   0]\n",
      " [  3  36   0  50   0 201   1   5   7   0]\n",
      " [  0   1   1   0   2   0 165   0   0   0]\n",
      " [  4   4   0   1   0   0   3 144   1   1]\n",
      " [  2   9   0   0  20   0   2   0 152   2]\n",
      " [  1   0   1   1   1   0   1   0   3 209]]\n",
      "Print Class: 0\n",
      "TP=162, FP=3, TN=1753, FN=20\n",
      "Accuracy: 0.988\n",
      "Precision: 0.982\n",
      "Sensitivity: 0.89\n",
      "F1-Score: 0.934\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.944\n",
      "MCC: 0.929\n",
      "\n",
      "Print Class: 1\n",
      "TP=130, FP=82, TN=1658, FN=68\n",
      "Accuracy: 0.923\n",
      "Precision: 0.613\n",
      "Sensitivity: 0.657\n",
      "F1-Score: 0.634\n",
      "Specificity: 0.953\n",
      "Balanced Accuracy: 0.805\n",
      "MCC: 0.591\n",
      "\n",
      "Print Class: 2\n",
      "TP=167, FP=8, TN=1760, FN=3\n",
      "Accuracy: 0.994\n",
      "Precision: 0.954\n",
      "Sensitivity: 0.982\n",
      "F1-Score: 0.968\n",
      "Specificity: 0.995\n",
      "Balanced Accuracy: 0.988\n",
      "MCC: 0.965\n",
      "\n",
      "Print Class: 3\n",
      "TP=142, FP=48, TN=1694, FN=54\n",
      "Accuracy: 0.947\n",
      "Precision: 0.747\n",
      "Sensitivity: 0.724\n",
      "F1-Score: 0.736\n",
      "Specificity: 0.972\n",
      "Balanced Accuracy: 0.848\n",
      "MCC: 0.707\n",
      "\n",
      "Print Class: 4\n",
      "TP=157, FP=5, TN=1733, FN=43\n",
      "Accuracy: 0.975\n",
      "Precision: 0.969\n",
      "Sensitivity: 0.785\n",
      "F1-Score: 0.867\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.891\n",
      "MCC: 0.86\n",
      "\n",
      "Print Class: 5\n",
      "TP=201, FP=102, TN=1631, FN=4\n",
      "Accuracy: 0.945\n",
      "Precision: 0.663\n",
      "Sensitivity: 0.98\n",
      "F1-Score: 0.791\n",
      "Specificity: 0.941\n",
      "Balanced Accuracy: 0.96\n",
      "MCC: 0.78\n",
      "\n",
      "Print Class: 6\n",
      "TP=165, FP=4, TN=1728, FN=41\n",
      "Accuracy: 0.977\n",
      "Precision: 0.976\n",
      "Sensitivity: 0.801\n",
      "F1-Score: 0.88\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.9\n",
      "MCC: 0.872\n",
      "\n",
      "Print Class: 7\n",
      "TP=144, FP=14, TN=1744, FN=36\n",
      "Accuracy: 0.974\n",
      "Precision: 0.911\n",
      "Sensitivity: 0.8\n",
      "F1-Score: 0.852\n",
      "Specificity: 0.992\n",
      "Balanced Accuracy: 0.896\n",
      "MCC: 0.84\n",
      "\n",
      "Print Class: 8\n",
      "TP=152, FP=35, TN=1723, FN=28\n",
      "Accuracy: 0.967\n",
      "Precision: 0.813\n",
      "Sensitivity: 0.844\n",
      "F1-Score: 0.828\n",
      "Specificity: 0.98\n",
      "Balanced Accuracy: 0.912\n",
      "MCC: 0.811\n",
      "\n",
      "Print Class: 9\n",
      "TP=209, FP=8, TN=1709, FN=12\n",
      "Accuracy: 0.99\n",
      "Precision: 0.963\n",
      "Sensitivity: 0.946\n",
      "F1-Score: 0.954\n",
      "Specificity: 0.995\n",
      "Balanced Accuracy: 0.97\n",
      "MCC: 0.949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'Decision Tree' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelDT,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y2_pred = modelBAG.predict(x_test)\n",
    "y2_pred_proba = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y2_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y2_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99d10f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 96.8%\n",
      "Precision: 85.91%\n",
      "Recall or Sensitivity: 84.09%\n",
      "F1-Score: 0.8444\n",
      "Specificity or True Nagative Rate: 98.21%\n",
      "Balanced Accuracy: 91.14%\n",
      "\n",
      "MCC: 0.8304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc64c2",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "191f113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162   0   0   0   0   0   3   1   2   0]\n",
      " [  1 186   0  17  17   5  16  24  29   5]\n",
      " [  2   0 158   0   2   0   0   0   1   2]\n",
      " [  1   3   0 168   0   2   4   8   1   3]\n",
      " [  0   0   6   0 145   1   3   0   0  10]\n",
      " [  1   2   1   3   0 189   0   0   6   3]\n",
      " [  2   2   1   3  10   1 170   1   5   3]\n",
      " [  3   3   0   1   3   3   4 144   3   4]\n",
      " [  8   2   0   3  10   0   6   0 127   5]\n",
      " [  2   0   4   1  13   4   0   2   6 186]]\n",
      "Print Class: 0\n",
      "TP=162, FP=6, TN=1750, FN=20\n",
      "Accuracy: 0.987\n",
      "Precision: 0.964\n",
      "Sensitivity: 0.89\n",
      "F1-Score: 0.926\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.944\n",
      "MCC: 0.919\n",
      "\n",
      "Print Class: 1\n",
      "TP=186, FP=114, TN=1626, FN=12\n",
      "Accuracy: 0.935\n",
      "Precision: 0.62\n",
      "Sensitivity: 0.939\n",
      "F1-Score: 0.747\n",
      "Specificity: 0.934\n",
      "Balanced Accuracy: 0.936\n",
      "MCC: 0.732\n",
      "\n",
      "Print Class: 2\n",
      "TP=158, FP=7, TN=1761, FN=12\n",
      "Accuracy: 0.99\n",
      "Precision: 0.958\n",
      "Sensitivity: 0.929\n",
      "F1-Score: 0.943\n",
      "Specificity: 0.996\n",
      "Balanced Accuracy: 0.962\n",
      "MCC: 0.938\n",
      "\n",
      "Print Class: 3\n",
      "TP=168, FP=22, TN=1720, FN=28\n",
      "Accuracy: 0.974\n",
      "Precision: 0.884\n",
      "Sensitivity: 0.857\n",
      "F1-Score: 0.87\n",
      "Specificity: 0.987\n",
      "Balanced Accuracy: 0.922\n",
      "MCC: 0.856\n",
      "\n",
      "Print Class: 4\n",
      "TP=145, FP=20, TN=1718, FN=55\n",
      "Accuracy: 0.961\n",
      "Precision: 0.879\n",
      "Sensitivity: 0.725\n",
      "F1-Score: 0.795\n",
      "Specificity: 0.988\n",
      "Balanced Accuracy: 0.856\n",
      "MCC: 0.778\n",
      "\n",
      "Print Class: 5\n",
      "TP=189, FP=16, TN=1717, FN=16\n",
      "Accuracy: 0.983\n",
      "Precision: 0.922\n",
      "Sensitivity: 0.922\n",
      "F1-Score: 0.922\n",
      "Specificity: 0.991\n",
      "Balanced Accuracy: 0.956\n",
      "MCC: 0.913\n",
      "\n",
      "Print Class: 6\n",
      "TP=170, FP=28, TN=1704, FN=36\n",
      "Accuracy: 0.967\n",
      "Precision: 0.859\n",
      "Sensitivity: 0.825\n",
      "F1-Score: 0.842\n",
      "Specificity: 0.984\n",
      "Balanced Accuracy: 0.904\n",
      "MCC: 0.823\n",
      "\n",
      "Print Class: 7\n",
      "TP=144, FP=24, TN=1734, FN=36\n",
      "Accuracy: 0.969\n",
      "Precision: 0.857\n",
      "Sensitivity: 0.8\n",
      "F1-Score: 0.828\n",
      "Specificity: 0.986\n",
      "Balanced Accuracy: 0.893\n",
      "MCC: 0.811\n",
      "\n",
      "Print Class: 8\n",
      "TP=127, FP=34, TN=1724, FN=53\n",
      "Accuracy: 0.955\n",
      "Precision: 0.789\n",
      "Sensitivity: 0.706\n",
      "F1-Score: 0.745\n",
      "Specificity: 0.981\n",
      "Balanced Accuracy: 0.843\n",
      "MCC: 0.722\n",
      "\n",
      "Print Class: 9\n",
      "TP=186, FP=32, TN=1685, FN=35\n",
      "Accuracy: 0.965\n",
      "Precision: 0.853\n",
      "Sensitivity: 0.842\n",
      "F1-Score: 0.847\n",
      "Specificity: 0.981\n",
      "Balanced Accuracy: 0.912\n",
      "MCC: 0.828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'Logistic Regression' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelLR,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y3_pred = modelBAG.predict(x_test)\n",
    "y3_pred_proba = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y3_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y3_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfcad4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 96.86%\n",
      "Precision: 85.85%\n",
      "Recall or Sensitivity: 84.35%\n",
      "F1-Score: 0.8465\n",
      "Specificity or True Nagative Rate: 98.25%\n",
      "Balanced Accuracy: 91.28%\n",
      "\n",
      "MCC: 0.832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca134086",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a576cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180   0   0   0   0   0   0   0   1   0]\n",
      " [  0 190   0   6   8   2   0  11   3   3]\n",
      " [  0   0 170   0   4   0   0   0   0   1]\n",
      " [  1   2   0 187   0   0   0   0   1   2]\n",
      " [  0   1   0   0 186   1   0   0   0   0]\n",
      " [  0   0   0   0   0 202   1   0   0   0]\n",
      " [  0   0   0   0   0   0 204   0   1   0]\n",
      " [  1   3   0   3   0   0   1 169   0   2]\n",
      " [  0   2   0   0   2   0   0   0 173   0]\n",
      " [  0   0   0   0   0   0   0   0   1 213]]\n",
      "Print Class: 0\n",
      "TP=180, FP=1, TN=1755, FN=2\n",
      "Accuracy: 0.998\n",
      "Precision: 0.994\n",
      "Sensitivity: 0.989\n",
      "F1-Score: 0.992\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.994\n",
      "MCC: 0.991\n",
      "\n",
      "Print Class: 1\n",
      "TP=190, FP=33, TN=1707, FN=8\n",
      "Accuracy: 0.979\n",
      "Precision: 0.852\n",
      "Sensitivity: 0.96\n",
      "F1-Score: 0.903\n",
      "Specificity: 0.981\n",
      "Balanced Accuracy: 0.97\n",
      "MCC: 0.893\n",
      "\n",
      "Print Class: 2\n",
      "TP=170, FP=5, TN=1763, FN=0\n",
      "Accuracy: 0.997\n",
      "Precision: 0.971\n",
      "Sensitivity: 1.0\n",
      "F1-Score: 0.986\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.998\n",
      "MCC: 0.984\n",
      "\n",
      "Print Class: 3\n",
      "TP=187, FP=6, TN=1736, FN=9\n",
      "Accuracy: 0.992\n",
      "Precision: 0.969\n",
      "Sensitivity: 0.954\n",
      "F1-Score: 0.961\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.976\n",
      "MCC: 0.957\n",
      "\n",
      "Print Class: 4\n",
      "TP=186, FP=2, TN=1736, FN=14\n",
      "Accuracy: 0.992\n",
      "Precision: 0.989\n",
      "Sensitivity: 0.93\n",
      "F1-Score: 0.959\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.964\n",
      "MCC: 0.955\n",
      "\n",
      "Print Class: 5\n",
      "TP=202, FP=1, TN=1732, FN=3\n",
      "Accuracy: 0.998\n",
      "Precision: 0.995\n",
      "Sensitivity: 0.985\n",
      "F1-Score: 0.99\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.992\n",
      "MCC: 0.989\n",
      "\n",
      "Print Class: 6\n",
      "TP=204, FP=1, TN=1731, FN=2\n",
      "Accuracy: 0.998\n",
      "Precision: 0.995\n",
      "Sensitivity: 0.99\n",
      "F1-Score: 0.993\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.994\n",
      "MCC: 0.992\n",
      "\n",
      "Print Class: 7\n",
      "TP=169, FP=10, TN=1748, FN=11\n",
      "Accuracy: 0.989\n",
      "Precision: 0.944\n",
      "Sensitivity: 0.939\n",
      "F1-Score: 0.942\n",
      "Specificity: 0.994\n",
      "Balanced Accuracy: 0.966\n",
      "MCC: 0.936\n",
      "\n",
      "Print Class: 8\n",
      "TP=173, FP=4, TN=1754, FN=7\n",
      "Accuracy: 0.994\n",
      "Precision: 0.977\n",
      "Sensitivity: 0.961\n",
      "F1-Score: 0.969\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.98\n",
      "MCC: 0.966\n",
      "\n",
      "Print Class: 9\n",
      "TP=213, FP=1, TN=1716, FN=8\n",
      "Accuracy: 0.995\n",
      "Precision: 0.995\n",
      "Sensitivity: 0.964\n",
      "F1-Score: 0.979\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.982\n",
      "MCC: 0.977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'KNeighborsClassifier' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelKNN,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y4_pred = modelBAG.predict(x_test)\n",
    "y4_pred_proba = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y4_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y4_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "755b5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 99.32%\n",
      "Precision: 96.81%\n",
      "Recall or Sensitivity: 96.72%\n",
      "F1-Score: 0.9674\n",
      "Specificity or True Nagative Rate: 99.62%\n",
      "Balanced Accuracy: 98.16%\n",
      "\n",
      "MCC: 0.964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cbbce",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "900d9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[174   0   0   0   0   0   0   0   0   0]\n",
      " [  0 182   0   0  22   2   8   8   5   3]\n",
      " [  1   0 169   0   4   0   0   0   0   0]\n",
      " [  0   2   0 190   0   0   3   4   1   2]\n",
      " [  0   1   0   0 157   0   0   0   0   0]\n",
      " [  1   9   0   6   0 203   2   1   2   0]\n",
      " [  0   1   1   0   3   0 193   0   0   0]\n",
      " [  1   2   0   0   0   0   0 167   0   1]\n",
      " [  5   1   0   0  14   0   0   0 172   0]\n",
      " [  0   0   0   0   0   0   0   0   0 215]]\n",
      "Print Class: 0\n",
      "TP=174, FP=0, TN=1756, FN=8\n",
      "Accuracy: 0.996\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.956\n",
      "F1-Score: 0.978\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.978\n",
      "MCC: 0.976\n",
      "\n",
      "Print Class: 1\n",
      "TP=182, FP=48, TN=1692, FN=16\n",
      "Accuracy: 0.967\n",
      "Precision: 0.791\n",
      "Sensitivity: 0.919\n",
      "F1-Score: 0.85\n",
      "Specificity: 0.972\n",
      "Balanced Accuracy: 0.946\n",
      "MCC: 0.835\n",
      "\n",
      "Print Class: 2\n",
      "TP=169, FP=5, TN=1763, FN=1\n",
      "Accuracy: 0.997\n",
      "Precision: 0.971\n",
      "Sensitivity: 0.994\n",
      "F1-Score: 0.983\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.996\n",
      "MCC: 0.981\n",
      "\n",
      "Print Class: 3\n",
      "TP=190, FP=12, TN=1730, FN=6\n",
      "Accuracy: 0.991\n",
      "Precision: 0.941\n",
      "Sensitivity: 0.969\n",
      "F1-Score: 0.955\n",
      "Specificity: 0.993\n",
      "Balanced Accuracy: 0.981\n",
      "MCC: 0.95\n",
      "\n",
      "Print Class: 4\n",
      "TP=157, FP=1, TN=1737, FN=43\n",
      "Accuracy: 0.977\n",
      "Precision: 0.994\n",
      "Sensitivity: 0.785\n",
      "F1-Score: 0.877\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.892\n",
      "MCC: 0.872\n",
      "\n",
      "Print Class: 5\n",
      "TP=203, FP=21, TN=1712, FN=2\n",
      "Accuracy: 0.988\n",
      "Precision: 0.906\n",
      "Sensitivity: 0.99\n",
      "F1-Score: 0.946\n",
      "Specificity: 0.988\n",
      "Balanced Accuracy: 0.989\n",
      "MCC: 0.941\n",
      "\n",
      "Print Class: 6\n",
      "TP=193, FP=5, TN=1727, FN=13\n",
      "Accuracy: 0.991\n",
      "Precision: 0.975\n",
      "Sensitivity: 0.937\n",
      "F1-Score: 0.955\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.967\n",
      "MCC: 0.95\n",
      "\n",
      "Print Class: 7\n",
      "TP=167, FP=4, TN=1754, FN=13\n",
      "Accuracy: 0.991\n",
      "Precision: 0.977\n",
      "Sensitivity: 0.928\n",
      "F1-Score: 0.952\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.963\n",
      "MCC: 0.947\n",
      "\n",
      "Print Class: 8\n",
      "TP=172, FP=20, TN=1738, FN=8\n",
      "Accuracy: 0.986\n",
      "Precision: 0.896\n",
      "Sensitivity: 0.956\n",
      "F1-Score: 0.925\n",
      "Specificity: 0.989\n",
      "Balanced Accuracy: 0.972\n",
      "MCC: 0.917\n",
      "\n",
      "Print Class: 9\n",
      "TP=215, FP=0, TN=1717, FN=6\n",
      "Accuracy: 0.997\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.973\n",
      "F1-Score: 0.986\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.986\n",
      "MCC: 0.985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'RandomForestClassifier' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier(criterion='gini', n_estimators=100, random_state=0)\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelRF,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y5_pred = modelBAG.predict(x_test)\n",
    "y5_pred_proba = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y5_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y5_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89ee4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 98.81%\n",
      "Precision: 94.51%\n",
      "Recall or Sensitivity: 94.07%\n",
      "F1-Score: 0.9407\n",
      "Specificity or True Nagative Rate: 99.33%\n",
      "Balanced Accuracy: 96.7%\n",
      "\n",
      "MCC: 0.9354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f7d09",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e7b9f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180   0   0   0   0   0   0   0   0   0]\n",
      " [  0 196   0   4   4   1   4  16   2   3]\n",
      " [  1   0 169   0   4   0   0   0   0   0]\n",
      " [  0   1   0 192   0   0   0   3   1   2]\n",
      " [  0   1   1   0 181   0   0   0   0   1]\n",
      " [  1   0   0   0   1 202   1   0   0   0]\n",
      " [  0   0   0   0   1   0 201   0   0   0]\n",
      " [  0   0   0   0   2   2   0 161   0   2]\n",
      " [  0   0   0   0   5   0   0   0 174   0]\n",
      " [  0   0   0   0   2   0   0   0   3 213]]\n",
      "Print Class: 0\n",
      "TP=180, FP=0, TN=1756, FN=2\n",
      "Accuracy: 0.999\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.989\n",
      "F1-Score: 0.994\n",
      "Specificity: 1.0\n",
      "Balanced Accuracy: 0.994\n",
      "MCC: 0.994\n",
      "\n",
      "Print Class: 1\n",
      "TP=196, FP=34, TN=1706, FN=2\n",
      "Accuracy: 0.981\n",
      "Precision: 0.852\n",
      "Sensitivity: 0.99\n",
      "F1-Score: 0.916\n",
      "Specificity: 0.98\n",
      "Balanced Accuracy: 0.985\n",
      "MCC: 0.909\n",
      "\n",
      "Print Class: 2\n",
      "TP=169, FP=5, TN=1763, FN=1\n",
      "Accuracy: 0.997\n",
      "Precision: 0.971\n",
      "Sensitivity: 0.994\n",
      "F1-Score: 0.983\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.996\n",
      "MCC: 0.981\n",
      "\n",
      "Print Class: 3\n",
      "TP=192, FP=7, TN=1735, FN=4\n",
      "Accuracy: 0.994\n",
      "Precision: 0.965\n",
      "Sensitivity: 0.98\n",
      "F1-Score: 0.972\n",
      "Specificity: 0.996\n",
      "Balanced Accuracy: 0.988\n",
      "MCC: 0.969\n",
      "\n",
      "Print Class: 4\n",
      "TP=181, FP=3, TN=1735, FN=19\n",
      "Accuracy: 0.989\n",
      "Precision: 0.984\n",
      "Sensitivity: 0.905\n",
      "F1-Score: 0.943\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.952\n",
      "MCC: 0.937\n",
      "\n",
      "Print Class: 5\n",
      "TP=202, FP=3, TN=1730, FN=3\n",
      "Accuracy: 0.997\n",
      "Precision: 0.985\n",
      "Sensitivity: 0.985\n",
      "F1-Score: 0.985\n",
      "Specificity: 0.998\n",
      "Balanced Accuracy: 0.992\n",
      "MCC: 0.984\n",
      "\n",
      "Print Class: 6\n",
      "TP=201, FP=1, TN=1731, FN=5\n",
      "Accuracy: 0.997\n",
      "Precision: 0.995\n",
      "Sensitivity: 0.976\n",
      "F1-Score: 0.985\n",
      "Specificity: 0.999\n",
      "Balanced Accuracy: 0.988\n",
      "MCC: 0.984\n",
      "\n",
      "Print Class: 7\n",
      "TP=161, FP=6, TN=1752, FN=19\n",
      "Accuracy: 0.987\n",
      "Precision: 0.964\n",
      "Sensitivity: 0.894\n",
      "F1-Score: 0.928\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.946\n",
      "MCC: 0.922\n",
      "\n",
      "Print Class: 8\n",
      "TP=174, FP=5, TN=1753, FN=6\n",
      "Accuracy: 0.994\n",
      "Precision: 0.972\n",
      "Sensitivity: 0.967\n",
      "F1-Score: 0.969\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.982\n",
      "MCC: 0.966\n",
      "\n",
      "Print Class: 9\n",
      "TP=213, FP=5, TN=1712, FN=8\n",
      "Accuracy: 0.993\n",
      "Precision: 0.977\n",
      "Sensitivity: 0.964\n",
      "F1-Score: 0.97\n",
      "Specificity: 0.997\n",
      "Balanced Accuracy: 0.98\n",
      "MCC: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model.\n",
    "# We set'SVM' as a base estimator and set 100 to the number of estimators, then train the model\n",
    "# with train data.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = None, class_weight=None,probability=True)\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelSVMGaussian,\n",
    "n_estimators=100, # number of LR models to create\n",
    "max_samples=1.0, # each model is trained from randomly sampled 1 instance\n",
    "max_features=1.0,\n",
    "bootstrap=True, # set to False to use Pasting instead of Bagging\n",
    "bootstrap_features=False,\n",
    "oob_score=False,\n",
    "warm_start=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0)\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict model with test data\n",
    "\n",
    "y6_pred = modelBAG.predict(x_test)\n",
    "y6_pred_proba = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "\n",
    "print(confusion_matrix(y6_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y6_pred\n",
    "\n",
    "# Class = Label 0-9\n",
    "\n",
    "lst_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Compute multi-class confusion matrix\n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = [];\n",
    "model_recall = [];\n",
    "model_prec = [];\n",
    "model_fscore = [];\n",
    "model_spec = [];\n",
    "model_bal_acc = [];\n",
    "model_mcc = [];\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class];\n",
    "    print(\"Print Class: {0}\".format(no_class));\n",
    "\n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3);\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "\n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(x), 3)\n",
    "    model_acc.append(accuracy);\n",
    "    model_prec.append(precision);\n",
    "    model_recall.append(sensitivity);\n",
    "    model_fscore.append(f1Score);\n",
    "    model_spec.append(specificity);\n",
    "    model_bal_acc.append(balanced_accuracy);\n",
    "    model_mcc.append(MCC);\n",
    "\n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1Score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87a1f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Performance Prediction:\n",
      "Accuracy: 99.28%\n",
      "Precision: 96.65%\n",
      "Recall or Sensitivity: 96.44%\n",
      "F1-Score: 0.9645\n",
      "Specificity or True Nagative Rate: 99.59%\n",
      "Balanced Accuracy: 98.03%\n",
      "\n",
      "MCC: 0.9613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall Performance Prediction:\");\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100, 4)));\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_fscore), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49443395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
