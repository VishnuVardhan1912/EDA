{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f5b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc145dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote_Num</th>\n",
       "      <th>Agent_Type</th>\n",
       "      <th>Q_Creation_DT</th>\n",
       "      <th>Q_Valid_DT</th>\n",
       "      <th>Policy_Bind_DT</th>\n",
       "      <th>Region</th>\n",
       "      <th>Agent_Num</th>\n",
       "      <th>Policy_Type</th>\n",
       "      <th>HH_Vehicles</th>\n",
       "      <th>HH_Drivers</th>\n",
       "      <th>...</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sal_Range</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Veh_Usage</th>\n",
       "      <th>Annual_Miles_Range</th>\n",
       "      <th>Vehicl_Cost_Range</th>\n",
       "      <th>Re_Quote</th>\n",
       "      <th>Quoted_Premium</th>\n",
       "      <th>Policy_Bind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AQ-C-139212</td>\n",
       "      <td>EA</td>\n",
       "      <td>2020/04/25</td>\n",
       "      <td>2020/06/23</td>\n",
       "      <td>2020/05/23</td>\n",
       "      <td>C</td>\n",
       "      <td>2156</td>\n",
       "      <td>Car</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Widow</td>\n",
       "      <td>High School</td>\n",
       "      <td>&gt; $ 25 K &lt;= $ 40 K</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>Commute</td>\n",
       "      <td>&gt; 55 K</td>\n",
       "      <td>&gt; $ 10 K &lt;= $ 20 K</td>\n",
       "      <td>No</td>\n",
       "      <td>693.86</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQ-F-136117</td>\n",
       "      <td>EA</td>\n",
       "      <td>2020/02/21</td>\n",
       "      <td>2020/04/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2153</td>\n",
       "      <td>Van</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Dirvorced</td>\n",
       "      <td>Ph.D</td>\n",
       "      <td>&gt; $ 40 K &lt;= $ 60 K</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>&gt; 7.5 K &amp; &lt;= 15 K</td>\n",
       "      <td>&lt;= $ 10 K</td>\n",
       "      <td>No</td>\n",
       "      <td>635.96</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AQ-F-126801</td>\n",
       "      <td>EA</td>\n",
       "      <td>2020/06/19</td>\n",
       "      <td>2020/08/17</td>\n",
       "      <td>2020/07/12</td>\n",
       "      <td>F</td>\n",
       "      <td>2056</td>\n",
       "      <td>Truck</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Dirvorced</td>\n",
       "      <td>Ph.D</td>\n",
       "      <td>&gt; $ 40 K &lt;= $ 60 K</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Commute</td>\n",
       "      <td>&gt; 35 K &amp; &lt;= 45 K</td>\n",
       "      <td>&gt; $ 10 K &lt;= $ 20 K</td>\n",
       "      <td>No</td>\n",
       "      <td>780.64</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AQ-E-143467</td>\n",
       "      <td>EA</td>\n",
       "      <td>2020/05/02</td>\n",
       "      <td>2020/06/30</td>\n",
       "      <td>2020/05/24</td>\n",
       "      <td>E</td>\n",
       "      <td>2138</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>Ph.D</td>\n",
       "      <td>&gt; $ 90 K</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>&lt;= 7.5 K</td>\n",
       "      <td>&lt;= $ 10 K</td>\n",
       "      <td>No</td>\n",
       "      <td>723.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQ-C-143827</td>\n",
       "      <td>EA</td>\n",
       "      <td>2020/02/12</td>\n",
       "      <td>2020/04/11</td>\n",
       "      <td>2020/02/25</td>\n",
       "      <td>C</td>\n",
       "      <td>2327</td>\n",
       "      <td>Truck</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Widow</td>\n",
       "      <td>High School</td>\n",
       "      <td>&lt;= $ 25 K</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>&gt; 35 K &amp; &lt;= 45 K</td>\n",
       "      <td>&lt;= $ 10 K</td>\n",
       "      <td>No</td>\n",
       "      <td>738.14</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Quote_Num Agent_Type Q_Creation_DT  Q_Valid_DT Policy_Bind_DT Region  \\\n",
       "0  AQ-C-139212         EA    2020/04/25  2020/06/23     2020/05/23      C   \n",
       "1  AQ-F-136117         EA    2020/02/21  2020/04/20            NaN      F   \n",
       "2  AQ-F-126801         EA    2020/06/19  2020/08/17     2020/07/12      F   \n",
       "3  AQ-E-143467         EA    2020/05/02  2020/06/30     2020/05/24      E   \n",
       "4  AQ-C-143827         EA    2020/02/12  2020/04/11     2020/02/25      C   \n",
       "\n",
       "   Agent_Num Policy_Type  HH_Vehicles  HH_Drivers  ...  Marital_Status  \\\n",
       "0       2156         Car            3           3  ...           Widow   \n",
       "1       2153         Van            2           2  ...       Dirvorced   \n",
       "2       2056       Truck            2           1  ...       Dirvorced   \n",
       "3       2138         Car            1           2  ...         Married   \n",
       "4       2327       Truck            3           1  ...           Widow   \n",
       "\n",
       "     Education           Sal_Range  Coverage Veh_Usage Annual_Miles_Range  \\\n",
       "0  High School  > $ 25 K <= $ 40 K  Balanced   Commute             > 55 K   \n",
       "1         Ph.D  > $ 40 K <= $ 60 K  Balanced  Pleasure  > 7.5 K & <= 15 K   \n",
       "2         Ph.D  > $ 40 K <= $ 60 K     Basic   Commute   > 35 K & <= 45 K   \n",
       "3         Ph.D           > $ 90 K      Basic  Pleasure           <= 7.5 K   \n",
       "4  High School           <= $ 25 K     Basic  Pleasure   > 35 K & <= 45 K   \n",
       "\n",
       "    Vehicl_Cost_Range Re_Quote Quoted_Premium Policy_Bind  \n",
       "0  > $ 10 K <= $ 20 K       No         693.86         Yes  \n",
       "1           <= $ 10 K       No         635.96          No  \n",
       "2  > $ 10 K <= $ 20 K       No         780.64         Yes  \n",
       "3           <= $ 10 K       No         723.15         Yes  \n",
       "4           <= $ 10 K       No         738.14         Yes  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Auto Quote ins data\n",
    "\n",
    "AutoIns = pd.read_csv(r\"D:\\iiit_notes\\Programming\\AI\\Internship practice\\Project-01\\Auto_Quote_Data_V1.0.csv\", header=0)\n",
    "AutoIns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df35013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146259, 25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoIns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db510eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target varaible data type into integer\n",
    "AutoIns['Policy_Bind'] = AutoIns['Policy_Bind'].str.replace('No', '0')\n",
    "AutoIns['Policy_Bind'] = AutoIns['Policy_Bind'].str.replace('Yes', '1')\n",
    "AutoIns['Policy_Bind'] = AutoIns['Policy_Bind'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bbf22cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 113757\n",
      "Class 1: 32502\n",
      "Proportion: 3.5 : 1\n",
      "Total Records: 146259\n"
     ]
    }
   ],
   "source": [
    "# Count the target or dependent varaible by '0' or '1' and\n",
    "# their proportion (> 10 : 1, then the dataset is imbalance dataset)\n",
    "\n",
    "Policy_Bind_count = AutoIns.Policy_Bind.value_counts()\n",
    "print('Class 0:', Policy_Bind_count[0])\n",
    "print('Class 1:', Policy_Bind_count[1])\n",
    "print('Proportion:', round(Policy_Bind_count[0]/ Policy_Bind_count[1], 2), ': 1')\n",
    "print('Total Records:', len(AutoIns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e112241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146259 entries, 0 to 146258\n",
      "Data columns (total 25 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Quote_Num           146259 non-null  object \n",
      " 1   Agent_Type          146259 non-null  object \n",
      " 2   Q_Creation_DT       146259 non-null  object \n",
      " 3   Q_Valid_DT          146259 non-null  object \n",
      " 4   Policy_Bind_DT      32502 non-null   object \n",
      " 5   Region              146259 non-null  object \n",
      " 6   Agent_Num           146259 non-null  int64  \n",
      " 7   Policy_Type         146259 non-null  object \n",
      " 8   HH_Vehicles         146259 non-null  int64  \n",
      " 9   HH_Drivers          146259 non-null  int64  \n",
      " 10  Driver_Age          146259 non-null  int64  \n",
      " 11  Driving_Exp         146259 non-null  int64  \n",
      " 12  Prev_Accidents      146259 non-null  int64  \n",
      " 13  Prev_Citations      146259 non-null  int64  \n",
      " 14  Gender              146259 non-null  object \n",
      " 15  Marital_Status      146259 non-null  object \n",
      " 16  Education           146259 non-null  object \n",
      " 17  Sal_Range           146259 non-null  object \n",
      " 18  Coverage            146259 non-null  object \n",
      " 19  Veh_Usage           146259 non-null  object \n",
      " 20  Annual_Miles_Range  146259 non-null  object \n",
      " 21  Vehicl_Cost_Range   146259 non-null  object \n",
      " 22  Re_Quote            146259 non-null  object \n",
      " 23  Quoted_Premium      146259 non-null  float64\n",
      " 24  Policy_Bind         146259 non-null  int32  \n",
      "dtypes: float64(1), int32(1), int64(7), object(16)\n",
      "memory usage: 27.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display the Dataset information \n",
    "\n",
    "AutoIns.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a63e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranges and new column as 'QP_Range' from 'Quoted_premium'\n",
    "\n",
    "AutoIns['QP_Range'] = pd.cut(AutoIns['Quoted_Premium'], [0, 800, 1000, 1200, 9999],\n",
    "                             labels = ['0-800', '801-1000', '1001-1200', '>1200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930c5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the varaibles which are not impacting the target variable\n",
    "\n",
    "AutoIns = AutoIns.drop(['Quote_Num', 'Agent_Num', 'Q_Creation_DT',\n",
    "                        'Q_Valid_DT', 'Policy_Bind_DT', 'Sal_Range',\n",
    "                        'Vehicl_Cost_Range', 'Quoted_Premium'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d8ddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoIns = pd.get_dummies(AutoIns, columns=['Agent_Type','Region', 'Policy_Type', 'Gender', 'Marital_Status', 'Education',\n",
    "                                           'Coverage', 'Veh_Usage', 'Annual_Miles_Range', \n",
    "                                          'Re_Quote', 'QP_Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e21019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HH_Vehicles</th>\n",
       "      <th>HH_Drivers</th>\n",
       "      <th>Driver_Age</th>\n",
       "      <th>Driving_Exp</th>\n",
       "      <th>Prev_Accidents</th>\n",
       "      <th>Prev_Citations</th>\n",
       "      <th>Policy_Bind</th>\n",
       "      <th>Agent_Type_EA</th>\n",
       "      <th>Agent_Type_IA</th>\n",
       "      <th>Region_A</th>\n",
       "      <th>...</th>\n",
       "      <th>Annual_Miles_Range_&gt; 35 K &amp; &lt;= 45 K</th>\n",
       "      <th>Annual_Miles_Range_&gt; 45 K &amp; &lt;= 55 K</th>\n",
       "      <th>Annual_Miles_Range_&gt; 55 K</th>\n",
       "      <th>Annual_Miles_Range_&gt; 7.5 K &amp; &lt;= 15 K</th>\n",
       "      <th>Re_Quote_No</th>\n",
       "      <th>Re_Quote_Yes</th>\n",
       "      <th>QP_Range_0-800</th>\n",
       "      <th>QP_Range_801-1000</th>\n",
       "      <th>QP_Range_1001-1200</th>\n",
       "      <th>QP_Range_&gt;1200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HH_Vehicles  HH_Drivers  Driver_Age  Driving_Exp  Prev_Accidents  \\\n",
       "0            3           3          43           26               0   \n",
       "1            2           2          30           13               0   \n",
       "2            2           1          25            8               0   \n",
       "3            1           2          43           26               0   \n",
       "4            3           1          40           23               0   \n",
       "\n",
       "   Prev_Citations  Policy_Bind  Agent_Type_EA  Agent_Type_IA  Region_A  ...  \\\n",
       "0               0            1              1              0         0  ...   \n",
       "1               0            0              1              0         0  ...   \n",
       "2               0            1              1              0         0  ...   \n",
       "3               0            1              1              0         0  ...   \n",
       "4               0            1              1              0         0  ...   \n",
       "\n",
       "   Annual_Miles_Range_> 35 K & <= 45 K  Annual_Miles_Range_> 45 K & <= 55 K  \\\n",
       "0                                    0                                    0   \n",
       "1                                    0                                    0   \n",
       "2                                    1                                    0   \n",
       "3                                    0                                    0   \n",
       "4                                    1                                    0   \n",
       "\n",
       "   Annual_Miles_Range_> 55 K  Annual_Miles_Range_> 7.5 K & <= 15 K  \\\n",
       "0                          1                                     0   \n",
       "1                          0                                     1   \n",
       "2                          0                                     0   \n",
       "3                          0                                     0   \n",
       "4                          0                                     0   \n",
       "\n",
       "   Re_Quote_No  Re_Quote_Yes  QP_Range_0-800  QP_Range_801-1000  \\\n",
       "0            1             0               1                  0   \n",
       "1            1             0               1                  0   \n",
       "2            1             0               1                  0   \n",
       "3            1             0               1                  0   \n",
       "4            1             0               1                  0   \n",
       "\n",
       "   QP_Range_1001-1200  QP_Range_>1200  \n",
       "0                   0               0  \n",
       "1                   0               0  \n",
       "2                   0               0  \n",
       "3                   0               0  \n",
       "4                   0               0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the dataset after converting categorical to numeric data\n",
    "\n",
    "AutoIns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b27369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146259, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoIns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent and Target variables\n",
    "\n",
    "IndepVar =[]\n",
    "for col in AutoIns.columns:\n",
    "    if col != 'Policy_Bind':\n",
    "        IndepVar.append(col)\n",
    "        \n",
    "TargetVar = 'Policy_Bind'\n",
    "x = AutoIns[IndepVar]\n",
    "y = AutoIns[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7557f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5dfb7",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f5542",
   "metadata": {},
   "source": [
    "# Decision Tree with random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9248962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling - Each independent variable is in different range. The process of transforming all the\n",
    "# features in the given data set to a fixed range is known as 'Scaling'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train['Driver_Age'] = sc.fit_transform(x_train['Driver_Age'].values.reshape(-1, 1))\n",
    "x_train['Driving_Exp'] = sc.fit_transform(x_train['Driving_Exp'].values.reshape(-1, 1))\n",
    "x_test['Driver_Age'] = sc.fit_transform(x_test['Driver_Age'].values.reshape(-1, 1)) \n",
    "x_test['Driving_Exp'] = sc.fit_transform(x_test['Driving_Exp'].values.reshape(-1, 1)) \n",
    "# Convert to dataframe x_train = pd.DataFrame(x_train) x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e0d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree model with random sampling and Train the model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "AutoInsDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, \n",
    "                                   max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                   min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                                   min_weight_fraction_leaf=0.0, random_state=None, splitter='best') \n",
    "AutoInsDT = AutoInsDT.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58306228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[ 2411  7405]\n",
      " [ 8686 25376]]\n",
      "Outcome values : \n",
      " 2411 7405 8686 25376\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.25      0.23      9816\n",
      "           0       0.77      0.74      0.76     34062\n",
      "\n",
      "    accuracy                           0.63     43878\n",
      "   macro avg       0.50      0.50      0.49     43878\n",
      "weighted avg       0.65      0.63      0.64     43878\n",
      "\n",
      "Accuracy : 63.3 %\n",
      "Precision : 21.7 %\n",
      "Recall : 24.6 %\n",
      "F1 Score : 0.231\n",
      "Balanced Accuracy : 49.6 %\n",
      "MCC : -0.009\n"
     ]
    }
   ],
   "source": [
    "# Predict the model with test data \n",
    "    \n",
    "y_pred = AutoInsDT.predict(x_test)\n",
    "    \n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = y_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a881bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.495\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score1 = AutoInsDT.predict_proba(x_test)[:,1] \n",
    "    \n",
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_score1), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d9da0",
   "metadata": {},
   "source": [
    "# Decision tree with Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4197c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227514, 59)\n",
      "(227514,)\n"
     ]
    }
   ],
   "source": [
    "# Random oversampling can be implemented using the RandomOverSampler class\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "print(x_over.shape)\n",
    "print(y_over.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee674363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    113757\n",
       "1    113757\n",
       "Name: Policy_Bind, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know the count of major and minor classes\n",
    "\n",
    "y_over.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e95b997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xo_train, xo_test, yo_train, yo_test = train_test_split(x_over, y_over, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4b648b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling - Each independent variable is in different range. The process of transforming all the\n",
    "# features in the given data set to a fixed range is known as â€˜Scalingâ€™\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#x_train = sc.fit_transform(x_train)\n",
    "#x_test = sc.fit_transform(x_test)\n",
    "\n",
    "xo_train['Driver_Age'] = sc.fit_transform(xo_train['Driver_Age'].values.reshape(-1, 1))\n",
    "xo_train['Driving_Exp'] = sc.fit_transform(xo_train['Driving_Exp'].values.reshape(-1, 1))\n",
    "xo_test['Driver_Age'] = sc.fit_transform(xo_test['Driver_Age'].values.reshape(-1, 1))\n",
    "xo_test['Driving_Exp'] = sc.fit_transform(xo_test['Driving_Exp'].values.reshape(-1, 1))\n",
    "\n",
    "# convert to dataframe\n",
    "\n",
    "xo_train = pd.DataFrame(xo_train)\n",
    "xo_test = pd.DataFrame(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c654d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the decision tree model with Under Sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "AutoInsDT_O = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                      min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                      random_state=None, splitter='best')\n",
    "\n",
    "AutoInsDT_O = AutoInsDT_O.fit(xo_train, yo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40a7d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with test data\n",
    "\n",
    "yo_pred = AutoInsDT_O.predict(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96563a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[32119  2059]\n",
      " [ 9940 24137]]\n",
      "Outcome values : \n",
      " 32119 2059 9940 24137\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.94      0.84     34178\n",
      "           0       0.92      0.71      0.80     34077\n",
      "\n",
      "    accuracy                           0.82     68255\n",
      "   macro avg       0.84      0.82      0.82     68255\n",
      "weighted avg       0.84      0.82      0.82     68255\n",
      "\n",
      "Accuracy : 82.4 %\n",
      "Precision : 76.4 %\n",
      "Recall : 94.0 %\n",
      "F1 Score : 0.843\n",
      "Balanced Accuracy : 82.4 %\n",
      "MCC : 0.666\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yo_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yo_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b15ac3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score2 = AutoInsDT_O.predict_proba(xo_test)[:,1] \n",
    "    \n",
    "false_positive_rate2, true_positive_rate2, threshold2 = roc_curve(yo_test, y_score2) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yo_test, y_score2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f69525",
   "metadata": {},
   "source": [
    "# Combining Under & Oversampling - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a98fb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193386, 59)\n",
      "(193386,)\n",
      "(193384, 59)\n",
      "(193384,)\n"
     ]
    }
   ],
   "source": [
    "# Combining Random Oversampling and Undersampling \n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# define oversampling strategy\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy=0.70)\n",
    "\n",
    "# fit and apply the transform\n",
    "\n",
    "x2, y2 = over.fit_resample(x,y)\n",
    "print(x2.shape)\n",
    "print(y2.shape)\n",
    "\n",
    "# define undersampling strategy\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.70)\n",
    "\n",
    "# fit and apply the transform\n",
    "\n",
    "x3, y3 = under.fit_resample(x2, y2)\n",
    "\n",
    "print(x3.shape)\n",
    "print(y3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c4dd20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    113755\n",
       "1     79629\n",
       "Name: Policy_Bind, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know the count of major and minor classes\n",
    "\n",
    "y3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6864237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xc_train, xc_test, yc_train, yc_test = train_test_split(x3, y3, test_size = 0.30, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3393c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling - Each independent variable is in different range. The process of transforming all the\n",
    "# features in the given data set to a fixed range is known as â€˜Scalingâ€™\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "xc_train['Driver_Age'] = sc.fit_transform(xc_train['Driver_Age'].values.reshape(-1, 1))\n",
    "xc_train['Driving_Exp'] = sc.fit_transform(xc_train['Driving_Exp'].values.reshape(-1, 1))\n",
    "xc_test['Driver_Age'] = sc.fit_transform(xc_test['Driver_Age'].values.reshape(-1, 1))\n",
    "xc_test['Driving_Exp'] = sc.fit_transform(xc_test['Driving_Exp'].values.reshape(-1, 1))\n",
    "\n",
    "# convert to dataframe\n",
    "\n",
    "xc_train = pd.DataFrame(xc_train)\n",
    "xc_test = pd.DataFrame(xc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7128b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the decision tree model with Under sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "AutoInsDT_C = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                                     max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                     min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                     random_state=None, splitter='best')\n",
    "\n",
    "AutoInsDT_C = AutoInsDT_C.fit(xc_train,yc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79883e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with test data\n",
    "\n",
    "yc_pred = AutoInsDT_C.predict(xc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9efac6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[20381  3631]\n",
      " [ 9703 24301]]\n",
      "Outcome values : \n",
      " 20381 3631 9703 24301\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.85      0.75     24012\n",
      "           0       0.87      0.71      0.78     34004\n",
      "\n",
      "    accuracy                           0.77     58016\n",
      "   macro avg       0.77      0.78      0.77     58016\n",
      "weighted avg       0.79      0.77      0.77     58016\n",
      "\n",
      "Accuracy : 77.0 %\n",
      "Precision : 67.7 %\n",
      "Recall : 84.9 %\n",
      "F1 Score : 0.754\n",
      "Balanced Accuracy : 78.2 %\n",
      "MCC : 0.555\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yc_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yc_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be28f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.782\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score3 = AutoInsDT_C.predict_proba(xc_test)[:,1] \n",
    "    \n",
    "false_positive_rate3, true_positive_rate3, threshold3 = roc_curve(yc_test, y_score3) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yc_test, y_score3), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800229dc",
   "metadata": {},
   "source": [
    "# Under sampling -Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c409be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65004, 59)\n",
      "(65004,)\n"
     ]
    }
   ],
   "source": [
    "# undersampling \n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy = 'majority')\n",
    "x_under, y_under = under.fit_resample(x, y)\n",
    "\n",
    "print(x_under.shape)\n",
    "print(y_under.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3682b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32502\n",
       "1    32502\n",
       "Name: Policy_Bind, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know the count of major and minor classes\n",
    "\n",
    "y_under.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efb40edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xu_train, xu_test, yu_train, yu_test = train_test_split(x_under, y_under, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80f3ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling - Each independent variable is in different range. The process of transforming all the\n",
    "# features in the given data set to a fixed range is known as â€˜Scalingâ€™\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "xu_train['Driver_Age'] = sc.fit_transform(xu_train['Driver_Age'].values.reshape(-1,1))\n",
    "xu_train['Driving_Exp'] = sc.fit_transform(xu_train['Driving_Exp'].values.reshape(-1,1))\n",
    "xu_train['Driver_Age'] = sc.fit_transform(xu_train['Driver_Age'].values.reshape(-1,1))\n",
    "xu_train['Driving_Exp'] = sc.fit_transform(xu_train['Driving_Exp'].values.reshape(-1,1))\n",
    "\n",
    "# convert to dataframe\n",
    "\n",
    "xu_train =  pd.DataFrame(xu_train)\n",
    "xu_test = pd.DataFrame(xu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "486f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the decision tree model with Under sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "AutoInsDT_C = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "                                     min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                     random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                     min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "AutoInsDT_C = AutoInsDT_C.fit(xu_train, yu_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9142376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with test data\n",
    "\n",
    "yu_pred = AutoInsDT_C.predict(xu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90933224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[5413 4336]\n",
      " [5351 4402]]\n",
      "Outcome values : \n",
      " 5413 4336 5351 4402\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.56      0.53      9749\n",
      "           0       0.50      0.45      0.48      9753\n",
      "\n",
      "    accuracy                           0.50     19502\n",
      "   macro avg       0.50      0.50      0.50     19502\n",
      "weighted avg       0.50      0.50      0.50     19502\n",
      "\n",
      "Accuracy : 50.3 %\n",
      "Precision : 50.3 %\n",
      "Recall : 55.5 %\n",
      "F1 Score : 0.528\n",
      "Balanced Accuracy : 50.3 %\n",
      "MCC : 0.007\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yu_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yu_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "196d07c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score4 = AutoInsDT_C.predict_proba(xu_test)[:,1] \n",
    "    \n",
    "false_positive_rate4, true_positive_rate4, threshold4 = roc_curve(yu_test, y_score4) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yu_test, y_score4), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b01c03",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf483a7",
   "metadata": {},
   "source": [
    "# Randomforest with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46e157aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[31321  2857]\n",
      " [  367 33710]]\n",
      "Outcome values : \n",
      " 31321 2857 367 33710\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.92      0.95     34178\n",
      "           0       0.92      0.99      0.95     34077\n",
      "\n",
      "    accuracy                           0.95     68255\n",
      "   macro avg       0.96      0.95      0.95     68255\n",
      "weighted avg       0.96      0.95      0.95     68255\n",
      "\n",
      "Accuracy : 95.3 %\n",
      "Precision : 98.8 %\n",
      "Recall : 91.6 %\n",
      "F1 Score : 0.951\n",
      "Balanced Accuracy : 95.2 %\n",
      "MCC : 0.908\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "AutoInsRF_O = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "criterion='gini', max_depth=None, max_features='auto',\n",
    "max_leaf_nodes=None, max_samples=None,\n",
    "min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "min_samples_leaf=1, min_samples_split=2,\n",
    "min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "warm_start=False)\n",
    "AutoInsRF_O = AutoInsRF_O.fit(xo_train, yo_train)\n",
    "\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yo1_pred = AutoInsRF_O.predict(xo_test)\n",
    "   \n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yo_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yo1_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb0ec99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.958\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score5 = AutoInsRF_O.predict_proba(xo_test)[:,1] \n",
    "    \n",
    "false_positive_rate5, true_positive_rate5, threshold5 = roc_curve(yo_test, y_score5) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yo_test, y_score5), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c804ff5",
   "metadata": {},
   "source": [
    "# RandomForest with Combined Over sampling and Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a6d4755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[18905  5107]\n",
      " [  139 33865]]\n",
      "Outcome values : \n",
      " 18905 5107 139 33865\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.79      0.88     24012\n",
      "           0       0.87      1.00      0.93     34004\n",
      "\n",
      "    accuracy                           0.91     58016\n",
      "   macro avg       0.93      0.89      0.90     58016\n",
      "weighted avg       0.92      0.91      0.91     58016\n",
      "\n",
      "Accuracy : 91.0 %\n",
      "Precision : 99.3 %\n",
      "Recall : 78.7 %\n",
      "F1 Score : 0.878\n",
      "Balanced Accuracy : 89.2 %\n",
      "MCC : 0.822\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "AutoInsRF_C = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "criterion='gini', max_depth=None, max_features='auto',\n",
    "max_leaf_nodes=None, max_samples=None,\n",
    "min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "min_samples_leaf=1, min_samples_split=2,\n",
    "min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "warm_start=False)\n",
    "AutoInsRF_C = AutoInsRF_C.fit(xc_train, yc_train)\n",
    "\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yc1_pred = AutoInsRF_C.predict(xc_test)\n",
    "\n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yc_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yc1_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd0742ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score6 = AutoInsRF_C.predict_proba(xc_test)[:,1] \n",
    "    \n",
    "false_positive_rate6, true_positive_rate6, threshold6 = roc_curve(yc_test, y_score6) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yc_test, y_score6), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304f2e4",
   "metadata": {},
   "source": [
    "# RandomForest with Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30ac41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[6542 3207]\n",
      " [6524 3229]]\n",
      "Outcome values : \n",
      " 6542 3207 6524 3229\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.67      0.57      9749\n",
      "           0       0.50      0.33      0.40      9753\n",
      "\n",
      "    accuracy                           0.50     19502\n",
      "   macro avg       0.50      0.50      0.49     19502\n",
      "weighted avg       0.50      0.50      0.49     19502\n",
      "\n",
      "Accuracy : 50.1 %\n",
      "Precision : 50.1 %\n",
      "Recall : 67.1 %\n",
      "F1 Score : 0.573\n",
      "Balanced Accuracy : 50.1 %\n",
      "MCC : 0.002\n",
      "roc_auc_score: 0.501\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "AutoInsRF_U = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "criterion='gini', max_depth=None, max_features='auto',\n",
    "max_leaf_nodes=None, max_samples=None,\n",
    "min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "min_samples_leaf=1, min_samples_split=2,\n",
    "min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "warm_start=False)\n",
    "AutoInsRF_U = AutoInsRF_U.fit(xu_train, yu_train)\n",
    "\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yu1_pred = AutoInsRF_U.predict(xu_test)\n",
    "\n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yu_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yu1_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score7 = AutoInsRF_U.predict_proba(xu_test)[:,1] \n",
    "    \n",
    "false_positive_rate7, true_positive_rate7, threshold7 = roc_curve(yu_test, y_score7) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yu_test, y_score7), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287a590",
   "metadata": {},
   "source": [
    "# RandomForest with randam sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da276f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[    4  9812]\n",
      " [   11 34051]]\n",
      "Outcome values : \n",
      " 4 9812 11 34051\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.00      0.00      9816\n",
      "           0       0.78      1.00      0.87     34062\n",
      "\n",
      "    accuracy                           0.78     43878\n",
      "   macro avg       0.52      0.50      0.44     43878\n",
      "weighted avg       0.66      0.78      0.68     43878\n",
      "\n",
      "Accuracy : 77.6 %\n",
      "Precision : 26.7 %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.001\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC : 0.002\n",
      "roc_auc_score: 0.499\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest classification model and Train the model using the training sets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "AutoInsRF = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "criterion='gini', max_depth=None, max_features='auto',\n",
    "max_leaf_nodes=None, max_samples=None,\n",
    "min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "min_samples_leaf=1, min_samples_split=2,\n",
    "min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "warm_start=False)\n",
    "AutoInsRF = AutoInsRF.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Predict the model with test data set\n",
    "    \n",
    "y2_pred = AutoInsRF.predict(x_test)\n",
    " \n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = y2_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score8 = AutoInsRF.predict_proba(x_test)[:,1] \n",
    "    \n",
    "false_positive_rate8, true_positive_rate8, threshold8 = roc_curve(y_test, y_score8) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_score8), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd49b6c",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b493046",
   "metadata": {},
   "source": [
    "# Oversampling - Logestic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b48b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[17938 16240]\n",
      " [17553 16524]]\n",
      "Outcome values : \n",
      " 17938 16240 17553 16524\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.52      0.51     34178\n",
      "           0       0.50      0.48      0.49     34077\n",
      "\n",
      "    accuracy                           0.50     68255\n",
      "   macro avg       0.50      0.50      0.50     68255\n",
      "weighted avg       0.50      0.50      0.50     68255\n",
      "\n",
      "Accuracy : 50.5 %\n",
      "Precision : 50.5 %\n",
      "Recall : 52.5 %\n",
      "F1 Score : 0.515\n",
      "Balanced Accuracy : 50.5 %\n",
      "MCC : 0.01\n",
      "roc_auc_score: 0.507\n"
     ]
    }
   ],
   "source": [
    "# To bulid the 'Logistic Regression' model with oversampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AutoInsLR_O = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "n_jobs=None, penalty='l2', random_state=None,\n",
    "solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "AutoInsLR_O = AutoInsLR_O.fit(xo_train,yo_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yo3_pred = AutoInsLR_O.predict(xo_test)\n",
    " \n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yo_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yo3_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score9 = AutoInsLR_O.predict_proba(xo_test)[:,1] \n",
    "    \n",
    "false_positive_rate9, true_positive_rate9, threshold9 = roc_curve(yo_test, y_score9) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yo_test, y_score9), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2684ee",
   "metadata": {},
   "source": [
    "# Combining under - Over sampling Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e79d1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[    0 24012]\n",
      " [    0 34004]]\n",
      "Outcome values : \n",
      " 0 24012 0 34004\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00     24012\n",
      "           0       0.59      1.00      0.74     34004\n",
      "\n",
      "    accuracy                           0.59     58016\n",
      "   macro avg       0.29      0.50      0.37     58016\n",
      "weighted avg       0.34      0.59      0.43     58016\n",
      "\n",
      "Accuracy : 58.6 %\n",
      "Precision : nan %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.0\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC : nan\n",
      "roc_auc_score: 0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AutoInsLR_C = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "n_jobs=None, penalty='l2', random_state=None,\n",
    "solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "AutoInsLR_C = AutoInsLR_C.fit(xc_train,yc_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yc3_pred = AutoInsLR_C.predict(xc_test)\n",
    "\n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yc_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yc3_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score10 = AutoInsLR_C.predict_proba(xo_test)[:,1] \n",
    "    \n",
    "false_positive_rate10, true_positive_rate10, threshold10 = roc_curve(yo_test, y_score10) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yo_test, y_score10), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc0ad7",
   "metadata": {},
   "source": [
    "# Under sampling with Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56faff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[2699 7050]\n",
      " [2700 7053]]\n",
      "Outcome values : \n",
      " 2699 7050 2700 7053\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.28      0.36      9749\n",
      "           0       0.50      0.72      0.59      9753\n",
      "\n",
      "    accuracy                           0.50     19502\n",
      "   macro avg       0.50      0.50      0.47     19502\n",
      "weighted avg       0.50      0.50      0.47     19502\n",
      "\n",
      "Accuracy : 50.0 %\n",
      "Precision : 50.0 %\n",
      "Recall : 27.7 %\n",
      "F1 Score : 0.356\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC : 0.0\n",
      "roc_auc_score: 0.503\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AutoInsLR_U = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "n_jobs=None, penalty='l2', random_state=None,\n",
    "solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "AutoInsLR_U = AutoInsLR_U.fit(xu_train,yu_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "yu3_pred = AutoInsLR_U.predict(xu_test)\n",
    "   \n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = yu_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = yu3_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC) \n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score11 = AutoInsLR_U.predict_proba(xu_test)[:,1] \n",
    "    \n",
    "false_positive_rate11, true_positive_rate11, threshold11 = roc_curve(yu_test, y_score11) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(yu_test, y_score11), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef5870",
   "metadata": {},
   "source": [
    "# Ramdom sampling with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "373d21ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[    0  9816]\n",
      " [    0 34062]]\n",
      "Outcome values : \n",
      " 0 9816 0 34062\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      9816\n",
      "           0       0.78      1.00      0.87     34062\n",
      "\n",
      "    accuracy                           0.78     43878\n",
      "   macro avg       0.39      0.50      0.44     43878\n",
      "weighted avg       0.60      0.78      0.68     43878\n",
      "\n",
      "Accuracy : 77.6 %\n",
      "Precision : nan %\n",
      "Recall : 0.0 %\n",
      "F1 Score : 0.0\n",
      "Balanced Accuracy : 50.0 %\n",
      "MCC : nan\n",
      "roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AutoInsLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "n_jobs=None, penalty='l2', random_state=None,\n",
    "solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "AutoInsLR = AutoInsLR.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y3_pred = AutoInsLR.predict(x_test)\n",
    "\n",
    "# confusion matrix in sklearn \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values \n",
    "\n",
    "predicted = y3_pred \n",
    "\n",
    "# confusion matrix \n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None) \n",
    "print('Confusion matrix : \\n', matrix) \n",
    "\n",
    "# outcome values order in sklearn \n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1) \n",
    "\n",
    "print('Outcome values : \\n', tp, fn, fp, tn) \n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy \n",
    "\n",
    "matrix = classification_report(actual,predicted,labels=[1,0]) \n",
    "print('Classification report : \\n',matrix) \n",
    "\n",
    "# calculating the metrics \n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3) \n",
    "specificity = round(tn/(tn+fp), 3); \n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3); \n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3); \n",
    "precision = round(tp/(tp+fp), 3); \n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model \n",
    "\n",
    "from math import sqrt \n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) \n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3) \n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%') \n",
    "print('Precision :', round(precision*100, 2),'%') \n",
    "print('Recall :', round(sensitivity*100,2), '%') \n",
    "print('F1 Score :', f1Score) \n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%') \n",
    "print('MCC :', MCC) \n",
    "\n",
    "# Area under ROC curve \n",
    "    \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "    \n",
    "y_score12 = AutoInsLR.predict_proba(x_test)[:,1] \n",
    "    \n",
    "false_positive_rate12, true_positive_rate12, threshold12 = roc_curve(y_test, y_score12) \n",
    "    \n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_score12), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b4f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
