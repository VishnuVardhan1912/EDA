{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0e3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessory the libraries\n",
    "\n",
    "# Dataframe operations - pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Math calculations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plotting data - matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data visulization library\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataframes using sql - To run queries \n",
    "\n",
    "import pandasql as psql\n",
    "\n",
    "# Ignore harmless warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb0c53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rec_ID</th>\n",
       "      <th>Policy_number</th>\n",
       "      <th>Agent_Type</th>\n",
       "      <th>Process_Dt</th>\n",
       "      <th>Days_Process_Bind</th>\n",
       "      <th>Bind_Dt</th>\n",
       "      <th>Eff_Dt</th>\n",
       "      <th>Renewal_Dt</th>\n",
       "      <th>State</th>\n",
       "      <th>Policy_Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Prev_Citations</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sal_Range</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Veh_Usage</th>\n",
       "      <th>Annual_Miles_Range</th>\n",
       "      <th>Vehicle_Cost_Range</th>\n",
       "      <th>Claims_Amount_Range</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>123459912</td>\n",
       "      <td>EA</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>20</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021-12-14</td>\n",
       "      <td>CA</td>\n",
       "      <td>Car</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>123673362</td>\n",
       "      <td>EA</td>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>8</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>2021-09-03</td>\n",
       "      <td>OR</td>\n",
       "      <td>Car</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>123978119</td>\n",
       "      <td>EA</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>OR</td>\n",
       "      <td>Van</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>123663280</td>\n",
       "      <td>EA</td>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Car</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>123686557</td>\n",
       "      <td>EA</td>\n",
       "      <td>2021-07-03</td>\n",
       "      <td>20</td>\n",
       "      <td>2021-07-23</td>\n",
       "      <td>2021-07-23</td>\n",
       "      <td>2022-01-20</td>\n",
       "      <td>OR</td>\n",
       "      <td>Car</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rec_ID  Policy_number Agent_Type Process_Dt  Days_Process_Bind    Bind_Dt  \\\n",
       "0       1      123459912         EA 2021-05-27                 20 2021-06-16   \n",
       "1       2      123673362         EA 2021-02-26                  8 2021-03-06   \n",
       "2       3      123978119         EA 2021-07-01                  5 2021-07-06   \n",
       "3       4      123663280         EA 2021-06-22                  2 2021-06-24   \n",
       "4       5      123686557         EA 2021-07-03                 20 2021-07-23   \n",
       "\n",
       "      Eff_Dt Renewal_Dt State Policy_Type  ...  Prev_Citations  \\\n",
       "0 2021-06-16 2021-12-14    CA         Car  ...               1   \n",
       "1 2021-03-06 2021-09-03    OR         Car  ...               1   \n",
       "2 2021-07-06 2022-01-03    OR         Van  ...               1   \n",
       "3 2021-06-24 2021-12-22    AZ         Car  ...               0   \n",
       "4 2021-07-23 2022-01-20    OR         Car  ...               0   \n",
       "\n",
       "   Marital_Status  Education  Sal_Range  Coverage Veh_Usage  \\\n",
       "0               1          4          2         1         2   \n",
       "1               3          4          5         1         1   \n",
       "2               3          1          4         1         2   \n",
       "3               1          3          2         1         1   \n",
       "4               1          1          7         1         3   \n",
       "\n",
       "   Annual_Miles_Range  Vehicle_Cost_Range  Claims_Amount_Range  Unnamed: 28  \n",
       "0                   2                   4                    0          NaN  \n",
       "1                   1                   3                    0          NaN  \n",
       "2                   6                   1                    0          NaN  \n",
       "3                   8                   3                    0          NaN  \n",
       "4                   5                   3                    0          NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Auto_Insurance dataset\n",
    "\n",
    "AutoIns = pd.read_excel(r\"D:\\iiit notes\\Programming\\AI\\Internship practice\\Capstone project - 02\\Auto_Insurance_Data V 1.0.xlsx\",sheet_name=0, header=0)\n",
    "\n",
    "# Display first five records \n",
    "\n",
    "AutoIns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5d6fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37492 entries, 0 to 37491\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   Rec_ID               37492 non-null  int64         \n",
      " 1   Policy_number        37492 non-null  int64         \n",
      " 2   Agent_Type           37492 non-null  object        \n",
      " 3   Process_Dt           37492 non-null  datetime64[ns]\n",
      " 4   Days_Process_Bind    37492 non-null  int64         \n",
      " 5   Bind_Dt              37492 non-null  datetime64[ns]\n",
      " 6   Eff_Dt               37492 non-null  datetime64[ns]\n",
      " 7   Renewal_Dt           37492 non-null  datetime64[ns]\n",
      " 8   State                37492 non-null  object        \n",
      " 9   Policy_Type          37492 non-null  object        \n",
      " 10  NW_Premium           37492 non-null  float64       \n",
      " 11  Taxes                37492 non-null  float64       \n",
      " 12  GW_Premium           37492 non-null  float64       \n",
      " 13  Num_Drivers          37492 non-null  int64         \n",
      " 14  Num_Vehicles         37492 non-null  int64         \n",
      " 15  Gender               37492 non-null  object        \n",
      " 16  Age                  37492 non-null  int64         \n",
      " 17  Driving_Exp          37492 non-null  int64         \n",
      " 18  Prev_Accidents       37492 non-null  int64         \n",
      " 19  Prev_Citations       37492 non-null  int64         \n",
      " 20  Marital_Status       37492 non-null  int64         \n",
      " 21  Education            37492 non-null  int64         \n",
      " 22  Sal_Range            37492 non-null  int64         \n",
      " 23  Coverage             37492 non-null  int64         \n",
      " 24  Veh_Usage            37492 non-null  int64         \n",
      " 25  Annual_Miles_Range   37492 non-null  int64         \n",
      " 26  Vehicle_Cost_Range   37492 non-null  int64         \n",
      " 27  Claims_Amount_Range  37492 non-null  int64         \n",
      " 28  Unnamed: 28          0 non-null      float64       \n",
      "dtypes: datetime64[ns](4), float64(4), int64(17), object(4)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset information\n",
    "\n",
    "AutoIns.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023d30c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rec_ID</th>\n",
       "      <th>Policy_number</th>\n",
       "      <th>Agent_Type</th>\n",
       "      <th>Process_Dt</th>\n",
       "      <th>Days_Process_Bind</th>\n",
       "      <th>Bind_Dt</th>\n",
       "      <th>Eff_Dt</th>\n",
       "      <th>Renewal_Dt</th>\n",
       "      <th>State</th>\n",
       "      <th>Policy_Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Prev_Citations</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sal_Range</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Veh_Usage</th>\n",
       "      <th>Annual_Miles_Range</th>\n",
       "      <th>Vehicle_Cost_Range</th>\n",
       "      <th>Claims_Amount_Range</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rec_ID, Policy_number, Agent_Type, Process_Dt, Days_Process_Bind, Bind_Dt, Eff_Dt, Renewal_Dt, State, Policy_Type, NW_Premium, Taxes, GW_Premium, Num_Drivers, Num_Vehicles, Gender, Age, Driving_Exp, Prev_Accidents, Prev_Citations, Marital_Status, Education, Sal_Range, Coverage, Veh_Usage, Annual_Miles_Range, Vehicle_Cost_Range, Claims_Amount_Range, Unnamed: 28]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Dupicate values with in dataset\n",
    "\n",
    "AutoIns_dup = AutoIns[AutoIns.duplicated(keep='last')]\n",
    "AutoIns_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61da01a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rec_ID                     0\n",
       "Policy_number              0\n",
       "Agent_Type                 0\n",
       "Process_Dt                 0\n",
       "Days_Process_Bind          0\n",
       "Bind_Dt                    0\n",
       "Eff_Dt                     0\n",
       "Renewal_Dt                 0\n",
       "State                      0\n",
       "Policy_Type                0\n",
       "NW_Premium                 0\n",
       "Taxes                      0\n",
       "GW_Premium                 0\n",
       "Num_Drivers                0\n",
       "Num_Vehicles               0\n",
       "Gender                     0\n",
       "Age                        0\n",
       "Driving_Exp                0\n",
       "Prev_Accidents             0\n",
       "Prev_Citations             0\n",
       "Marital_Status             0\n",
       "Education                  0\n",
       "Sal_Range                  0\n",
       "Coverage                   0\n",
       "Veh_Usage                  0\n",
       "Annual_Miles_Range         0\n",
       "Vehicle_Cost_Range         0\n",
       "Claims_Amount_Range        0\n",
       "Unnamed: 28            37492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is any Missing values present in the dataset\n",
    "\n",
    "AutoIns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42cbf9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rec_ID                 37492\n",
       "Policy_number          36221\n",
       "Agent_Type                 2\n",
       "Process_Dt               211\n",
       "Days_Process_Bind         31\n",
       "Bind_Dt                  181\n",
       "Eff_Dt                   181\n",
       "Renewal_Dt               181\n",
       "State                      4\n",
       "Policy_Type                3\n",
       "NW_Premium             33205\n",
       "Taxes                   8385\n",
       "GW_Premium             33192\n",
       "Num_Drivers                4\n",
       "Num_Vehicles               4\n",
       "Gender                     2\n",
       "Age                       48\n",
       "Driving_Exp               48\n",
       "Prev_Accidents             2\n",
       "Prev_Citations             2\n",
       "Marital_Status             4\n",
       "Education                  4\n",
       "Sal_Range                  7\n",
       "Coverage                   3\n",
       "Veh_Usage                  3\n",
       "Annual_Miles_Range         8\n",
       "Vehicle_Cost_Range         8\n",
       "Claims_Amount_Range        7\n",
       "Unnamed: 28                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display nunique values of each variable\n",
    "\n",
    "AutoIns.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe801c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049     1\n",
      "4791     1\n",
      "25241    1\n",
      "31386    1\n",
      "29339    1\n",
      "        ..\n",
      "5448     1\n",
      "7497     1\n",
      "1354     1\n",
      "3403     1\n",
      "2047     1\n",
      "Name: Rec_ID, Length: 37492, dtype: int64\n",
      "123894605    4\n",
      "123984952    3\n",
      "123674946    3\n",
      "123580319    3\n",
      "123901327    3\n",
      "            ..\n",
      "123705048    1\n",
      "123530967    1\n",
      "123850451    1\n",
      "123797167    1\n",
      "123602943    1\n",
      "Name: Policy_number, Length: 36221, dtype: int64\n",
      "EA    24911\n",
      "IA    12581\n",
      "Name: Agent_Type, dtype: int64\n",
      "2021-06-09    253\n",
      "2021-06-20    249\n",
      "2021-06-17    247\n",
      "2021-06-23    246\n",
      "2021-06-30    245\n",
      "             ... \n",
      "2021-01-03     24\n",
      "2021-07-29     13\n",
      "2021-01-02      8\n",
      "2021-07-30      7\n",
      "2021-01-01      6\n",
      "Name: Process_Dt, Length: 211, dtype: int64\n",
      "12    1284\n",
      "23    1271\n",
      "18    1266\n",
      "19    1255\n",
      "6     1248\n",
      "8     1247\n",
      "25    1245\n",
      "24    1241\n",
      "15    1236\n",
      "20    1230\n",
      "28    1221\n",
      "31    1220\n",
      "22    1219\n",
      "7     1219\n",
      "17    1216\n",
      "30    1208\n",
      "10    1207\n",
      "13    1202\n",
      "11    1202\n",
      "3     1191\n",
      "16    1187\n",
      "21    1187\n",
      "26    1180\n",
      "14    1179\n",
      "2     1177\n",
      "29    1173\n",
      "5     1170\n",
      "9     1163\n",
      "1     1162\n",
      "27    1159\n",
      "4     1127\n",
      "Name: Days_Process_Bind, dtype: int64\n",
      "2021-06-13    250\n",
      "2021-06-20    246\n",
      "2021-07-20    243\n",
      "2021-06-11    243\n",
      "2021-06-27    242\n",
      "             ... \n",
      "2021-02-13    168\n",
      "2021-03-04    167\n",
      "2021-03-27    166\n",
      "2021-03-30    164\n",
      "2021-03-12    162\n",
      "Name: Bind_Dt, Length: 181, dtype: int64\n",
      "2021-06-13    250\n",
      "2021-06-20    246\n",
      "2021-07-20    243\n",
      "2021-06-11    243\n",
      "2021-06-27    242\n",
      "             ... \n",
      "2021-02-13    168\n",
      "2021-03-04    167\n",
      "2021-03-27    166\n",
      "2021-03-30    164\n",
      "2021-03-12    162\n",
      "Name: Eff_Dt, Length: 181, dtype: int64\n",
      "2021-12-11    250\n",
      "2021-12-18    246\n",
      "2021-12-09    243\n",
      "2022-01-17    243\n",
      "2021-12-05    242\n",
      "             ... \n",
      "2021-08-13    168\n",
      "2021-09-01    167\n",
      "2021-09-24    166\n",
      "2021-09-27    164\n",
      "2021-09-09    162\n",
      "Name: Renewal_Dt, Length: 181, dtype: int64\n",
      "CA    17246\n",
      "AZ     8663\n",
      "OR     5798\n",
      "NV     5785\n",
      "Name: State, dtype: int64\n",
      "Car      20419\n",
      "Van      10321\n",
      "Truck     6752\n",
      "Name: Policy_Type, dtype: int64\n",
      "1181.771991    15\n",
      "1095.300869    12\n",
      "1072.510031    12\n",
      "1398.621414    11\n",
      "1201.010139    11\n",
      "               ..\n",
      "1634.550738     1\n",
      "1356.161519     1\n",
      "1339.005146     1\n",
      "1134.201157     1\n",
      "1355.384815     1\n",
      "Name: NW_Premium, Length: 33205, dtype: int64\n",
      "25.000000    27009\n",
      "23.635440       15\n",
      "21.906017       12\n",
      "21.450201       12\n",
      "20.887133       11\n",
      "             ...  \n",
      "24.560027        1\n",
      "22.510562        1\n",
      "22.632203        1\n",
      "20.618797        1\n",
      "24.048133        1\n",
      "Name: Taxes, Length: 8385, dtype: int64\n",
      "1205.407430    15\n",
      "1117.206887    12\n",
      "1093.960232    12\n",
      "1065.243776    11\n",
      "1423.621414    11\n",
      "               ..\n",
      "1230.541963     1\n",
      "1332.096641     1\n",
      "1518.009177     1\n",
      "1649.099482     1\n",
      "1824.473619     1\n",
      "Name: GW_Premium, Length: 33192, dtype: int64\n",
      "1    20749\n",
      "2     8382\n",
      "9     4220\n",
      "3     4141\n",
      "Name: Num_Drivers, dtype: int64\n",
      "1    16530\n",
      "2    12583\n",
      "9     4231\n",
      "3     4148\n",
      "Name: Num_Vehicles, dtype: int64\n",
      "Male      23507\n",
      "Female    13985\n",
      "Name: Gender, dtype: int64\n",
      "21    842\n",
      "40    840\n",
      "29    820\n",
      "38    812\n",
      "25    808\n",
      "59    804\n",
      "34    803\n",
      "49    802\n",
      "18    797\n",
      "57    795\n",
      "23    795\n",
      "22    793\n",
      "39    793\n",
      "56    792\n",
      "44    792\n",
      "37    791\n",
      "31    791\n",
      "30    790\n",
      "54    790\n",
      "63    789\n",
      "42    789\n",
      "48    784\n",
      "27    783\n",
      "52    782\n",
      "47    780\n",
      "24    776\n",
      "36    775\n",
      "19    773\n",
      "62    772\n",
      "45    771\n",
      "60    770\n",
      "20    770\n",
      "51    769\n",
      "26    768\n",
      "28    768\n",
      "46    767\n",
      "33    765\n",
      "32    765\n",
      "65    761\n",
      "64    759\n",
      "61    759\n",
      "55    757\n",
      "35    757\n",
      "43    756\n",
      "53    749\n",
      "58    743\n",
      "50    743\n",
      "41    742\n",
      "Name: Age, dtype: int64\n",
      "3     842\n",
      "22    840\n",
      "11    820\n",
      "20    812\n",
      "7     808\n",
      "41    804\n",
      "16    803\n",
      "31    802\n",
      "0     797\n",
      "39    795\n",
      "5     795\n",
      "21    793\n",
      "4     793\n",
      "26    792\n",
      "38    792\n",
      "13    791\n",
      "19    791\n",
      "36    790\n",
      "12    790\n",
      "45    789\n",
      "24    789\n",
      "30    784\n",
      "9     783\n",
      "34    782\n",
      "29    780\n",
      "6     776\n",
      "18    775\n",
      "1     773\n",
      "44    772\n",
      "27    771\n",
      "2     770\n",
      "42    770\n",
      "33    769\n",
      "10    768\n",
      "8     768\n",
      "28    767\n",
      "14    765\n",
      "15    765\n",
      "47    761\n",
      "46    759\n",
      "43    759\n",
      "37    757\n",
      "17    757\n",
      "25    756\n",
      "35    749\n",
      "40    743\n",
      "32    743\n",
      "23    742\n",
      "Name: Driving_Exp, dtype: int64\n",
      "0    30787\n",
      "1     6705\n",
      "Name: Prev_Accidents, dtype: int64\n",
      "0    29530\n",
      "1     7962\n",
      "Name: Prev_Citations, dtype: int64\n",
      "3    15019\n",
      "1     9939\n",
      "2     7508\n",
      "4     5026\n",
      "Name: Marital_Status, dtype: int64\n",
      "2    13624\n",
      "1    13594\n",
      "3     6890\n",
      "4     3384\n",
      "Name: Education, dtype: int64\n",
      "4    10338\n",
      "3    10217\n",
      "6     3425\n",
      "7     3411\n",
      "5     3404\n",
      "2     3357\n",
      "1     3340\n",
      "Name: Sal_Range, dtype: int64\n",
      "1    20472\n",
      "2    10235\n",
      "3     6785\n",
      "Name: Coverage, dtype: int64\n",
      "1    16942\n",
      "3    10327\n",
      "2    10223\n",
      "Name: Veh_Usage, dtype: int64\n",
      "3    10775\n",
      "2    10725\n",
      "4     5443\n",
      "1     3488\n",
      "8     1802\n",
      "7     1778\n",
      "6     1754\n",
      "5     1727\n",
      "Name: Annual_Miles_Range, dtype: int64\n",
      "3    10808\n",
      "2    10745\n",
      "4     5318\n",
      "1     3502\n",
      "5     1819\n",
      "8     1807\n",
      "6     1762\n",
      "7     1731\n",
      "Name: Vehicle_Cost_Range, dtype: int64\n",
      "0    30787\n",
      "4     1494\n",
      "2     1419\n",
      "6     1000\n",
      "1      968\n",
      "5      916\n",
      "3      908\n",
      "Name: Claims_Amount_Range, dtype: int64\n",
      "Series([], Name: Unnamed: 28, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Display information of each column\n",
    "\n",
    "for i in AutoIns.columns:\n",
    "    print(AutoIns[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa165c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rec_ID', 'Policy_number', 'Agent_Type', 'Process_Dt',\n",
       "       'Days_Process_Bind', 'Bind_Dt', 'Eff_Dt', 'Renewal_Dt', 'State',\n",
       "       'Policy_Type', 'NW_Premium', 'Taxes', 'GW_Premium', 'Num_Drivers',\n",
       "       'Num_Vehicles', 'Gender', 'Age', 'Driving_Exp', 'Prev_Accidents',\n",
       "       'Prev_Citations', 'Marital_Status', 'Education', 'Sal_Range',\n",
       "       'Coverage', 'Veh_Usage', 'Annual_Miles_Range', 'Vehicle_Cost_Range',\n",
       "       'Claims_Amount_Range', 'Unnamed: 28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns in the dataset\n",
    "\n",
    "AutoIns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeca4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the variables which are not influencing the Target variable\n",
    "\n",
    "AutoIns = AutoIns.drop(['Rec_ID', 'Policy_number', 'Process_Dt',\n",
    "                   'Days_Process_Bind', 'Bind_Dt', 'Eff_Dt', \n",
    "                   'Renewal_Dt','NW_Premium', 'Taxes', 'Unnamed: 28'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8223a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37492 entries, 0 to 37491\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Agent_Type           37492 non-null  object \n",
      " 1   State                37492 non-null  object \n",
      " 2   Policy_Type          37492 non-null  object \n",
      " 3   GW_Premium           37492 non-null  float64\n",
      " 4   Num_Drivers          37492 non-null  int64  \n",
      " 5   Num_Vehicles         37492 non-null  int64  \n",
      " 6   Gender               37492 non-null  object \n",
      " 7   Age                  37492 non-null  int64  \n",
      " 8   Driving_Exp          37492 non-null  int64  \n",
      " 9   Prev_Accidents       37492 non-null  int64  \n",
      " 10  Prev_Citations       37492 non-null  int64  \n",
      " 11  Marital_Status       37492 non-null  int64  \n",
      " 12  Education            37492 non-null  int64  \n",
      " 13  Sal_Range            37492 non-null  int64  \n",
      " 14  Coverage             37492 non-null  int64  \n",
      " 15  Veh_Usage            37492 non-null  int64  \n",
      " 16  Annual_Miles_Range   37492 non-null  int64  \n",
      " 17  Vehicle_Cost_Range   37492 non-null  int64  \n",
      " 18  Claims_Amount_Range  37492 non-null  int64  \n",
      "dtypes: float64(1), int64(14), object(4)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset information after dropping the variables which are not influencing the target variable\n",
    "\n",
    "AutoIns.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f2d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30787\n",
       "1      968\n",
       "2     1419\n",
       "3      908\n",
       "4     1494\n",
       "5      916\n",
       "6     1000\n",
       "Name: Claims_Amount_Range, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display 'Claims_Amount_Range' column data\n",
    "\n",
    "AutoIns['Claims_Amount_Range'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06f3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Claims_Amount_Range == 0 from the dataset because they are Not claiming \n",
    "\n",
    "AutoIns_n = AutoIns[AutoIns['Claims_Amount_Range'] != 0]\n",
    "\n",
    "# copy the dataset\n",
    "\n",
    "AutoIns = AutoIns_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff1ebbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1494\n",
       "1     968\n",
       "5     916\n",
       "2    1419\n",
       "6    1000\n",
       "3     908\n",
       "Name: Claims_Amount_Range, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display 'Claims_Amount_Range' column data\n",
    "\n",
    "AutoIns['Claims_Amount_Range'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c93de97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CA    3019\n",
       "NV    1055\n",
       "OR    1013\n",
       "AZ    1618\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display 'State' column data\n",
    "\n",
    "AutoIns['State'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da4f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Independent variable 'State' and convert into numeric form\n",
    "\n",
    "AutoIns['State'] = AutoIns['State'].str.replace('OR', '1')\n",
    "AutoIns['State'] = AutoIns['State'].str.replace('NV', '2')\n",
    "AutoIns['State'] = AutoIns['State'].str.replace('AZ', '3')\n",
    "AutoIns['State'] = AutoIns['State'].str.replace('CA', '4')\n",
    "AutoIns['State'] = AutoIns['State'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f68185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3019\n",
       "1    1013\n",
       "2    1055\n",
       "3    1618\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display 'State' column data after replacement\n",
    "\n",
    "AutoIns['State'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6fa5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranges and new column as 'Age_Range' from 'Age'\n",
    "\n",
    "AutoIns['Age_Range'] = pd.cut(AutoIns['Age'], [0, 20, 25, 35, 45, 55, 65, 75], \n",
    "                       labels=['<=20', '>20 & <=25', '>25 & <=35 ', '>35 & <=45', '>45 & <=55', '>55 & <=65','>65'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb612f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'Age_Range' into numeric form\n",
    "\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>65','1')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>55 & <=65','2')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>45 & <=55','3')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>35 & <=45','4')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>25 & <=35','5')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('>20 & <=25','6')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].str.replace('<=20','7')\n",
    "AutoIns['Age_Range']=AutoIns['Age_Range'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ec4fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranges and new column as 'Driving_Exp_Range' from 'Driving_Exp'\n",
    "\n",
    "AutoIns['Driving_Exp_Range'] = pd.cut(AutoIns['Driving_Exp'], [-1, 0, 3, 7, 11, 16, 21, 80],\n",
    "                                labels=['0', '<=3', '>3 & <=7', '>7 & <=11', '>11 & <=16', '>16 & <=21', '>21'])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c918fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'Driving_Exp_Range' into numeric form\n",
    "\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('<=3','1')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('>3 & <=7','2')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('>7 & <=11','3')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('>11 & <=16','4')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('>16 & <=21','5')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].str.replace('>21','6')\n",
    "AutoIns['Driving_Exp_Range']=AutoIns['Driving_Exp_Range'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c9d596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6705 entries, 20 to 37490\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Agent_Type           6705 non-null   object \n",
      " 1   State                6705 non-null   int32  \n",
      " 2   Policy_Type          6705 non-null   object \n",
      " 3   GW_Premium           6705 non-null   float64\n",
      " 4   Num_Drivers          6705 non-null   int64  \n",
      " 5   Num_Vehicles         6705 non-null   int64  \n",
      " 6   Gender               6705 non-null   object \n",
      " 7   Age                  6705 non-null   int64  \n",
      " 8   Driving_Exp          6705 non-null   int64  \n",
      " 9   Prev_Accidents       6705 non-null   int64  \n",
      " 10  Prev_Citations       6705 non-null   int64  \n",
      " 11  Marital_Status       6705 non-null   int64  \n",
      " 12  Education            6705 non-null   int64  \n",
      " 13  Sal_Range            6705 non-null   int64  \n",
      " 14  Coverage             6705 non-null   int64  \n",
      " 15  Veh_Usage            6705 non-null   int64  \n",
      " 16  Annual_Miles_Range   6705 non-null   int64  \n",
      " 17  Vehicle_Cost_Range   6705 non-null   int64  \n",
      " 18  Claims_Amount_Range  6705 non-null   int64  \n",
      " 19  Age_Range            6705 non-null   int32  \n",
      " 20  Driving_Exp_Range    6705 non-null   int32  \n",
      "dtypes: float64(1), int32(3), int64(14), object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information after creating new columns\n",
    "\n",
    "AutoIns.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34e1091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the variables which are not influencing the Target variable\n",
    "\n",
    "AutoIns = AutoIns.drop(['Age', 'Driving_Exp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e421d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cols1 for creating dummy variables\n",
    "\n",
    "cols1 = ['Agent_Type','Gender','Policy_Type']\n",
    "\n",
    "# Create cols2 for scaling the data - normalization\n",
    "\n",
    "cols2 = ['Num_Drivers','Num_Vehicles','State','Age_Range', 'Driving_Exp_Range', 'Prev_Accidents',\n",
    "       'Prev_Citations', 'Marital_Status', 'Education', 'Sal_Range',\n",
    "       'Coverage', 'Veh_Usage', 'Annual_Miles_Range', 'Vehicle_Cost_Range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8b43ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>22</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GW_Premium</th>\n",
       "      <td>2189.291011</td>\n",
       "      <td>1167.063994</td>\n",
       "      <td>1762.78488</td>\n",
       "      <td>2298.439907</td>\n",
       "      <td>1758.541058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Drivers</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Vehicles</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prev_Accidents</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prev_Citations</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marital_Status</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sal_Range</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coverage</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veh_Usage</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual_Miles_Range</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vehicle_Cost_Range</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claims_Amount_Range</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_Range</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Driving_Exp_Range</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agent_Type_EA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agent_Type_IA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_Female</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_Male</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy_Type_Car</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy_Type_Truck</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy_Type_Van</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              20           22          30           31  \\\n",
       "State                   4.000000     1.000000     4.00000     2.000000   \n",
       "GW_Premium           2189.291011  1167.063994  1762.78488  2298.439907   \n",
       "Num_Drivers             9.000000     1.000000     3.00000     3.000000   \n",
       "Num_Vehicles            1.000000     1.000000     3.00000     2.000000   \n",
       "Prev_Accidents          1.000000     1.000000     1.00000     1.000000   \n",
       "Prev_Citations          0.000000     0.000000     0.00000     0.000000   \n",
       "Marital_Status          3.000000     3.000000     3.00000     3.000000   \n",
       "Education               1.000000     4.000000     2.00000     1.000000   \n",
       "Sal_Range               4.000000     6.000000     7.00000     3.000000   \n",
       "Coverage                3.000000     1.000000     1.00000     1.000000   \n",
       "Veh_Usage               3.000000     1.000000     1.00000     3.000000   \n",
       "Annual_Miles_Range      1.000000     2.000000     8.00000     3.000000   \n",
       "Vehicle_Cost_Range      5.000000     2.000000     1.00000     7.000000   \n",
       "Claims_Amount_Range     2.000000     2.000000     4.00000     5.000000   \n",
       "Age_Range               2.000000     2.000000     6.00000     6.000000   \n",
       "Driving_Exp_Range       6.000000     6.000000     1.00000     1.000000   \n",
       "Agent_Type_EA           1.000000     1.000000     1.00000     1.000000   \n",
       "Agent_Type_IA           0.000000     0.000000     0.00000     0.000000   \n",
       "Gender_Female           0.000000     1.000000     1.00000     1.000000   \n",
       "Gender_Male             1.000000     0.000000     0.00000     0.000000   \n",
       "Policy_Type_Car         1.000000     1.000000     0.00000     0.000000   \n",
       "Policy_Type_Truck       0.000000     0.000000     1.00000     1.000000   \n",
       "Policy_Type_Van         0.000000     0.000000     0.00000     0.000000   \n",
       "\n",
       "                              32  \n",
       "State                   2.000000  \n",
       "GW_Premium           1758.541058  \n",
       "Num_Drivers             9.000000  \n",
       "Num_Vehicles            1.000000  \n",
       "Prev_Accidents          1.000000  \n",
       "Prev_Citations          0.000000  \n",
       "Marital_Status          3.000000  \n",
       "Education               2.000000  \n",
       "Sal_Range               4.000000  \n",
       "Coverage                2.000000  \n",
       "Veh_Usage               1.000000  \n",
       "Annual_Miles_Range      2.000000  \n",
       "Vehicle_Cost_Range      2.000000  \n",
       "Claims_Amount_Range     4.000000  \n",
       "Age_Range               7.000000  \n",
       "Driving_Exp_Range       1.000000  \n",
       "Agent_Type_EA           1.000000  \n",
       "Agent_Type_IA           0.000000  \n",
       "Gender_Female           0.000000  \n",
       "Gender_Male             1.000000  \n",
       "Policy_Type_Car         1.000000  \n",
       "Policy_Type_Truck       0.000000  \n",
       "Policy_Type_Van         0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variable for all col1 columns\n",
    "\n",
    "AutoIns = pd.get_dummies(AutoIns, columns=cols1)\n",
    "AutoIns.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2295ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent and Target variables\n",
    "\n",
    "IndepVar=[]\n",
    "for col in AutoIns.columns:\n",
    "    if col != 'Claims_Amount_Range':\n",
    "        IndepVar.append(col)\n",
    "        \n",
    "TargetVar = 'Claims_Amount_Range'\n",
    "\n",
    "x = AutoIns[IndepVar]\n",
    "y = AutoIns[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a61c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=6)\n",
    "\n",
    "# Copy the test data to back-up file\n",
    "\n",
    "x_test_F1 = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6226bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cols2 columns by using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train[cols2] = mmscaler.fit_transform(x_train[cols2])\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test[cols2] = mmscaler.fit_transform(x_test[cols2])\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ecc46",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "614eb2a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [276 421 267 480 294 274]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=0, FP=421, TN=1591, FN=0\n",
      "Accuracy: 0.791\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.791\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=480, FP=0, TN=0, FN=1532\n",
      "Accuracy: 0.239\n",
      "Precision: 1.0\n",
      "Sensitivity: 0.239\n",
      "F1-Score: 0.385\n",
      "Specificity: nan\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1738, FN=0\n",
      "Accuracy: 0.864\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.6333%\n",
      "Precision: 16.6667%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.0642\n",
      "Specificity or True Nagative Rate: nan%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "AutoIns = AutoInsLR.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y1_pred = AutoInsLR.predict(x_test)\n",
    "y1_pred_proba = AutoInsLR.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y1_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y1_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64cf1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14922453 0.18772762 0.14397197 0.21738681 0.12984043 0.17184864]\n",
      " [0.14866887 0.15199738 0.15366996 0.19932285 0.13300125 0.21333969]\n",
      " [0.14823143 0.26701408 0.118935   0.24815629 0.11976764 0.09789556]\n",
      " ...\n",
      " [0.12086537 0.22090375 0.11524834 0.2559448  0.11758911 0.16944864]\n",
      " [0.1549004  0.22774138 0.15102679 0.21137966 0.12373684 0.13121494]\n",
      " [0.20201401 0.19285015 0.162359   0.19745207 0.11616118 0.12916359]]\n"
     ]
    }
   ],
   "source": [
    "# Fit OneVsRestClassifier model to calculate the ROC SUC and plot ROC curve\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "modelORC = OneVsRestClassifier(LogisticRegression())\n",
    "modelORC.fit(x_train, y_train)\n",
    "y1_pred = modelORC.predict(x_test)\n",
    "y1_pred_proba = modelORC.predict_proba(x_test)\n",
    "print(y1_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72595c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score:  0.536\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y1_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844a44d",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cc0bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 44  61  36  63  42  42]\n",
      " [ 56  88  53  90  52  52]\n",
      " [ 45  55  44  72  40  36]\n",
      " [ 51  93  53 108  76  67]\n",
      " [ 43  66  40  67  38  39]\n",
      " [ 37  58  41  80  46  38]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=44, FP=232, TN=1492, FN=244\n",
      "Accuracy: 0.763\n",
      "Precision: 0.159\n",
      "Sensitivity: 0.153\n",
      "F1-Score: 0.156\n",
      "Specificity: 0.865\n",
      "Balanced Accuracy: 0.509\n",
      "\n",
      "MCC: 0.019\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=88, FP=333, TN=1288, FN=303\n",
      "Accuracy: 0.684\n",
      "Precision: 0.209\n",
      "Sensitivity: 0.225\n",
      "F1-Score: 0.217\n",
      "Specificity: 0.795\n",
      "Balanced Accuracy: 0.51\n",
      "\n",
      "MCC: 0.019\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=44, FP=223, TN=1497, FN=248\n",
      "Accuracy: 0.766\n",
      "Precision: 0.165\n",
      "Sensitivity: 0.151\n",
      "F1-Score: 0.157\n",
      "Specificity: 0.87\n",
      "Balanced Accuracy: 0.51\n",
      "\n",
      "MCC: 0.022\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=108, FP=372, TN=1192, FN=340\n",
      "Accuracy: 0.646\n",
      "Precision: 0.225\n",
      "Sensitivity: 0.241\n",
      "F1-Score: 0.233\n",
      "Specificity: 0.762\n",
      "Balanced Accuracy: 0.502\n",
      "\n",
      "MCC: 0.003\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=38, FP=256, TN=1463, FN=255\n",
      "Accuracy: 0.746\n",
      "Precision: 0.129\n",
      "Sensitivity: 0.13\n",
      "F1-Score: 0.129\n",
      "Specificity: 0.851\n",
      "Balanced Accuracy: 0.49\n",
      "\n",
      "MCC: -0.019\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=38, FP=236, TN=1476, FN=262\n",
      "Accuracy: 0.752\n",
      "Precision: 0.139\n",
      "Sensitivity: 0.127\n",
      "F1-Score: 0.132\n",
      "Specificity: 0.862\n",
      "Balanced Accuracy: 0.494\n",
      "\n",
      "MCC: -0.012\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.6167%\n",
      "Precision: 17.1%\n",
      "Recall or Sensitivity: 17.1167%\n",
      "F1-Score: 0.1707\n",
      "Specificity or True Nagative Rate: 83.4167%\n",
      "Balanced Accuracy: 50.25%\n",
      "\n",
      "MCC: 0.0053\n",
      "\n",
      "ROC Score:  0.503\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Decision Tree Classification' model with random sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsDR = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "AutoInsDR = AutoInsDR.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y2_pred = AutoInsDR.predict(x_test)\n",
    "y2_pred_proba = AutoInsDR.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y2_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y2_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y2_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319e47d",
   "metadata": {},
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ed9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19  34  33  36  25  16]\n",
      " [116 134  84 151  99  93]\n",
      " [ 17  28  25  40  23  19]\n",
      " [ 86 152  88 162  99 102]\n",
      " [ 17  26  14  31  15  15]\n",
      " [ 21  47  23  60  33  29]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=19, FP=257, TN=1592, FN=144\n",
      "Accuracy: 0.801\n",
      "Precision: 0.069\n",
      "Sensitivity: 0.117\n",
      "F1-Score: 0.087\n",
      "Specificity: 0.861\n",
      "Balanced Accuracy: 0.489\n",
      "\n",
      "MCC: -0.018\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=134, FP=287, TN=1048, FN=543\n",
      "Accuracy: 0.587\n",
      "Precision: 0.318\n",
      "Sensitivity: 0.198\n",
      "F1-Score: 0.244\n",
      "Specificity: 0.785\n",
      "Balanced Accuracy: 0.492\n",
      "\n",
      "MCC: -0.02\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=25, FP=242, TN=1618, FN=127\n",
      "Accuracy: 0.817\n",
      "Precision: 0.094\n",
      "Sensitivity: 0.164\n",
      "F1-Score: 0.119\n",
      "Specificity: 0.87\n",
      "Balanced Accuracy: 0.517\n",
      "\n",
      "MCC: 0.027\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=162, FP=318, TN=1005, FN=527\n",
      "Accuracy: 0.58\n",
      "Precision: 0.338\n",
      "Sensitivity: 0.235\n",
      "F1-Score: 0.277\n",
      "Specificity: 0.76\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.006\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=15, FP=279, TN=1615, FN=103\n",
      "Accuracy: 0.81\n",
      "Precision: 0.051\n",
      "Sensitivity: 0.127\n",
      "F1-Score: 0.073\n",
      "Specificity: 0.853\n",
      "Balanced Accuracy: 0.49\n",
      "\n",
      "MCC: -0.013\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=29, FP=245, TN=1554, FN=184\n",
      "Accuracy: 0.787\n",
      "Precision: 0.106\n",
      "Sensitivity: 0.136\n",
      "F1-Score: 0.119\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.5\n",
      "\n",
      "MCC: -0.0\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.0333%\n",
      "Precision: 16.2667%\n",
      "Recall or Sensitivity: 16.2833%\n",
      "F1-Score: 0.1532\n",
      "Specificity or True Nagative Rate: 83.2167%\n",
      "Balanced Accuracy: 49.7667%\n",
      "\n",
      "MCC: -0.005\n",
      "\n",
      "ROC Score:  0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Random Forest Classification' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsRF = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                   n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                   ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "AutoInsRF = AutoInsRF.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y3_pred = AutoInsRF.predict(x_test)\n",
    "y3_pred_proba = AutoInsRF.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y3_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y3_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y3_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288f518",
   "metadata": {},
   "source": [
    "# Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "651c9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [ 95 151  84 156  78  76]\n",
      " [  0   0   0   0   0   0]\n",
      " [181 270 183 324 216 198]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=151, FP=270, TN=1102, FN=489\n",
      "Accuracy: 0.623\n",
      "Precision: 0.359\n",
      "Sensitivity: 0.236\n",
      "F1-Score: 0.285\n",
      "Specificity: 0.803\n",
      "Balanced Accuracy: 0.52\n",
      "\n",
      "MCC: 0.045\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=324, FP=156, TN=484, FN=1048\n",
      "Accuracy: 0.402\n",
      "Precision: 0.675\n",
      "Sensitivity: 0.236\n",
      "F1-Score: 0.35\n",
      "Specificity: 0.756\n",
      "Balanced Accuracy: 0.496\n",
      "\n",
      "MCC: -0.008\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1738, FN=0\n",
      "Accuracy: 0.864\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.55%\n",
      "Precision: 17.2333%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.1058\n",
      "Specificity or True Nagative Rate: 83.45%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Support Vector Classification' model with random sampling\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsSVC = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, \n",
    "                 tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', \n",
    "                 break_ties=False, random_state=None)\n",
    "\n",
    "AutoInsSVC = AutoInsSVC.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y4_pred = AutoInsSVC.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y4_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y4_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y3_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007600d",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c12af759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 69  83  62  94  59  63]\n",
      " [ 74 109  73 137  85  74]\n",
      " [ 27  50  28  55  26  32]\n",
      " [ 50  86  52 103  64  59]\n",
      " [ 30  55  24  51  35  21]\n",
      " [ 26  38  28  40  25  25]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=69, FP=207, TN=1375, FN=361\n",
      "Accuracy: 0.718\n",
      "Precision: 0.25\n",
      "Sensitivity: 0.16\n",
      "F1-Score: 0.195\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.514\n",
      "\n",
      "MCC: 0.035\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=109, FP=312, TN=1148, FN=443\n",
      "Accuracy: 0.625\n",
      "Precision: 0.259\n",
      "Sensitivity: 0.197\n",
      "F1-Score: 0.224\n",
      "Specificity: 0.786\n",
      "Balanced Accuracy: 0.492\n",
      "\n",
      "MCC: -0.018\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=28, FP=239, TN=1555, FN=190\n",
      "Accuracy: 0.787\n",
      "Precision: 0.105\n",
      "Sensitivity: 0.128\n",
      "F1-Score: 0.115\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.004\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=103, FP=377, TN=1221, FN=311\n",
      "Accuracy: 0.658\n",
      "Precision: 0.215\n",
      "Sensitivity: 0.249\n",
      "F1-Score: 0.23\n",
      "Specificity: 0.764\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.012\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=35, FP=259, TN=1537, FN=181\n",
      "Accuracy: 0.781\n",
      "Precision: 0.119\n",
      "Sensitivity: 0.162\n",
      "F1-Score: 0.137\n",
      "Specificity: 0.856\n",
      "Balanced Accuracy: 0.509\n",
      "\n",
      "MCC: 0.016\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=25, FP=249, TN=1581, FN=157\n",
      "Accuracy: 0.798\n",
      "Precision: 0.091\n",
      "Sensitivity: 0.137\n",
      "F1-Score: 0.11\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.5\n",
      "\n",
      "MCC: 0.001\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.7833%\n",
      "Precision: 17.3167%\n",
      "Recall or Sensitivity: 17.2167%\n",
      "F1-Score: 0.1685\n",
      "Specificity or True Nagative Rate: 83.4333%\n",
      "Balanced Accuracy: 50.3167%\n",
      "\n",
      "MCC: 0.007\n",
      "\n",
      "ROC Score:  0.507\n"
     ]
    }
   ],
   "source": [
    "# To build the 'K-Nearest Neighbors' model with random sampling\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsKNN = KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', \n",
    "                                  metric_params=None, n_jobs=None)\n",
    "\n",
    "AutoInsKNN = AutoInsKNN.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y5_pred = AutoInsKNN.predict(x_test)\n",
    "y5_pred_proba = AutoInsKNN.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y5_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y5_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y5_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a8a7",
   "metadata": {},
   "source": [
    "# Navie Bayers Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ce42022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [ 99 145  95 159 120 110]\n",
      " [  0   0   0   0   0   0]\n",
      " [177 275 172 321 174 164]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=145, FP=276, TN=1008, FN=583\n",
      "Accuracy: 0.573\n",
      "Precision: 0.344\n",
      "Sensitivity: 0.199\n",
      "F1-Score: 0.252\n",
      "Specificity: 0.785\n",
      "Balanced Accuracy: 0.492\n",
      "\n",
      "MCC: -0.019\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=321, FP=159, TN=570, FN=962\n",
      "Accuracy: 0.443\n",
      "Precision: 0.669\n",
      "Sensitivity: 0.25\n",
      "F1-Score: 0.364\n",
      "Specificity: 0.782\n",
      "Balanced Accuracy: 0.516\n",
      "\n",
      "MCC: 0.036\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1737, FN=1\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: 0.0\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.432\n",
      "\n",
      "MCC: -0.009\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.3833%\n",
      "Precision: 16.8833%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.1027\n",
      "Specificity or True Nagative Rate: 83.5833%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Navie Bayers Classifier' model with random sampling\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsNB = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "AutoInsNB = AutoInsNB.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y6_pred = AutoInsNB.predict(x_test)\n",
    "y6_pred_proba = AutoInsNB.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y6_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y6_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y6_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c822ff9",
   "metadata": {},
   "source": [
    "# Cat boost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a557157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.268384\n",
      "0:\tlearn: 1.7743777\ttest: 1.7806246\tbest: 1.7806246 (0)\ttotal: 146ms\tremaining: 14.4s\n",
      "10:\tlearn: 1.6743214\ttest: 1.7734822\tbest: 1.7692695 (4)\ttotal: 230ms\tremaining: 1.86s\n",
      "20:\tlearn: 1.6097222\ttest: 1.7768384\tbest: 1.7692695 (4)\ttotal: 306ms\tremaining: 1.15s\n",
      "30:\tlearn: 1.5432308\ttest: 1.7834916\tbest: 1.7692695 (4)\ttotal: 390ms\tremaining: 868ms\n",
      "40:\tlearn: 1.4822039\ttest: 1.7919097\tbest: 1.7692695 (4)\ttotal: 476ms\tremaining: 684ms\n",
      "50:\tlearn: 1.4206817\ttest: 1.7994513\tbest: 1.7692695 (4)\ttotal: 551ms\tremaining: 529ms\n",
      "60:\tlearn: 1.3737583\ttest: 1.8059900\tbest: 1.7692695 (4)\ttotal: 625ms\tremaining: 400ms\n",
      "70:\tlearn: 1.3196923\ttest: 1.8099066\tbest: 1.7692695 (4)\ttotal: 734ms\tremaining: 300ms\n",
      "80:\tlearn: 1.2669932\ttest: 1.8176440\tbest: 1.7692695 (4)\ttotal: 892ms\tremaining: 209ms\n",
      "90:\tlearn: 1.2197397\ttest: 1.8224272\tbest: 1.7692695 (4)\ttotal: 1.01s\tremaining: 100ms\n",
      "99:\tlearn: 1.1798066\ttest: 1.8272441\tbest: 1.7692695 (4)\ttotal: 1.08s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.769269518\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "[[  4   2   1   3   1   3]\n",
      " [128 195 134 207 121 128]\n",
      " [  0   2   1   3   5   1]\n",
      " [135 208 119 254 157 134]\n",
      " [  1   1   0   4   1   2]\n",
      " [  8  13  12   9   9   6]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=4, FP=272, TN=1726, FN=10\n",
      "Accuracy: 0.86\n",
      "Precision: 0.014\n",
      "Sensitivity: 0.286\n",
      "F1-Score: 0.028\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.575\n",
      "\n",
      "MCC: 0.036\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=195, FP=226, TN=873, FN=718\n",
      "Accuracy: 0.531\n",
      "Precision: 0.463\n",
      "Sensitivity: 0.214\n",
      "F1-Score: 0.292\n",
      "Specificity: 0.794\n",
      "Balanced Accuracy: 0.504\n",
      "\n",
      "MCC: 0.01\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=1, FP=266, TN=1734, FN=11\n",
      "Accuracy: 0.862\n",
      "Precision: 0.004\n",
      "Sensitivity: 0.083\n",
      "F1-Score: 0.007\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.475\n",
      "\n",
      "MCC: -0.011\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=254, FP=226, TN=779, FN=753\n",
      "Accuracy: 0.513\n",
      "Precision: 0.529\n",
      "Sensitivity: 0.252\n",
      "F1-Score: 0.342\n",
      "Specificity: 0.775\n",
      "Balanced Accuracy: 0.514\n",
      "\n",
      "MCC: 0.032\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=1, FP=293, TN=1710, FN=8\n",
      "Accuracy: 0.85\n",
      "Precision: 0.003\n",
      "Sensitivity: 0.111\n",
      "F1-Score: 0.007\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: 0.482\n",
      "\n",
      "MCC: -0.007\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=6, FP=268, TN=1687, FN=51\n",
      "Accuracy: 0.841\n",
      "Precision: 0.022\n",
      "Sensitivity: 0.105\n",
      "F1-Score: 0.036\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: 0.484\n",
      "\n",
      "MCC: -0.015\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.2833%\n",
      "Precision: 17.25%\n",
      "Recall or Sensitivity: 17.5167%\n",
      "F1-Score: 0.1187\n",
      "Specificity or True Nagative Rate: 83.6167%\n",
      "Balanced Accuracy: 50.5667%\n",
      "\n",
      "MCC: 0.0075\n",
      "\n",
      "ROC Score:  0.519\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Cat boost Classification' model with random sampling\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost.utils import eval_metric\n",
    "\n",
    "AutoInsCBC = CatBoostClassifier(iterations=100, verbose=10)\n",
    "AutoInsCBC.fit(x_train, y_train, eval_set=(x_test, y_test))\n",
    "AutoInsCBC.set_feature_names(x_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y7_pred = AutoInsCBC.predict(x_test)\n",
    "y7_pred_proba = AutoInsCBC.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y7_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y7_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y7_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab3d96",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c82da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  27  15  23  11  14]\n",
      " [112 171  97 179  87  82]\n",
      " [ 16  20  13  26  16   8]\n",
      " [ 97 153 109 183 132 123]\n",
      " [  9  16   4  18  14  12]\n",
      " [ 20  34  29  51  34  35]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=22, FP=254, TN=1646, FN=90\n",
      "Accuracy: 0.829\n",
      "Precision: 0.08\n",
      "Sensitivity: 0.196\n",
      "F1-Score: 0.113\n",
      "Specificity: 0.866\n",
      "Balanced Accuracy: 0.531\n",
      "\n",
      "MCC: 0.042\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=171, FP=250, TN=1034, FN=557\n",
      "Accuracy: 0.599\n",
      "Precision: 0.406\n",
      "Sensitivity: 0.235\n",
      "F1-Score: 0.298\n",
      "Specificity: 0.805\n",
      "Balanced Accuracy: 0.52\n",
      "\n",
      "MCC: 0.047\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=13, FP=254, TN=1659, FN=86\n",
      "Accuracy: 0.831\n",
      "Precision: 0.049\n",
      "Sensitivity: 0.131\n",
      "F1-Score: 0.071\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.499\n",
      "\n",
      "MCC: -0.001\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=183, FP=297, TN=918, FN=614\n",
      "Accuracy: 0.547\n",
      "Precision: 0.381\n",
      "Sensitivity: 0.23\n",
      "F1-Score: 0.287\n",
      "Specificity: 0.756\n",
      "Balanced Accuracy: 0.493\n",
      "\n",
      "MCC: -0.017\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=14, FP=280, TN=1659, FN=59\n",
      "Accuracy: 0.832\n",
      "Precision: 0.048\n",
      "Sensitivity: 0.192\n",
      "F1-Score: 0.076\n",
      "Specificity: 0.856\n",
      "Balanced Accuracy: 0.524\n",
      "\n",
      "MCC: 0.025\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=35, FP=239, TN=1570, FN=168\n",
      "Accuracy: 0.798\n",
      "Precision: 0.128\n",
      "Sensitivity: 0.172\n",
      "F1-Score: 0.147\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.52\n",
      "\n",
      "MCC: 0.035\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.9333%\n",
      "Precision: 18.2%\n",
      "Recall or Sensitivity: 19.2667%\n",
      "F1-Score: 0.1653\n",
      "Specificity or True Nagative Rate: 83.6333%\n",
      "Balanced Accuracy: 51.45%\n",
      "\n",
      "MCC: 0.0218\n",
      "\n",
      "ROC Score:  0.528\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsGBC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, \n",
    "                                        criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "                                        min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
    "                                        min_impurity_split=None, init=None, random_state=None, max_features=None, \n",
    "                                        verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                        n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "AutoInsGBC = AutoInsGBC.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y8_pred = AutoInsGBC.predict(x_test)\n",
    "y8_pred_proba = AutoInsGBC.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y8_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y8_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y8_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe140a94",
   "metadata": {},
   "source": [
    "# Bagging Classifier for All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b331534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  None\n",
      "[[ 53  67  50  85  58  37]\n",
      " [ 83 124  80 142  77  71]\n",
      " [ 37  49  33  53  36  28]\n",
      " [ 60 110  59 116  70  77]\n",
      " [ 20  30  21  36  33  25]\n",
      " [ 23  41  24  48  20  36]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=53, FP=223, TN=1439, FN=297\n",
      "Accuracy: 0.742\n",
      "Precision: 0.192\n",
      "Sensitivity: 0.151\n",
      "F1-Score: 0.169\n",
      "Specificity: 0.866\n",
      "Balanced Accuracy: 0.508\n",
      "\n",
      "MCC: 0.019\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=124, FP=297, TN=1138, FN=453\n",
      "Accuracy: 0.627\n",
      "Precision: 0.295\n",
      "Sensitivity: 0.215\n",
      "F1-Score: 0.248\n",
      "Specificity: 0.793\n",
      "Balanced Accuracy: 0.504\n",
      "\n",
      "MCC: 0.009\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=33, FP=234, TN=1542, FN=203\n",
      "Accuracy: 0.783\n",
      "Precision: 0.124\n",
      "Sensitivity: 0.14\n",
      "F1-Score: 0.131\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.504\n",
      "\n",
      "MCC: 0.008\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=116, FP=364, TN=1156, FN=376\n",
      "Accuracy: 0.632\n",
      "Precision: 0.242\n",
      "Sensitivity: 0.236\n",
      "F1-Score: 0.239\n",
      "Specificity: 0.761\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.004\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=33, FP=261, TN=1586, FN=132\n",
      "Accuracy: 0.805\n",
      "Precision: 0.112\n",
      "Sensitivity: 0.2\n",
      "F1-Score: 0.144\n",
      "Specificity: 0.859\n",
      "Balanced Accuracy: 0.53\n",
      "\n",
      "MCC: 0.046\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=36, FP=238, TN=1582, FN=156\n",
      "Accuracy: 0.804\n",
      "Precision: 0.131\n",
      "Sensitivity: 0.188\n",
      "F1-Score: 0.155\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.528\n",
      "\n",
      "MCC: 0.049\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.2167%\n",
      "Precision: 18.2667%\n",
      "Recall or Sensitivity: 18.8333%\n",
      "F1-Score: 0.181\n",
      "Specificity or True Nagative Rate: 83.6%\n",
      "Balanced Accuracy: 51.2%\n",
      "\n",
      "MCC: 0.0212\n",
      "\n",
      "ROC Score:  0.51\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  LogisticRegression()\n",
      "[[  0   0   0   0   0   0]\n",
      " [ 96 149 100 153 114 100]\n",
      " [  0   1   0   0   0   0]\n",
      " [175 259 152 320 170 171]\n",
      " [  0   0   0   0   0   0]\n",
      " [  5  12  15   7  10   3]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=149, FP=272, TN=1028, FN=563\n",
      "Accuracy: 0.585\n",
      "Precision: 0.354\n",
      "Sensitivity: 0.209\n",
      "F1-Score: 0.263\n",
      "Specificity: 0.791\n",
      "Balanced Accuracy: 0.5\n",
      "\n",
      "MCC: 0.0\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1744, FN=1\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: 0.0\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.434\n",
      "\n",
      "MCC: -0.009\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=320, FP=160, TN=605, FN=927\n",
      "Accuracy: 0.46\n",
      "Precision: 0.667\n",
      "Sensitivity: 0.257\n",
      "F1-Score: 0.371\n",
      "Specificity: 0.791\n",
      "Balanced Accuracy: 0.524\n",
      "\n",
      "MCC: 0.054\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=3, FP=271, TN=1689, FN=49\n",
      "Accuracy: 0.841\n",
      "Precision: 0.011\n",
      "Sensitivity: 0.058\n",
      "F1-Score: 0.018\n",
      "Specificity: 0.862\n",
      "Balanced Accuracy: 0.46\n",
      "\n",
      "MCC: -0.037\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.5%\n",
      "Precision: 17.2%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.1087\n",
      "Specificity or True Nagative Rate: 83.8%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.518\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  DecisionTreeClassifier()\n",
      "[[ 55  80  40  99  52  53]\n",
      " [ 81 148  86 146  75  67]\n",
      " [ 36  42  31  49  30  40]\n",
      " [ 59  95  61 104  83  60]\n",
      " [ 24  28  20  32  24  26]\n",
      " [ 21  28  29  50  30  28]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=55, FP=221, TN=1412, FN=324\n",
      "Accuracy: 0.729\n",
      "Precision: 0.199\n",
      "Sensitivity: 0.145\n",
      "F1-Score: 0.168\n",
      "Specificity: 0.865\n",
      "Balanced Accuracy: 0.505\n",
      "\n",
      "MCC: 0.011\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=148, FP=273, TN=1136, FN=455\n",
      "Accuracy: 0.638\n",
      "Precision: 0.352\n",
      "Sensitivity: 0.245\n",
      "F1-Score: 0.289\n",
      "Specificity: 0.806\n",
      "Balanced Accuracy: 0.526\n",
      "\n",
      "MCC: 0.058\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=31, FP=236, TN=1548, FN=197\n",
      "Accuracy: 0.785\n",
      "Precision: 0.116\n",
      "Sensitivity: 0.136\n",
      "F1-Score: 0.125\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.502\n",
      "\n",
      "MCC: 0.003\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=104, FP=376, TN=1174, FN=358\n",
      "Accuracy: 0.635\n",
      "Precision: 0.217\n",
      "Sensitivity: 0.225\n",
      "F1-Score: 0.221\n",
      "Specificity: 0.757\n",
      "Balanced Accuracy: 0.491\n",
      "\n",
      "MCC: -0.017\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=24, FP=270, TN=1588, FN=130\n",
      "Accuracy: 0.801\n",
      "Precision: 0.082\n",
      "Sensitivity: 0.156\n",
      "F1-Score: 0.107\n",
      "Specificity: 0.855\n",
      "Balanced Accuracy: 0.505\n",
      "\n",
      "MCC: 0.008\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=28, FP=246, TN=1580, FN=158\n",
      "Accuracy: 0.799\n",
      "Precision: 0.102\n",
      "Sensitivity: 0.151\n",
      "F1-Score: 0.122\n",
      "Specificity: 0.865\n",
      "Balanced Accuracy: 0.508\n",
      "\n",
      "MCC: 0.013\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.1167%\n",
      "Precision: 17.8%\n",
      "Recall or Sensitivity: 17.6333%\n",
      "F1-Score: 0.172\n",
      "Specificity or True Nagative Rate: 83.6%\n",
      "Balanced Accuracy: 50.6167%\n",
      "\n",
      "MCC: 0.0127\n",
      "\n",
      "ROC Score:  0.507\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  RandomForestClassifier()\n",
      "[[ 13  16  19  28  15  12]\n",
      " [115 160  91 177 102  92]\n",
      " [  9  18  10  19  17   9]\n",
      " [107 177 121 192 124 118]\n",
      " [ 11  14   8  17  10  11]\n",
      " [ 21  36  18  47  26  32]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=13, FP=263, TN=1646, FN=90\n",
      "Accuracy: 0.825\n",
      "Precision: 0.047\n",
      "Sensitivity: 0.126\n",
      "F1-Score: 0.069\n",
      "Specificity: 0.862\n",
      "Balanced Accuracy: 0.494\n",
      "\n",
      "MCC: -0.007\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=160, FP=261, TN=1014, FN=577\n",
      "Accuracy: 0.583\n",
      "Precision: 0.38\n",
      "Sensitivity: 0.217\n",
      "F1-Score: 0.276\n",
      "Specificity: 0.795\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.015\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=10, FP=257, TN=1673, FN=72\n",
      "Accuracy: 0.836\n",
      "Precision: 0.037\n",
      "Sensitivity: 0.122\n",
      "F1-Score: 0.057\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.494\n",
      "\n",
      "MCC: -0.007\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=192, FP=288, TN=885, FN=647\n",
      "Accuracy: 0.535\n",
      "Precision: 0.4\n",
      "Sensitivity: 0.229\n",
      "F1-Score: 0.291\n",
      "Specificity: 0.754\n",
      "Balanced Accuracy: 0.492\n",
      "\n",
      "MCC: -0.019\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=10, FP=284, TN=1657, FN=61\n",
      "Accuracy: 0.829\n",
      "Precision: 0.034\n",
      "Sensitivity: 0.141\n",
      "F1-Score: 0.055\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.003\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=32, FP=242, TN=1590, FN=148\n",
      "Accuracy: 0.806\n",
      "Precision: 0.117\n",
      "Sensitivity: 0.178\n",
      "F1-Score: 0.141\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.523\n",
      "\n",
      "MCC: 0.038\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.5667%\n",
      "Precision: 16.9167%\n",
      "Recall or Sensitivity: 16.8833%\n",
      "F1-Score: 0.1482\n",
      "Specificity or True Nagative Rate: 83.3333%\n",
      "Balanced Accuracy: 50.1167%\n",
      "\n",
      "MCC: 0.0028\n",
      "\n",
      "ROC Score:  0.509\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  SVC()\n",
      "[[  0   0   0   0   0   0]\n",
      " [218 331 205 361 219 189]\n",
      " [  0   0   0   0   0   0]\n",
      " [ 58  90  62 119  75  85]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=331, FP=90, TN=399, FN=1192\n",
      "Accuracy: 0.363\n",
      "Precision: 0.786\n",
      "Sensitivity: 0.217\n",
      "F1-Score: 0.341\n",
      "Specificity: 0.816\n",
      "Balanced Accuracy: 0.516\n",
      "\n",
      "MCC: 0.035\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=119, FP=361, TN=1162, FN=370\n",
      "Accuracy: 0.637\n",
      "Precision: 0.248\n",
      "Sensitivity: 0.243\n",
      "F1-Score: 0.246\n",
      "Specificity: 0.763\n",
      "Balanced Accuracy: 0.503\n",
      "\n",
      "MCC: 0.006\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1738, FN=0\n",
      "Accuracy: 0.864\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.1333%\n",
      "Precision: 17.2333%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.0978\n",
      "Specificity or True Nagative Rate: 83.7833%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.506\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  KNeighborsClassifier()\n",
      "[[ 43  55  36  44  38  36]\n",
      " [ 53  81  68 122  79  62]\n",
      " [ 39  53  34  57  28  38]\n",
      " [ 65  99  57 116  63  65]\n",
      " [ 31  66  29  70  46  31]\n",
      " [ 45  67  43  71  40  42]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=43, FP=233, TN=1527, FN=209\n",
      "Accuracy: 0.78\n",
      "Precision: 0.156\n",
      "Sensitivity: 0.171\n",
      "F1-Score: 0.163\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.52\n",
      "\n",
      "MCC: 0.037\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=81, FP=340, TN=1207, FN=384\n",
      "Accuracy: 0.64\n",
      "Precision: 0.192\n",
      "Sensitivity: 0.174\n",
      "F1-Score: 0.183\n",
      "Specificity: 0.78\n",
      "Balanced Accuracy: 0.477\n",
      "\n",
      "MCC: -0.047\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=34, FP=233, TN=1530, FN=215\n",
      "Accuracy: 0.777\n",
      "Precision: 0.127\n",
      "Sensitivity: 0.137\n",
      "F1-Score: 0.132\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.502\n",
      "\n",
      "MCC: 0.004\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=116, FP=364, TN=1183, FN=349\n",
      "Accuracy: 0.646\n",
      "Precision: 0.242\n",
      "Sensitivity: 0.249\n",
      "F1-Score: 0.246\n",
      "Specificity: 0.765\n",
      "Balanced Accuracy: 0.507\n",
      "\n",
      "MCC: 0.014\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=46, FP=248, TN=1491, FN=227\n",
      "Accuracy: 0.764\n",
      "Precision: 0.156\n",
      "Sensitivity: 0.168\n",
      "F1-Score: 0.162\n",
      "Specificity: 0.857\n",
      "Balanced Accuracy: 0.512\n",
      "\n",
      "MCC: 0.025\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=42, FP=232, TN=1472, FN=266\n",
      "Accuracy: 0.752\n",
      "Precision: 0.153\n",
      "Sensitivity: 0.136\n",
      "F1-Score: 0.144\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.5\n",
      "\n",
      "MCC: 0.0\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.65%\n",
      "Precision: 17.1%\n",
      "Recall or Sensitivity: 17.25%\n",
      "F1-Score: 0.1717\n",
      "Specificity or True Nagative Rate: 83.3667%\n",
      "Balanced Accuracy: 50.3%\n",
      "\n",
      "MCC: 0.0055\n",
      "\n",
      "ROC Score:  0.511\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Method:  MultinomialNB()\n",
      "[[  0   0   0   0   0   0]\n",
      " [101 155 110 164 113 110]\n",
      " [  0   0   0   0   0   0]\n",
      " [175 266 157 316 181 164]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=155, FP=266, TN=993, FN=598\n",
      "Accuracy: 0.571\n",
      "Precision: 0.368\n",
      "Sensitivity: 0.206\n",
      "F1-Score: 0.264\n",
      "Specificity: 0.789\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.006\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=316, FP=164, TN=589, FN=943\n",
      "Accuracy: 0.45\n",
      "Precision: 0.658\n",
      "Sensitivity: 0.251\n",
      "F1-Score: 0.363\n",
      "Specificity: 0.782\n",
      "Balanced Accuracy: 0.516\n",
      "\n",
      "MCC: 0.038\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1738, FN=0\n",
      "Accuracy: 0.864\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.4833%\n",
      "Precision: 17.1%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.1045\n",
      "Specificity or True Nagative Rate: 83.65%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.506\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Bagging Classifier' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# Using Bagging Regressor for All Algorithms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "AutoInsLR1 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "# Decision Tree Regression\n",
    "\n",
    "AutoInsDR1 = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "# Random Forest Regression\n",
    "\n",
    "AutoInsRF1 = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                   n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                   ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "# SVR\n",
    "\n",
    "AutoInsSVC1 = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, \n",
    "                 tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', \n",
    "                 break_ties=False, random_state=None)\n",
    "\n",
    "# KNN Regression\n",
    "\n",
    "AutoInsKNN1 = KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', \n",
    "                                  metric_params=None, n_jobs=None)\n",
    "\n",
    "# Navie Bayers Classifier\n",
    "\n",
    "AutoInsNB1 = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "base_methods = [None, AutoInsLR1, AutoInsDR1, AutoInsRF1, AutoInsSVC1, AutoInsKNN1, AutoInsNB1]\n",
    "\n",
    "for bm in base_methods:\n",
    "    print(\"Method: \", bm)\n",
    "    \n",
    "    AutoInsBAG = BaggingClassifier(base_estimator=bm, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                      bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    "    \n",
    "    AutoInsBAG = AutoInsBAG.fit(x_train,y_train)\n",
    "\n",
    "    # Predict the model with test data set\n",
    "\n",
    "    y9_pred = AutoInsBAG.predict(x_test)\n",
    "    y9_pred_proba = AutoInsBAG.predict_proba(x_test)\n",
    "\n",
    "\n",
    "    # confusion matrix in sklearn\n",
    "\n",
    "    from sklearn.metrics import multilabel_confusion_matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    print(confusion_matrix(y9_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "    # Actual and predicted classes\n",
    "\n",
    "    lst_actual_class = y_test\n",
    "    lst_predicted_class = y9_pred\n",
    "\n",
    "    # Class = Label 1-6\n",
    "\n",
    "    lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    # Compute multi-class confusion matrix \n",
    "\n",
    "    arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "    # Temp store results\n",
    "\n",
    "    model_acc = []\n",
    "    model_recall =[]\n",
    "    model_prec = []\n",
    "    model_f1score = []\n",
    "    model_spec = []\n",
    "    model_bal_acc = []\n",
    "    model_mcc = []\n",
    "    for no_class in range(len(lst_classes)):\n",
    "        arr_data = arr_out_matrix[no_class]\n",
    "        print('Print Class: {0}\\n'.format(no_class))\n",
    "\n",
    "        tp = arr_data[1][1]\n",
    "        fp = arr_data[0][1]\n",
    "        tn = arr_data[0][0]\n",
    "        fn = arr_data[1][0]\n",
    "\n",
    "        sensitivity = round(tp/(tp+fn), 3)\n",
    "        specificity = round(tn/(tn+fp), 3)\n",
    "        accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "        balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "\n",
    "        precision =round(tp/(tp+fp), 3)\n",
    "        f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "\n",
    "        x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "        MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "\n",
    "        model_acc.append(accuracy)\n",
    "        model_prec.append(precision)\n",
    "        model_recall.append(sensitivity)\n",
    "        model_f1score.append(f1score)\n",
    "        model_spec.append(specificity)\n",
    "        model_bal_acc.append(balanced_accuracy)\n",
    "        model_mcc.append(MCC)\n",
    "\n",
    "        print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "        print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "        print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "        print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "        print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "        print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "        print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "        print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "\n",
    "    # OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "    # importing mean()\n",
    "\n",
    "    from statistics import mean\n",
    "    import math\n",
    "\n",
    "    print(\"Overall performace Prediction:\")\n",
    "    print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "    print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "    print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "    print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "    print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "    print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "    print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "    # ROC AUC SCORE\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    ROC_AUC_Score = roc_auc_score(y_test, y9_pred_proba, multi_class='ovr', average='weighted')\n",
    "    print('ROC Score: ', round(ROC_AUC_Score, 3)) \n",
    "    print('------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431e7a9",
   "metadata": {},
   "source": [
    "# Adaboost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38b64e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  15   6   7   2  10]\n",
      " [133 182 110 188  94 104]\n",
      " [ 16  18   6  22  12   6]\n",
      " [ 99 181 112 210 148 117]\n",
      " [  0   1   2   6   1   1]\n",
      " [ 16  24  31  47  37  36]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=12, FP=264, TN=1696, FN=40\n",
      "Accuracy: 0.849\n",
      "Precision: 0.043\n",
      "Sensitivity: 0.231\n",
      "F1-Score: 0.073\n",
      "Specificity: 0.865\n",
      "Balanced Accuracy: 0.548\n",
      "\n",
      "MCC: 0.044\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=182, FP=239, TN=962, FN=629\n",
      "Accuracy: 0.569\n",
      "Precision: 0.432\n",
      "Sensitivity: 0.224\n",
      "F1-Score: 0.295\n",
      "Specificity: 0.801\n",
      "Balanced Accuracy: 0.513\n",
      "\n",
      "MCC: 0.031\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=6, FP=261, TN=1671, FN=74\n",
      "Accuracy: 0.833\n",
      "Precision: 0.022\n",
      "Sensitivity: 0.075\n",
      "F1-Score: 0.035\n",
      "Specificity: 0.865\n",
      "Balanced Accuracy: 0.47\n",
      "\n",
      "MCC: -0.035\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=210, FP=270, TN=875, FN=657\n",
      "Accuracy: 0.539\n",
      "Precision: 0.438\n",
      "Sensitivity: 0.242\n",
      "F1-Score: 0.312\n",
      "Specificity: 0.764\n",
      "Balanced Accuracy: 0.503\n",
      "\n",
      "MCC: 0.007\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=1, FP=293, TN=1708, FN=10\n",
      "Accuracy: 0.849\n",
      "Precision: 0.003\n",
      "Sensitivity: 0.091\n",
      "F1-Score: 0.007\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: 0.472\n",
      "\n",
      "MCC: -0.012\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=36, FP=238, TN=1583, FN=155\n",
      "Accuracy: 0.805\n",
      "Precision: 0.131\n",
      "Sensitivity: 0.188\n",
      "F1-Score: 0.155\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.528\n",
      "\n",
      "MCC: 0.049\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.0667%\n",
      "Precision: 17.8167%\n",
      "Recall or Sensitivity: 17.5167%\n",
      "F1-Score: 0.1462\n",
      "Specificity or True Nagative Rate: 83.6333%\n",
      "Balanced Accuracy: 50.5667%\n",
      "\n",
      "MCC: 0.014\n",
      "\n",
      "ROC Score:  0.528\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Adaboost Classifier' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "AutoInsABC = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
    "\n",
    "AutoInsABC = AutoInsABC.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y10_pred = AutoInsABC.predict(x_test)\n",
    "y10_pred_proba = AutoInsGBC.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y10_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y10_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y10_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07effd7",
   "metadata": {},
   "source": [
    "# Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1dde48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 28  38  31  42  26  23]\n",
      " [ 93 116  78 112  67  72]\n",
      " [ 27  34  20  46  21  16]\n",
      " [ 76 135  90 172 107  88]\n",
      " [ 19  35  20  38  30  28]\n",
      " [ 33  63  28  70  43  47]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=28, FP=248, TN=1576, FN=160\n",
      "Accuracy: 0.797\n",
      "Precision: 0.101\n",
      "Sensitivity: 0.149\n",
      "F1-Score: 0.121\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.011\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=116, FP=305, TN=1169, FN=422\n",
      "Accuracy: 0.639\n",
      "Precision: 0.276\n",
      "Sensitivity: 0.216\n",
      "F1-Score: 0.242\n",
      "Specificity: 0.793\n",
      "Balanced Accuracy: 0.505\n",
      "\n",
      "MCC: 0.009\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=20, FP=247, TN=1601, FN=144\n",
      "Accuracy: 0.806\n",
      "Precision: 0.075\n",
      "Sensitivity: 0.122\n",
      "F1-Score: 0.093\n",
      "Specificity: 0.866\n",
      "Balanced Accuracy: 0.494\n",
      "\n",
      "MCC: -0.009\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=172, FP=308, TN=1036, FN=496\n",
      "Accuracy: 0.6\n",
      "Precision: 0.358\n",
      "Sensitivity: 0.257\n",
      "F1-Score: 0.3\n",
      "Specificity: 0.771\n",
      "Balanced Accuracy: 0.514\n",
      "\n",
      "MCC: 0.031\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=30, FP=264, TN=1578, FN=140\n",
      "Accuracy: 0.799\n",
      "Precision: 0.102\n",
      "Sensitivity: 0.176\n",
      "F1-Score: 0.129\n",
      "Specificity: 0.857\n",
      "Balanced Accuracy: 0.516\n",
      "\n",
      "MCC: 0.026\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=47, FP=227, TN=1501, FN=237\n",
      "Accuracy: 0.769\n",
      "Precision: 0.172\n",
      "Sensitivity: 0.165\n",
      "F1-Score: 0.168\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.517\n",
      "\n",
      "MCC: 0.035\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 73.5%\n",
      "Precision: 18.0667%\n",
      "Recall or Sensitivity: 18.0833%\n",
      "F1-Score: 0.1755\n",
      "Specificity or True Nagative Rate: 83.6667%\n",
      "Balanced Accuracy: 50.8667%\n",
      "\n",
      "MCC: 0.0172\n",
      "\n",
      "ROC Score:  0.525\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Light GBM Classifier' model with random sampling\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "AutoInsLGBM = LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=100, \n",
    "                             subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                             min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n",
    "                             colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=- 1, \n",
    "                             silent='warn', importance_type='split')\n",
    "\n",
    "AutoInsLGBM = AutoInsLGBM.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y11_pred = AutoInsLGBM.predict(x_test)\n",
    "y11_pred_proba = AutoInsLGBM.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y11_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y11_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y11_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e292b6",
   "metadata": {},
   "source": [
    "# Train the Principal Component Analysis (PCA) with train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91592f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99974616e-01 4.25227511e-06 4.03842100e-06 3.46766761e-06\n",
      " 1.87959253e-06 1.56016968e-06 1.51690767e-06 1.39388703e-06\n",
      " 1.27138974e-06 1.07198765e-06 9.47421440e-07 9.13410583e-07\n",
      " 8.61316319e-07 7.59081005e-07 6.43844377e-07 5.75601914e-07\n",
      " 1.81684135e-07 4.95711396e-08 1.15619144e-37 8.44860123e-38\n",
      " 3.83822812e-38 4.46974479e-40]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "applyPCA = PCA()\n",
    "\n",
    "x1_train = applyPCA.fit_transform(x_train)\n",
    "x1_test = applyPCA.transform(x_test)\n",
    "explained_variance = applyPCA.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cad468",
   "metadata": {},
   "source": [
    "# Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8e5ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6   3  11   7   7   3]\n",
      " [168 255 155 253 139 139]\n",
      " [  1   2   0   4   1   0]\n",
      " [ 79 129  69 169  97  92]\n",
      " [  0   0   1   2   4   0]\n",
      " [ 22  32  31  45  46  40]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=6, FP=270, TN=1705, FN=31\n",
      "Accuracy: 0.85\n",
      "Precision: 0.022\n",
      "Sensitivity: 0.162\n",
      "F1-Score: 0.038\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: 0.512\n",
      "\n",
      "MCC: 0.01\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=255, FP=166, TN=737, FN=854\n",
      "Accuracy: 0.493\n",
      "Precision: 0.606\n",
      "Sensitivity: 0.23\n",
      "F1-Score: 0.333\n",
      "Specificity: 0.816\n",
      "Balanced Accuracy: 0.523\n",
      "\n",
      "MCC: 0.056\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1737, FN=8\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: 0.0\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.434\n",
      "\n",
      "MCC: -0.025\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=169, FP=311, TN=1066, FN=466\n",
      "Accuracy: 0.614\n",
      "Precision: 0.352\n",
      "Sensitivity: 0.266\n",
      "F1-Score: 0.303\n",
      "Specificity: 0.774\n",
      "Balanced Accuracy: 0.52\n",
      "\n",
      "MCC: 0.044\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=4, FP=290, TN=1715, FN=3\n",
      "Accuracy: 0.854\n",
      "Precision: 0.014\n",
      "Sensitivity: 0.571\n",
      "F1-Score: 0.027\n",
      "Specificity: 0.855\n",
      "Balanced Accuracy: 0.713\n",
      "\n",
      "MCC: 0.071\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=40, FP=234, TN=1562, FN=176\n",
      "Accuracy: 0.796\n",
      "Precision: 0.146\n",
      "Sensitivity: 0.185\n",
      "F1-Score: 0.163\n",
      "Specificity: 0.87\n",
      "Balanced Accuracy: 0.528\n",
      "\n",
      "MCC: 0.05\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.5%\n",
      "Precision: 19.0%\n",
      "Recall or Sensitivity: 23.5667%\n",
      "F1-Score: 0.144\n",
      "Specificity or True Nagative Rate: 84.0833%\n",
      "Balanced Accuracy: 53.8333%\n",
      "\n",
      "MCC: 0.0343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                            intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "                            n_jobs=None, penalty='l2', random_state=None,\n",
    "                            solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "AutoInsLR = AutoInsLR.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y12_pred = AutoInsLR.predict(x1_test)\n",
    "y12_pred_proba = AutoInsLR.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y12_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y12_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87e5caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16115864 0.21535345 0.13434624 0.21258402 0.11986637 0.15669128]\n",
      " [0.16619358 0.19077789 0.14199015 0.20496689 0.11889917 0.17717232]\n",
      " [0.14649929 0.25643306 0.1195309  0.2483127  0.12727552 0.10194854]\n",
      " ...\n",
      " [0.12673658 0.20835697 0.10725738 0.25578884 0.128475   0.17338524]\n",
      " [0.1764268  0.2404702  0.1437959  0.20179856 0.12092578 0.11658276]\n",
      " [0.20580582 0.21886513 0.16451665 0.19715163 0.0999651  0.11369567]]\n"
     ]
    }
   ],
   "source": [
    "# Fit OneVsRestClassifier model to calculate the ROC SUC and plot ROC curve\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "modelORC = OneVsRestClassifier(LogisticRegression())\n",
    "modelORC.fit(x1_train, y_train)\n",
    "y12_pred = modelORC.predict(x1_test)\n",
    "y12_pred_proba = modelORC.predict_proba(x1_test)\n",
    "print(y12_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fdf3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score:  0.557\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y12_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e43be",
   "metadata": {},
   "source": [
    "# Decision Tree Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "457e82c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63 72 31 60 31 24]\n",
      " [71 92 65 89 49 56]\n",
      " [32 64 29 71 45 36]\n",
      " [53 96 75 98 77 68]\n",
      " [30 46 34 71 41 42]\n",
      " [27 51 33 91 51 48]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=63, FP=213, TN=1518, FN=218\n",
      "Accuracy: 0.786\n",
      "Precision: 0.228\n",
      "Sensitivity: 0.224\n",
      "F1-Score: 0.226\n",
      "Specificity: 0.877\n",
      "Balanced Accuracy: 0.55\n",
      "\n",
      "MCC: 0.102\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=92, FP=329, TN=1261, FN=330\n",
      "Accuracy: 0.672\n",
      "Precision: 0.219\n",
      "Sensitivity: 0.218\n",
      "F1-Score: 0.218\n",
      "Specificity: 0.793\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.011\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=29, FP=238, TN=1497, FN=248\n",
      "Accuracy: 0.758\n",
      "Precision: 0.109\n",
      "Sensitivity: 0.105\n",
      "F1-Score: 0.107\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: 0.484\n",
      "\n",
      "MCC: -0.033\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=98, FP=382, TN=1163, FN=369\n",
      "Accuracy: 0.627\n",
      "Precision: 0.204\n",
      "Sensitivity: 0.21\n",
      "F1-Score: 0.207\n",
      "Specificity: 0.753\n",
      "Balanced Accuracy: 0.482\n",
      "\n",
      "MCC: -0.037\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=41, FP=253, TN=1495, FN=223\n",
      "Accuracy: 0.763\n",
      "Precision: 0.139\n",
      "Sensitivity: 0.155\n",
      "F1-Score: 0.147\n",
      "Specificity: 0.855\n",
      "Balanced Accuracy: 0.505\n",
      "\n",
      "MCC: 0.01\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=48, FP=226, TN=1485, FN=253\n",
      "Accuracy: 0.762\n",
      "Precision: 0.175\n",
      "Sensitivity: 0.159\n",
      "F1-Score: 0.167\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.514\n",
      "\n",
      "MCC: 0.028\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.8%\n",
      "Precision: 17.9%\n",
      "Recall or Sensitivity: 17.85%\n",
      "F1-Score: 0.1787\n",
      "Specificity or True Nagative Rate: 83.4833%\n",
      "Balanced Accuracy: 50.6833%\n",
      "\n",
      "MCC: 0.0135\n",
      "\n",
      "ROC Score:  0.504\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Decision Tree Classification' model with random sampling\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsDR = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "AutoInsDR = AutoInsDR.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y13_pred = AutoInsDR.predict(x1_test)\n",
    "y13_pred_proba = AutoInsDR.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y13_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y13_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y13_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840dbf85",
   "metadata": {},
   "source": [
    "# Random Forest Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63d12034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38  56  24  38  17  18]\n",
      " [133 167  97 167  79  61]\n",
      " [ 14  25  14  26  15   8]\n",
      " [ 75 126  91 161 106 109]\n",
      " [  5  20  20  25  22  24]\n",
      " [ 11  27  21  63  55  54]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=38, FP=238, TN=1583, FN=153\n",
      "Accuracy: 0.806\n",
      "Precision: 0.138\n",
      "Sensitivity: 0.199\n",
      "F1-Score: 0.163\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.534\n",
      "\n",
      "MCC: 0.058\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=167, FP=254, TN=1054, FN=537\n",
      "Accuracy: 0.607\n",
      "Precision: 0.397\n",
      "Sensitivity: 0.237\n",
      "F1-Score: 0.297\n",
      "Specificity: 0.806\n",
      "Balanced Accuracy: 0.522\n",
      "\n",
      "MCC: 0.05\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=14, FP=253, TN=1657, FN=88\n",
      "Accuracy: 0.831\n",
      "Precision: 0.052\n",
      "Sensitivity: 0.137\n",
      "F1-Score: 0.076\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.502\n",
      "\n",
      "MCC: 0.003\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=161, FP=319, TN=1025, FN=507\n",
      "Accuracy: 0.589\n",
      "Precision: 0.335\n",
      "Sensitivity: 0.241\n",
      "F1-Score: 0.28\n",
      "Specificity: 0.763\n",
      "Balanced Accuracy: 0.502\n",
      "\n",
      "MCC: 0.004\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=22, FP=272, TN=1624, FN=94\n",
      "Accuracy: 0.818\n",
      "Precision: 0.075\n",
      "Sensitivity: 0.19\n",
      "F1-Score: 0.107\n",
      "Specificity: 0.857\n",
      "Balanced Accuracy: 0.524\n",
      "\n",
      "MCC: 0.03\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=54, FP=220, TN=1561, FN=177\n",
      "Accuracy: 0.803\n",
      "Precision: 0.197\n",
      "Sensitivity: 0.234\n",
      "F1-Score: 0.214\n",
      "Specificity: 0.876\n",
      "Balanced Accuracy: 0.555\n",
      "\n",
      "MCC: 0.102\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.2333%\n",
      "Precision: 19.9%\n",
      "Recall or Sensitivity: 20.6333%\n",
      "F1-Score: 0.1895\n",
      "Specificity or True Nagative Rate: 83.9833%\n",
      "Balanced Accuracy: 52.3167%\n",
      "\n",
      "MCC: 0.0412\n",
      "\n",
      "ROC Score:  0.567\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Random Forest Classification' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsRF = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "                                   n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                   ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "AutoInsRF = AutoInsRF.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y14_pred = AutoInsRF.predict(x1_test)\n",
    "y14_pred_proba = AutoInsRF.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y14_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y14_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y14_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eed3bc",
   "metadata": {},
   "source": [
    "# Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63b86bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [ 95 151  84 156  78  76]\n",
      " [  0   0   0   0   0   0]\n",
      " [181 270 183 324 216 198]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=0, FP=276, TN=1736, FN=0\n",
      "Accuracy: 0.863\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.863\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=110, FP=311, TN=1190, FN=401\n",
      "Accuracy: 0.646\n",
      "Precision: 0.261\n",
      "Sensitivity: 0.215\n",
      "F1-Score: 0.236\n",
      "Specificity: 0.793\n",
      "Balanced Accuracy: 0.504\n",
      "\n",
      "MCC: 0.009\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=0, FP=267, TN=1745, FN=0\n",
      "Accuracy: 0.867\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=354, FP=126, TN=385, FN=1147\n",
      "Accuracy: 0.367\n",
      "Precision: 0.738\n",
      "Sensitivity: 0.236\n",
      "F1-Score: 0.357\n",
      "Specificity: 0.753\n",
      "Balanced Accuracy: 0.494\n",
      "\n",
      "MCC: -0.011\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=0, FP=294, TN=1718, FN=0\n",
      "Accuracy: 0.854\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.854\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=0, FP=274, TN=1738, FN=0\n",
      "Accuracy: 0.864\n",
      "Precision: 0.0\n",
      "Sensitivity: nan\n",
      "F1-Score: 0.0\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: nan\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 74.35%\n",
      "Precision: 16.65%\n",
      "Recall or Sensitivity: nan%\n",
      "F1-Score: 0.0988\n",
      "Specificity or True Nagative Rate: 83.2333%\n",
      "Balanced Accuracy: nan%\n",
      "\n",
      "MCC: nan\n",
      "\n",
      "ROC Score:  0.567\n"
     ]
    }
   ],
   "source": [
    "# To build the 'Support Vector Classification' model with random sampling\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsSVC = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, \n",
    "                 tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', \n",
    "                 break_ties=False, random_state=None)\n",
    "\n",
    "AutoInsSVC = AutoInsSVC.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y15_pred = AutoInsSVC.predict(x1_test)\n",
    "y15_pred_proba = AutoInsRF.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y4_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y15_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y15_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afca9f",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0f8dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 69  83  62  94  59  63]\n",
      " [ 74 109  73 137  85  74]\n",
      " [ 27  50  28  55  26  32]\n",
      " [ 50  86  52 103  64  59]\n",
      " [ 30  55  24  51  35  21]\n",
      " [ 26  38  28  40  25  25]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=69, FP=207, TN=1375, FN=361\n",
      "Accuracy: 0.718\n",
      "Precision: 0.25\n",
      "Sensitivity: 0.16\n",
      "F1-Score: 0.195\n",
      "Specificity: 0.869\n",
      "Balanced Accuracy: 0.514\n",
      "\n",
      "MCC: 0.035\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=109, FP=312, TN=1148, FN=443\n",
      "Accuracy: 0.625\n",
      "Precision: 0.259\n",
      "Sensitivity: 0.197\n",
      "F1-Score: 0.224\n",
      "Specificity: 0.786\n",
      "Balanced Accuracy: 0.492\n",
      "\n",
      "MCC: -0.018\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=28, FP=239, TN=1555, FN=190\n",
      "Accuracy: 0.787\n",
      "Precision: 0.105\n",
      "Sensitivity: 0.128\n",
      "F1-Score: 0.115\n",
      "Specificity: 0.867\n",
      "Balanced Accuracy: 0.498\n",
      "\n",
      "MCC: -0.004\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=103, FP=377, TN=1221, FN=311\n",
      "Accuracy: 0.658\n",
      "Precision: 0.215\n",
      "Sensitivity: 0.249\n",
      "F1-Score: 0.23\n",
      "Specificity: 0.764\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.012\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=35, FP=259, TN=1537, FN=181\n",
      "Accuracy: 0.781\n",
      "Precision: 0.119\n",
      "Sensitivity: 0.162\n",
      "F1-Score: 0.137\n",
      "Specificity: 0.856\n",
      "Balanced Accuracy: 0.509\n",
      "\n",
      "MCC: 0.016\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=25, FP=249, TN=1581, FN=157\n",
      "Accuracy: 0.798\n",
      "Precision: 0.091\n",
      "Sensitivity: 0.137\n",
      "F1-Score: 0.11\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.5\n",
      "\n",
      "MCC: 0.001\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.7833%\n",
      "Precision: 17.3167%\n",
      "Recall or Sensitivity: 17.2167%\n",
      "F1-Score: 0.1685\n",
      "Specificity or True Nagative Rate: 83.4333%\n",
      "Balanced Accuracy: 50.3167%\n",
      "\n",
      "MCC: 0.007\n",
      "\n",
      "ROC Score:  0.507\n"
     ]
    }
   ],
   "source": [
    "# To build the 'K-Nearest Neighbors' model with random sampling\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsKNN = KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', \n",
    "                                  metric_params=None, n_jobs=None)\n",
    "\n",
    "AutoInsKNN = AutoInsKNN.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y16_pred = AutoInsKNN.predict(x1_test)\n",
    "y16_pred_proba = AutoInsKNN.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y16_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y16_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y16_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bd8c9",
   "metadata": {},
   "source": [
    "# Navie Bayers Classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af15b21f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-3934f69e0b58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mAutoInsNB1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mAutoInsNB1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoInsNB1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Predict the model with test data set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "# To build the 'Navie Bayers Classifier' model with random sampling\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsNB1 = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "AutoInsNB1 = AutoInsNB1.fit(x1_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y17_pred = AutoInsNB1.predict(x1_test)\n",
    "y17_pred_proba = AutoInsNB1.predict_proba(x1_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y6_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y17_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y17_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3379e",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d0f1d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 27\n"
     ]
    }
   ],
   "source": [
    "# List Hyperparameters that we want to tune.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "# Convert to dictionary\n",
    "\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "# Create new KNN object\n",
    "\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=5,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', \n",
    "                             metric_params=None, n_jobs=None)\n",
    "\n",
    "#Use GridSearch\n",
    "\n",
    "g_search = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#Fit the model\n",
    "\n",
    "g_search = g_search.fit(x_train,y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', g_search.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', g_search.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', g_search.best_estimator_.get_params()['n_neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "273f527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 33  54  28  57  29  24]\n",
      " [102 131  84 183 113  88]\n",
      " [ 16  33  19  32  21  10]\n",
      " [ 90 121  87 137  91  98]\n",
      " [  9  35  17  27  16  22]\n",
      " [ 26  47  32  44  24  32]]\n",
      "Print Class: 0\n",
      "\n",
      "TP=33, FP=243, TN=1544, FN=192\n",
      "Accuracy: 0.784\n",
      "Precision: 0.12\n",
      "Sensitivity: 0.147\n",
      "F1-Score: 0.132\n",
      "Specificity: 0.864\n",
      "Balanced Accuracy: 0.505\n",
      "\n",
      "MCC: 0.01\n",
      "\n",
      "Print Class: 1\n",
      "\n",
      "TP=131, FP=290, TN=1021, FN=570\n",
      "Accuracy: 0.573\n",
      "Precision: 0.311\n",
      "Sensitivity: 0.187\n",
      "F1-Score: 0.234\n",
      "Specificity: 0.779\n",
      "Balanced Accuracy: 0.483\n",
      "\n",
      "MCC: -0.04\n",
      "\n",
      "Print Class: 2\n",
      "\n",
      "TP=19, FP=248, TN=1633, FN=112\n",
      "Accuracy: 0.821\n",
      "Precision: 0.071\n",
      "Sensitivity: 0.145\n",
      "F1-Score: 0.095\n",
      "Specificity: 0.868\n",
      "Balanced Accuracy: 0.506\n",
      "\n",
      "MCC: 0.01\n",
      "\n",
      "Print Class: 3\n",
      "\n",
      "TP=137, FP=343, TN=1045, FN=487\n",
      "Accuracy: 0.587\n",
      "Precision: 0.285\n",
      "Sensitivity: 0.22\n",
      "F1-Score: 0.248\n",
      "Specificity: 0.753\n",
      "Balanced Accuracy: 0.486\n",
      "\n",
      "MCC: -0.03\n",
      "\n",
      "Print Class: 4\n",
      "\n",
      "TP=16, FP=278, TN=1608, FN=110\n",
      "Accuracy: 0.807\n",
      "Precision: 0.054\n",
      "Sensitivity: 0.127\n",
      "F1-Score: 0.076\n",
      "Specificity: 0.853\n",
      "Balanced Accuracy: 0.49\n",
      "\n",
      "MCC: -0.014\n",
      "\n",
      "Print Class: 5\n",
      "\n",
      "TP=32, FP=242, TN=1565, FN=173\n",
      "Accuracy: 0.794\n",
      "Precision: 0.117\n",
      "Sensitivity: 0.156\n",
      "F1-Score: 0.134\n",
      "Specificity: 0.866\n",
      "Balanced Accuracy: 0.511\n",
      "\n",
      "MCC: 0.02\n",
      "\n",
      "Overall performace Prediction:\n",
      "Accuracy: 72.7667%\n",
      "Precision: 15.9667%\n",
      "Recall or Sensitivity: 16.3667%\n",
      "F1-Score: 0.1532\n",
      "Specificity or True Nagative Rate: 83.05%\n",
      "Balanced Accuracy: 49.6833%\n",
      "\n",
      "MCC: -0.0073\n",
      "\n",
      "ROC Score:  0.508\n"
     ]
    }
   ],
   "source": [
    "# To build the 'K-Nearest Neighbors' model with random sampling\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from math import sqrt\n",
    "\n",
    "AutoInsKNN = KNeighborsClassifier(n_neighbors=27,weights='uniform', algorithm='auto', leaf_size=1, p=1, metric='minkowski', \n",
    "                                  metric_params=None, n_jobs=None)\n",
    "\n",
    "AutoInsKNN = AutoInsKNN.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y18_pred = AutoInsKNN.predict(x_test)\n",
    "y18_pred_proba = AutoInsKNN.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(y18_pred, y_test)) # Verticle is actual values & horizontal is predicted values\n",
    "\n",
    "# Actual and predicted classes\n",
    "\n",
    "lst_actual_class = y_test\n",
    "lst_predicted_class = y18_pred\n",
    "\n",
    "# Class = Label 1-6\n",
    "\n",
    "lst_classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Compute multi-class confusion matrix \n",
    "\n",
    "arr_out_matrix = multilabel_confusion_matrix(lst_predicted_class, lst_actual_class, labels = lst_classes)\n",
    "\n",
    "# Temp store results\n",
    "\n",
    "model_acc = []\n",
    "model_recall =[]\n",
    "model_prec = []\n",
    "model_f1score = []\n",
    "model_spec = []\n",
    "model_bal_acc = []\n",
    "model_mcc = []\n",
    "for no_class in range(len(lst_classes)):\n",
    "    arr_data = arr_out_matrix[no_class]\n",
    "    print('Print Class: {0}\\n'.format(no_class))\n",
    "    \n",
    "    tp = arr_data[1][1]\n",
    "    fp = arr_data[0][1]\n",
    "    tn = arr_data[0][0]\n",
    "    fn = arr_data[1][0]\n",
    "    \n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3)\n",
    "    accuracy = round((tp+tn)/ (tp+fp+fn+tn), 3)\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2 , 3)\n",
    "    \n",
    "    precision =round(tp/(tp+fp), 3)\n",
    "    f1score = round((2*tp/(2*tp+fp+fn)), 3)\n",
    "    \n",
    "    x = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn)-(fp * fn)) / sqrt(x), 3)\n",
    "    \n",
    "    model_acc.append(accuracy)\n",
    "    model_prec.append(precision)\n",
    "    model_recall.append(sensitivity)\n",
    "    model_f1score.append(f1score)\n",
    "    model_spec.append(specificity)\n",
    "    model_bal_acc.append(balanced_accuracy)\n",
    "    model_mcc.append(MCC)\n",
    "    \n",
    "    print(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: {0}\".format(accuracy)); # Accuracy score\n",
    "    print(\"Precision: {0}\".format(precision)); # Precision score\n",
    "    print(\"Sensitivity: {0}\".format(sensitivity)); # Recall score\n",
    "    print(\"F1-Score: {0}\".format(f1score)); # F1 score\n",
    "    print(\"Specificity: {0}\".format(specificity)); # True Nagative Rate\n",
    "    print(\"Balanced Accuracy: {0}\\n\".format(balanced_accuracy)); # Balance accuracy score\n",
    "    print(\"MCC: {0}\\n\".format(MCC)); # Matthews Correlation Coefficient\n",
    "    \n",
    "# OVERALL - FINAL PREDICTION PERFORMANCE\n",
    "\n",
    "# importing mean()\n",
    "\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "print(\"Overall performace Prediction:\")\n",
    "print(\"Accuracy: {0}%\".format(round(mean(model_acc)*100 , 4)))\n",
    "print(\"Precision: {0}%\".format(round(mean(model_prec)*100, 4)));\n",
    "print(\"Recall or Sensitivity: {0}%\".format(round(mean(model_recall)*100, 4)));\n",
    "print(\"F1-Score: {0}\".format(round(mean(model_f1score), 4)));\n",
    "print(\"Specificity or True Nagative Rate: {0}%\".format(round(mean(model_spec)*100, 4)));\n",
    "print(\"Balanced Accuracy: {0}%\\n\".format(round(mean(model_bal_acc)*100, 4)));\n",
    "print(\"MCC: {0}\\n\".format(round(mean(model_mcc), 4)));\n",
    "\n",
    "# ROC AUC SCORE\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC_Score = roc_auc_score(y_test, y18_pred_proba, multi_class='ovr', average='weighted')\n",
    "print('ROC Score: ', round(ROC_AUC_Score, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b838a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
